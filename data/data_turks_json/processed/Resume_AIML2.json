{"content": "DEBJYOTI\n        \t   Mobile: +91 - 8050740371\t \n            \tE-mail:d.hota84@outlook.com\n\n      \nDATA WAREHOUSE SPECIALIST\n\nOrchestrating technology change management initiatives to drive cost savings and ROI while                                     expanding infrastructure capacity and performance.\n\n\n       EXPERIENCE SUMMARY\n\nSeasoned data warehouse/Cloud specialist and business analyst around 9 years of data management experience seeking a challenging position at a company that uses analytics to drive strategy.  Specializing in rapid deployment of Data Warehouse solutions, I have a track record of delivering data management solutions on time and under budget, providing significant documentable ROI.  Experienced with full-lifecycle development including complex SQL development, OLAP development, DBA duties, ETL coding, requirements documentation, data modeling, and report writing in a cloud environment.  Reputation for high-quality work, clever solutions, and a commitment to user satisfaction. Core competencies include:\n\n\t* Business analysis and requirement gathering\n* Project planning\n* System Configuration                                        \n* Business intelligence solution architecture\n* Data warehouse design                                    \n* ETL design\n* ETL process development                                 \n* OLAP design\n* Data cleaning\n\t* Data integration from multiple sources & confirmation\n* Data validation                                                \n* Performance tuning\n* Report development                                         \n* Dashboard design\n* Security plan design                                         \n* Data warehouse maintenance\n* Ongoing user support and education\n* Cloud\n\n\n                                              \n\n· Having around 8.8+ years of IT experience, in various data warehousing tools like Informatica, ICS (Informatica Cloud Services), Mercator (Data stage TX), and OBIEE reports testing.\n· Worked on all phases of data warehouse development lifecycle, from gathering requirements to testing, implementation, upgrade and support (In cloud)002E\n· Worked on various source systems like Oracle, Siebel, SAP, and MS SQL Server, DB2 etc.\n· Having 5 lifecycle Implementations, 1 Upgrade and 2 supporting (including testing) project knowledge with Good analysis, design, development, customization, upgradation and implementation and testing knowledge.\n· Good experience on Data Mapping, Transformation and Loading in complex, high-volume job environment using Informatica (including MDM and IDQ), Informatica cloud, Data stage TX, Mercator and super tools like HP3000 Emulator.\n· Possesses extensive programming skills in ETL and EDI. \n· Good knowledge on OBIEE and MicroStrategy.\n· Excellent technical and analytical skills with clear understanding of design goals of ER modeling for OLTP and dimension modeling for OLAP.\n· Excellent skills in Oracle 9i/10g, SQL, PL/SQL.\n· A self-motivated team player with Excellent interpersonal and communication skills. Experienced in working with senior level managers, business people and developers across multiple disciplines.\n\n\n\nEDUCATION\n\n                 Master’s in computer application (M.C.A.) Degree (2008)\nSilicon Institute of Technology\n                 ***Completed with recognition***\n \n    TECHNICAL PROFILE\n\n     \nETL/EDI tools              :      Informatica 8.6/9.1, Mercator (Data stage TX), HP 3000   Emulator\nReporting tool            :     Business Objects, OBIEE\nLanguages\t\t     :     SQL, PL/SQL, C, C++, Java, J2EE\nDatabase\t\t     :     Oracle 9i, SQL Server 2005\nThird Party Tools       :     TOAD, WINSCP, putty, SQL developer, MS SQL Server\nOperating System\t     :     Windows, UNIX, Linux \n\n \n\n     CERTIFICATIONS\n\n· Oracle Certified Associate (Oracle 9i)\n· LOMA 280(Insurance Domain certification, US)\n\n \nJOB PROFILE\n· Working as a senior analyst for Accenture, Bangalore from August’2014 to till date.\n· Engaged with IGATE, Bangalore as a Technical Lead from July’2013 to August’2014.\n· Engaged with L & T INFOTECH, Chennai/Bangalore as an Informatica Analyst from May’2010 to July’ 2013.\n\n\n                                 PROFESSIONAL EXPERIENCE\n\n------------------------------------------------------------------------------------\nAccenture– Bangalore, INDIA                                                                2014 to Present\nAccenture is a multinational management consulting, technology services and outsourcing company. Its incorporated headquarters are in Dublin, Ireland since September 1, 2009. It is the world's largest consulting firm as measured by revenues and is a Fortune Global 500 company.As of 2014, the company reported net revenues of $30.0 billionwith approximately 319,000 employees, serving clients in more than 200 cities in 56 countries. Accenture has more employees in India than in any other country; in the US, it has about 40,000 employees and 35,000 located in the Philippines.[Accenture's current clients include 89 of the Fortune Global 100 and more than three-quarters of the Fortune Global 500.\n\n\nProject #7: ECOLABS, USA\n\n        Client \t                   : ECOLABS,USA\n        Environment             : ICS (Informatica cloud service), UNIX, Oracle, SQL Server \n 2005, Microstrategy\n        Tools                        :  Oracle SQL Developer,  Service Now, JIRA, Uc4\n        Role                          : Lead\n        Team Size                 :  15\n        Time period\t         :  MAY’18 to till date.\n        Work Location            :  Offshore\n\n\n\nProject Description\n              Ecolab Inc., headquartered in St. Paul, Minnesota, is an American global provider of water, hygiene and energy technologies and services to the food, energy, healthcare, industrial and hospitality markets. The company's food safety services provide consulting to restaurants, hospitals, food retailers and food & beverage manufacturing facilities. It is also a supplier of chemicals used by beef and poultry processors - to reduce pathogens, such as E. coli and salmonella - in uncooked beef and poultry. It was founded as Economics Laboratory in 1923 by Merritt J. Osborn and renamed \"Ecolab\" in 1986.\n\nResponsibilities:\n\n· Supported integrations for Informatica PowerCenter and Informatica cloud services.\n· Responsible for Change, Incident, Problem and Release management.\n· Working in an agile environment for each phase like analyzing, designing, coding, testing, support and documentation.\n· Worked closely with Architects when designing the Data Marts and Data Warehouses, with Business Analyst in understanding the business needs and interacting with other team members in completing the task as scheduled.\n· Identifying risks proactively and proposing solutions to resolve them.\n· Interact with Business Analysts for reporting requirements to define business and functional specifications.\n· Involved in ETL process from development to testing and production environments.\n· Worked on Informatica Source Analyzer (Analyzing Source System data), Warehouse Designer, Mapping Designer, Mapplet and Transformations.\n· Used most of the Transformations such as Source Qualifier, Aggregator, Lookup, Filter, Sequence generator, Router, Update strategy etc.,\n-----------------------------------------------------------\n\n\n\nProject #7: Disney, USA\n\n        Client \t                   : DISNEY,USA\n        Environment             :  Informatica , UNIX, Oracle, SQL Server 2005,Microstrategy\n        Tools                        :  Oracle SQL Developer\n        Role                          : Senior Tech Architect\n        Team Size                 :  15\n        Time period\t         :  DEC’15 to till date.\n        Work Location            :  Offshore\n\n\n\nProject Description\n              The Walt Disney company commonly known as Disney  is an American  diversified multinational mass media and entertainment conglomerate, headquartered at the Walt Disney Studios in Burbank, California. It is the world's second-largest media conglomerate in terms of revenue, after Comcast The Project requirement is to build a Data warehouse to integrate different source applications so as to pull data for a set of report and dashboard. As a part of the long term solution the requirement is to understand the user requirements, source systems, gain access to source systems, design and develop data warehouse tables, develop reporting tables and maintain historical data. \n\nResponsibilities:\n\n· Implemented ETL solution using Informatica.\n· Analyzing Source System data.\n· Worked on Informatica – Source Analyzer, Warehouse Designer, Mapping Designer, Mapplet and Transformations.\n· Involved in importing Source/Target Tables from the respective databases.\n· Developed data Mappings between source systems and warehouse components using Mapping Designer to resolve the code issue.\n· Used most of the Transformations such as Source Qualifier, Aggregator, Lookup, Filter, Sequence generator, Router, Update strategy etc.,\n· Extensively used ETL to load data from Oracle and Flat files to Data Warehouse. \n· Analysis of complex issue by backtracking the code and providing the solution to resolve the issue .\n\n-----------------------------------------------------------\n\n\n\n\n\nProject #6: KPN, nETHERLANDS\n\n        Client \t                   : KPN, NETHERLANDS\n        Environment             :  Informatica , UNIX, Oracle, SQL Server 2005\n        Tools                        :  Oracle SQL Developer\n        Role                          : Senior Analyst\n        Team Size                 :  5\n        Time period\t         :  AUGl’14 to NOV’15.\n        Work Location            :  Offshore\n\n\n\nProject Description:\n        KPN is a Dutch is a landline and mobile telecommunications company. In the Netherlands, KPN has 6.3 million fixed-line telephone customers. Its mobile division, KPN Mobile, has more than 33 million subscribers in the Netherlands, Germany, Belgium, France, and Spain under different brand names. Through its ownership of several European Internet service providers, KPN also provides Internet access to 2.1 million customers, and it offers business network services and data transport throughout Western Europe.The Project requirement is to build a Data warehouse to integrate different source applications so as to pull data for a set of report and dashboard. As a part of the long term solution the requirement is to understand the user requirements, source systems, gain access to source systems, design and develop data warehouse tables, develop reporting tables and maintain historical data. \n\nResponsibilities:\n\n· Implemented ETL solution using Informatica.\n· Analyzing Source System data.\n· Worked on Informatica – Source Analyzer, Warehouse Designer, Mapping Designer, Mapplet and Transformations.\n· Involved in importing Source/Target Tables from the respective databases.\n· Developed data Mappings between source systems and warehouse components using Mapping Designer\n· Used most of the Transformations such as Source Qualifier, Aggregator, Lookup, Filter, Sequence generator, Router, Update strategy etc.,\n· Extensively used ETL to load data from Oracle and Flat files to Data Warehouse. \n· Used Informatica Workflow Manager to create Sessions, Workflows to run with the logic embedded in the mappings.\n\n-----------------------------------------------------------\n\n\n\n\n\n\niGATE Corp – Bangalore, INDIA                                                                2013 to Present\n\n       iGATE Corporation (IGTE:US) is a NASDAQ-listed, US-incorporated outsourcing company. iGate Inc is an American multinational corporation which provides information technology, consulting and business process outsourcing (BPO) services. It is headquartered in Fremont, California, USA. Customers include companies in banking & financial services; insurance & healthcare; life sciences; manufacturing, retail, distribution & logistics; media, entertainment, leisure & travel; communication, energy & utilities; federal government; and independent software vendors across the Americas, Europe- Middle East-Africa (EMEA) and Asia-Pacific.\n-------------------------------------------------------------------\n\n\nProject #5: GE, us \n\n        Client \t                   : GE, US\n        Environment             :  Informatica Power center 9.1, UNIX, Oracle, SQL Server 2005, \n        Tools                        :  Oracle SQL Developer\n        Role                          : Technical Lead\n        Team Size                 :  7\n        Time period\t         :  Jul’13 to Jul’14.\n        Work Location            :  Offshore\n\n\n\nProject Description:\n        The Project requirement is to build a Data warehouse to integrate different source applications so as to pull data for a set of metrics. This data will be used to generate reports so as to digitize the current Ops metrics which is currently done manually. As a part of the long term solution the requirement is to understand the user requirements, source systems, gain access to source systems, design and develop data warehouse tables, develop reporting tables and maintain historical data. \n\nResponsibilities:\n\n· Debugging and fixing the code bugs of Informatica mapping.\n· Involvement in design with onsite POC development.\n· Worked extensively on Oracle and SQL Server database and flat files as data sources.\n· Making changes in mapping as per the requirement and then did unit testing.\n· Managed the entry in transport table for new interface using aqua data for moving data from one location to other.\n· Testing – Unit, Preparing the UTC, UTR and process related documents.\n· Sharing business knowledge to team and sharing best practices.\n· Worked on various issues on existing Informatica Mappings to produce correct output.\n· Developed Informatica objects - Mappings, sessions, workflows based on the prepared low level design documents.\n\n\n\n-------------------------------------------------------------------\nL&T InfoTech – Bangalore, INDIA                                                                 2010 to 2103\n\n\tL&T InfoTech is one of the biggest IT services and consulting firm in India. It has its reach in over more than 36 countries. It specializes in software services and consulting, engineering, construction, manufacturing goods and financial services.\n-------------------------------------------------------------------\n\n\n\nProject #4: Paramount, us \n\n        Client \t                   : Paramount, US\n        Environment             :  Informatica Power center 8.6.1, UNIX, Oracle, SQL Server 2005\n        Role                         : Informatica Analyst \n        Tools                        :  JIRA, Confluence, VL Trader, AQUA DATA\n        Team Size                 :  4\n        Time period\t         :  Aug’12 to JUN’13.\n        Work Location           :  Offshore\n\n\n\nProject Description:\n        The oldest film studio and an American film production and Distribution Company. This project includes development and enhancement of numerous components to automate client’s business data processing and integration. There are various interfaces like TAP, RAFT, TAP web services, claim etc.\n\n\nResponsibilities:\n\n· Understanding and exploring Business requirement and developing the Confluence page for the same and get the approval by client for freezing      the requirement.\n· Debugging and fixing the code bugs of Informatica mapping.\n· Involvement in design with onsite POC development.\n· Worked extensively on SQL Server database and flat files as data sources.\n· Making changes in mapping as per the requirement and then did unit testing.\n· Developed confluence pages with respect to each interface.\n· Managed the entry in transport table for new interface using aqua data for moving data from one location to other.\n· Developed script in VL trader.\n· Testing – Unit, Preparing the UTC, UTR and process related documents.\n· Sharing business knowledge to team and sharing best practices.\n· Worked on various issues on existing Informatica Mappings to produce correct output.\n· Developed Informatica objects - Mappings, sessions, workflows based on the prepared low level design documents.\n\n\n\n\n\n\nProject #3: Coloplast, UK\n\n               Client \t                   : Coloplast, UK\n        Environment             : Informatica, Siebel, OBIEE, SQL server 2008, DAC\n        Role                         : Informatica Consultant\n        Team Size                 :  3\n        Time period\t         :  Jan’12 to Aug’12.\n        Work Location           : Offshore\n\n\nProject Description:  \n               Coloplast in the UK has been using Siebel on Premises Life Sciences industry vertical and Siebel Analytics for more than 6 years and has been one of the early Oracle customers embracing Siebel as its CRM solution. Siebel CRM application in the UK organization, caters to 80000+ customers, churns over 3000+ sales orders per day, integrates with JDE ERP for order fulfillment and is a vital stakeholder in serving the customers in true sense. Coloplast uses Siebel analytics instance for marketing analytics and has been using hundreds of custom reports and dashboards. The Siebel CRM and Siebel Analytics instances have been running on very old versions and hence qualify for an upgrade. Coloplast believes that the Siebel CRM and Siebel analytics are very critical to their UK business and has decided to upgrade these applications to the latest releases. Coloplast is also planning to upgrade the hardware along with the application upgrade. This upgrade is a Like for like upgrade, which means Coloplast would expect the upgraded applications with same functionalities and reports/ dashboards as they are at present.\n\n\n\n\nResponsibilities:\n\n· Played a critical role in upgrading the environment (Informatica 7.0 to 9.1). \n· Involved in Retro fix of various mappings with the same functionalities as they were present in old environment.\n· Testing – Unit, Preparing the UTC, UTR and process related documents.\n· Implemented ETL solution using Informatica.\n· Analyzing Source System data.\n· Worked on Informatica – Source Analyzer, Warehouse Designer, Mapping Designer, Mapplet and Transformations.\n· Sharing business knowledge to team and sharing best practices.\n\n\n\n\n\n\n\nProject #2: MMC Global Technology Infrastructure, USA\n\n\n          Client \t         : MMC Global Technology Infrastructure, USA\n        Environment   : Oracle10g, Informatica 8.6, UNIX \n        Tool               : putty, WINSCP, SQL Developer\n        Role               : Informatica Consultant\n        Team Size       :  4\n        Time period     : Aug’11 to Dec’11        \n        Work Location  : Offshore\n\n\nProject Description:\n\n              Marsh & McLennan Companies, Inc. (MMC) is a US-based global professional service and insurance brokerage firm.MMC is a diversified risk, insurance and professional services firm which is the world’s leading broker and risk adviser. The ultimate aim of the project is used in the  Revenue Management and Billing (RM&B) Business Intelligence (BI) .This describes the data stores, source data flows and target reporting & warehouse Tiers using diagrams required to provide the reporting capabilities needed for the US BASYS replacement.  The intention is to provide the basis for a global reporting platform; however this initial phase intends to focus on the US requirements.\n\n\n\nResponsibilities:\n\n· Implemented ETL solution using Informatica.\n· Analyzing Source System data.\n· Worked on Informatica – Source Analyzer, Warehouse Designer, Mapping Designer, Mapplet and Transformations.\n· Involved in importing Source/Target Tables from the respective databases.\n· Developed data Mappings between source systems and warehouse components using Mapping Designer\n· Used most of the Transformations such as Source Qualifier, Aggregator, Lookup, Filter, Sequence generator, Router, Update strategy etc.,\n· Extensively used ETL to load data from Oracle and Flat files to Data Warehouse. \n· Used Informatica Workflow Manager to create Sessions, Workflows to run with the logic embedded in the mappings.\n\n\n\n\nProject #1: MMC Global Technology Infrastructure, USA\n\n\nClient                           : MMC Global Technology Infrastructure, USA\nEnvironment                 : Informatica 8.5, Mercator (Data Stage TX), putty, FTP client, UNIX,\nTool                             : putty, WINSCP, SQL Developer, Sup tool (HP3000 Emulator)\nRole                             : EDI Analyst                            \nTeam Size\t               :  2\nTime period                  : May’10 to Aug’11           \nWork Location               : Offshore\n\n\nProject Description: \n\n              Marsh & McLennan Companies, Inc. (MMC) is a US-based global professional service and insurance brokerage firm.MMC is a diversified risk, insurance and professional services firm which is the world’s leading broker and risk adviser. The ultimate aim of the project is used as a data validation and conversion package for many of the above mentioned file types that is being sent and received to or from various internal systems.  It allows legacy systems to create or accept one file layout because it can accept basically any layout and convert it to the layout that either the legacy or client system is expecting.  The system is a control file driven system.\n\nResponsibilities:  \n\n\n· Analysis of business requirements.\n· Production support (40%), enhancement (60%).\n· Working on issues, enhancements, defects and Change Requests.\n· Involved in data validation and conversion package for many of the file types that is being sent & received.\n· Responsible for conversion and validation of data and for applying the business rule in order to obtain the required data from input file.\n· Used most of the Transformations such as Source Qualifier, Aggregator, Lookup, Filter, Sequence generator, Router, Update strategy etc.,\n· Extensively used ETL to load data from Oracle and Flat files to Data Warehouse. \n· Used Informatica Workflow Manager to create Sessions, Workflows to run with the logic embedded in the mappings.\n\n\n\n\nKey Achievements:\n\n· Established a positive and productive workplace by personally training, supporting and interacting with employees and managers at all levels of the organization.\n· Managed significant change and improvement initiatives.\n· Led help desk and support functions for 650 users in global locations; ensured immediate diagnosis and resolution of complex issues and updates.\n· Identified and capitalized on cost reductions by merging team functions, training employees and updating or transitioning to more cost-effective systems (such as involving automated systems).\n\nOther Achievements & Awards:\n\n· Awarded best Employee in 2012 for Coloplast.","annotation":[{"label":["Skills"],"points":[{"start":20366,"end":20368,"text":"SQL"}]},{"label":["Tools"],"points":[{"start":20358,"end":20363,"text":"WINSCP"}]},{"label":["Tools"],"points":[{"start":20351,"end":20355,"text":"putty"}]},{"label":["Operating_Systems"],"points":[{"start":20310,"end":20314,"text":"UNIX,"}]},{"label":["Tools"],"points":[{"start":20291,"end":20295,"text":"putty"}]},{"label":["Skills"],"points":[{"start":18470,"end":18472,"text":"SQL"}]},{"label":["Tools"],"points":[{"start":18462,"end":18467,"text":"WINSCP"}]},{"label":["Tools"],"points":[{"start":18455,"end":18459,"text":"putty"}]},{"label":["Skills"],"points":[{"start":18420,"end":18423,"text":"UNIX"}]},{"label":["Skills"],"points":[{"start":16315,"end":16317,"text":"SQL"}]},{"label":["Tools"],"points":[{"start":16308,"end":16312,"text":"OBIEE"}]},{"label":["Skills"],"points":[{"start":15483,"end":15492,"text":"SQL Server"}]},{"label":["Skills"],"points":[{"start":14548,"end":14557,"text":"SQL Server"}]},{"label":["Operating_Systems"],"points":[{"start":14534,"end":14538,"text":"UNIX,"}]},{"label":["Skills"],"points":[{"start":13301,"end":13310,"text":"SQL Server"}]},{"label":["Skills"],"points":[{"start":12404,"end":12406,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":12339,"end":12348,"text":"SQL Server"}]},{"label":["Operating_Systems"],"points":[{"start":12325,"end":12329,"text":"UNIX,"}]},{"label":["Skills"],"points":[{"start":9457,"end":9459,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":9394,"end":9403,"text":"SQL Server"}]},{"label":["Operating_Systems"],"points":[{"start":9380,"end":9384,"text":"UNIX,"}]},{"label":["Skills"],"points":[{"start":7523,"end":7525,"text":"SQL"}]},{"label":["Certifications"],"points":[{"start":3805,"end":3849,"text":"LOMA 280(Insurance Domain certification, US)\n"}]},{"label":["Certifications"],"points":[{"start":3764,"end":3801,"text":"Oracle Certified Associate (Oracle 9i)"}]},{"label":["Operating_Systems"],"points":[{"start":3730,"end":3734,"text":"Linux"}]},{"label":["Operating_Systems"],"points":[{"start":3724,"end":3727,"text":"UNIX"}]},{"label":["Operating_Systems"],"points":[{"start":3715,"end":3721,"text":"Windows"}]},{"label":["Tools"],"points":[{"start":3673,"end":3685,"text":"MS SQL Server"}]},{"label":["Tools"],"points":[{"start":3658,"end":3670,"text":"SQL developer"}]},{"label":["Tools"],"points":[{"start":3651,"end":3655,"text":"putty"}]},{"label":["Tools"],"points":[{"start":3643,"end":3648,"text":"WINSCP"}]},{"label":["Tools"],"points":[{"start":3637,"end":3640,"text":"TOAD"}]},{"label":["Skills"],"points":[{"start":3591,"end":3600,"text":"SQL Server"}]},{"label":["Skills"],"points":[{"start":3580,"end":3588,"text":"Oracle 9i"}]},{"label":["Skills"],"points":[{"start":3554,"end":3557,"text":"J2EE"}]},{"label":["Skills"],"points":[{"start":3548,"end":3551,"text":"Java"}]},{"label":["Skills"],"points":[{"start":3543,"end":3545,"text":"C++"}]},{"label":["Skills"],"points":[{"start":3540,"end":3540,"text":"C"}]},{"label":["Skills"],"points":[{"start":3532,"end":3537,"text":"PL/SQL"}]},{"label":["Skills"],"points":[{"start":3527,"end":3529,"text":"SQL"}]},{"label":["Tools"],"points":[{"start":3499,"end":3503,"text":"OBIEE"}]},{"label":["Tools"],"points":[{"start":3481,"end":3496,"text":"Business Objects"}]},{"label":["Tools"],"points":[{"start":3430,"end":3447,"text":"HP 3000   Emulator"}]},{"label":["Tools"],"points":[{"start":3404,"end":3427,"text":"Mercator (Data stage TX)"}]},{"label":["Tools"],"points":[{"start":3383,"end":3401,"text":"Informatica 8.6/9.1"}]},{"label":["Degree"],"points":[{"start":3180,"end":3220,"text":"Master’s in computer application (M.C.A.)"}]},{"label":["Skills"],"points":[{"start":2944,"end":2949,"text":"PL/SQL"}]},{"label":["Skills"],"points":[{"start":2939,"end":2941,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":2924,"end":2932,"text":"Oracle 9i"}]},{"label":["Tools"],"points":[{"start":2735,"end":2739,"text":"OBIEE"}]},{"label":["Tools"],"points":[{"start":2194,"end":2206,"text":"MS SQL Server"}]},{"label":["Tools"],"points":[{"start":1951,"end":1955,"text":"OBIEE"}]},{"label":["Tools"],"points":[{"start":1921,"end":1944,"text":"Mercator (Data stage TX)"}]},{"label":["Skills"],"points":[{"start":775,"end":777,"text":"SQL"}]},{"label":["Years_of_Experience"],"points":[{"start":400,"end":406,"text":"9 years"}]},{"label":["Email_Address"],"points":[{"start":68,"end":87,"text":"d.hota84@outlook.com"}]},{"label":["Mobile_No"],"points":[{"start":29,"end":44,"text":"+91 - 8050740371"}]},{"label":["Name"],"points":[{"start":0,"end":7,"text":"DEBJYOTI"}]}],"extras":null,"metadata":{"first_done_at":1564124102000,"last_updated_at":1564124102000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "http://dilshob.com \n\n+91-855-385-3505  \n\n  DILSHOB KK                                                                                                                              dilshob@live.in\n  \n\nOBJECTIVE  To develop my career in a software industry, where I will be a valuable team member, \ncontributing quality ideas and work for an organization where there is an ample scope \nfor individual as well as organization growth. \n\nPROFILE SUMMARY  Currently working as Technology Analyst, Infosys Ltd, Mangalore \n\nJava Web Developer with good analytical and problem-solving skills. \n\nHands on experience in Spring boot with Java 8. \n\nBachelor of Engineering in Computer Science from Anna-University. \n\nGood Work experience in Web-Services (REST & SOAP) and Microservices architecture. \n\nWeb application development and deployment in Agile methodology and TDD. \n\nSKILLS & ABILITIES  PROGRAMING LANGUAGES \nSpring Boot, Java 8, FTL, Jasmine, JAVA, J2EE, Web Services (REST/SOAP) HTML       \nCSS, JavaScript, SQL, Shell Script. \n\nSOFTWARE SUITS \nDatabase: SQL Server, MySQL, H2, PostgreSQL, MongoDB (Beginner) \n\n IDE’s – JBoss Data-Studio, Visual Studio Code, IBM RSA, Eclipse, IntelliJ \n Servers – JBoss EAP, WebSphere, Apache Tomcat \n Repositories – GitHub, Git, CVS, Maven, Nexus \n Utility Software – Jira, Jenkins, SonarQube, Visio, MS Office, WinSCP \n\n \n\nPLATFORMS \n\n Windows, Ubuntu \n\n  Accomplishments \n\n✓ Developed an interactive UI application which co-ordinate with various micro \nservices in backend. \n\n✓ Developed various micro services for various purposes to provide a better \nexperience for user UI application. \n\n✓ Good working knowledge of Spring boot with TDD. \n✓ Developed. \n✓ Worked in Agile environment, took part in requirement analysis, estimation, \n\ndesign, development, build and unit testing. \n✓ Participated in sprint planning, backlog grooming, retrospective and sprint \n\ndemo meetings. Direct client interaction through daily Stand up call. \n✓ Developed UI application which is replacement of google analytics. Which is \n\nfeatured as good as google analytics and have extra features according to \nclient needs. It has a fully responsive and clean UI. \n\nhttp://dilshob.com/\nhttp://dilshob.com/\n\n\nWORK EXPERIENCE \n\n2018 – DECEMBER \n\n \n\n \n\n \n\n \n\n2018 -MARCH \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n \n\n2017 – JANUARY \n\n \n\n \n\n \n\n \n\n \n\n \n\n2016 – AUGUST \n\n \n\n \n\n \n\n \n\n2016 -JANUARY \n\n \n\n \n\n TECHNOLOGY ANALYST: INFOSYS LTD (OCT 2014 – TILL DATE) \nLeading Mortgage Insurance in US \n\nPROJECT: USER CENTER \n\nInternal application used by sales people of Mortgage insurance. This is kind of admin \n\napplication for the Connect application below. Interact with set of services for \n\noperations.  \n\n[Spring boot, TDD, FTL, Agile Env., Selenium Automation Script, Jasmine, Mockito, \n\nMock MVC, Java 8, CSS, Jenkins, GitHub, Micro services, Sonar cube, JBoss EAP Server] \n\nPROJECT: CONNECT \n\nNew version of Origination application, Web application used by bank officials who are \n\nregistered users of Mortgage Insurance. This UI application interact with set of services \n\ninternally for data manipulations. This Next Gen application is built under complete \n\ntest-driven-development environment and with latest tech stack. Application uses \n\nSpring-boot as framework. FTL, SCSS, jQuery for frontend development. Used Jasmine \n\ntestcases and Selenium automation scripts for UI testing. Mockito is used for backend \n\ntests. Focused on Unit test, Integration test and Mock MVC in the backend. Used H2 \n\ndata base and Fakes for local application testing.   \n\n[Spring boot, TDD, FTL, Agile Env., Selenium Automation Script, Jasmine, Mockito, \n\nMock MVC, Java 8, CSS, Jenkins, GitHub, Micro services, Sonar cube, JBoss EAP Server] \n\nPROJECT: ORIGINATION \n\nWeb application used by bank officials who are registered users of Mortgage \n\nInsurance. Using this bank official can originate mortgage insurance request for \n\nproperty loans for their customers and can track the status of the application in all the \n\nstages. This interact with other SOAP/REST services for processing and managing \n\napplication. High customer engagement is enabled by Gamification, Rich email and \n\nSMS alert in different stages of application.  \n\n[JAVA EE, JSP, Servlets, JDBC, JavaScript, CSS, REST, SOAP, JSON, XML, LDAP, HTTP Basic \n\nAuthentication, jQuery, AJAX, JPA, UI plugins, EDMS, WebSphere] \n\nPROJECT: RATESTAR \n\nWeb application used by online users for checking mortgage insurance rates, \n\ncomparing it with FHA, getting rate quotes, Submitting rate quote to Mortgage \n\nInsurance and its subsidiaries. Used google analytics and SEO for website traffic \n\nmanagement.   \n\n[HTML5, CSS3, JavaScript, jQuery, Angular JS, LDAP, REST Services and JPA] \n\nPROJECT: INVESTIGATION \nWeb application used by internal users of Mortgage Insurance, used to investigate \nfrauds or any other issues can occur in stage of Claim payment. Customer tracks \ninvestigation status here and interact with lender to clear the issues for smooth claim \npayment.  \n[JSP, Servlet, Spring, JDBC, DB2] \n\n\n\n \n\n \n\n \n\n \n\nTRAINEE DEVELOPER: SPARTEQZ PVT LTD. (SEP13 - DEC13) \nRecruitment Management System [Technical Area:  Java with Struts2 & Hibernate] \n\n✓ Converted burden of recruitment into simple electronic system with ease of \naccess to both employers and Candidates. It manages candidate’s application \nfrom Registration till Employment. \n\nInventory Management System [Technical Area: Java, JSP, Servlet, SQL Server] \n✓ Reduced manpower in monitoring stocks each time it being processed. \n\nManagement of store data per client’s preference. Manages and reporting of \ndata with Real time alerts. \n\nCERTIFICATION \n\n/TRAINING \n\n ORACLE WORKFORCE DEVELOPMENT PROGRAM \n\n[TIMES TECHNOLOGIES, BANGALORE DEC12] \n\nJAVA Programing Language-JAVA SE6. \n\n[Technical Area: Core JAVA, J2EE, Struts2, Hibernate] \n\nEDUCATION \n\n \n\n Bachelor of Engineering in Computer Science,  \n\nDhana-Lakshmi Srinivasan College of Engineering and Technology, Chennai \n\nAnna University 2008-12. \n\nSuccessfully completed the course with 7.04 CGPA. \n\nACADAMIC PROJECT  A NEW DATA-MINING BASED APPROACH FOR NETWORK IDS [ Java, SQL Server] \n\nDomain/Platform: Network Security/ Java \n\nThe application is designed to monitor the packets in network and check each error \n\nand verify with database and alert the user about the attack. The goal is to identify \n\nand to cluster different alert belonging to specific attack instance which has been \n\ninitiated by an attacker at a certain point of time. \n\nPERSONAL PROFILE  DATE OF BIRTH 3rd December 1990. \n\nMARITAL STATUS/SEX/NATIONALITY SINGLE/MALE/INDIAN \n\nLANGUAGES ENGLISH, HINDI, MALAYALAM, TAMIL. \n\nADDRESS OF COMMUNICATION PRATHEEKSHA-HO, VELLIKULANGARA, \nVADAKARA, KOZHIKODE-673501. \n\n \n\nREFERENCE  CAN BE PROVIDED ON REQUEST \n\n DECLARATION  I hereby declare that the above-mentioned information is correct up to my knowledge \n\nand I bear the responsibility for the correctness of the same \n\nDILSHOB KK (-----SIGN-----)","annotation":[{"label":["Skills"],"points":[{"start":6186,"end":6189,"text":"Java"}]},{"label":["Skills"],"points":[{"start":6137,"end":6146,"text":"SQL Server"}]},{"label":["Skills"],"points":[{"start":6131,"end":6134,"text":"Java"}]},{"label":["Degree"],"points":[{"start":5861,"end":5883,"text":"Bachelor of Engineering"}]},{"label":["Skills"],"points":[{"start":5817,"end":5820,"text":"J2EE"}]},{"label":["Skills"],"points":[{"start":5811,"end":5814,"text":"JAVA"}]},{"label":["Skills"],"points":[{"start":5777,"end":5780,"text":"JAVA"}]},{"label":["Certifications"],"points":[{"start":5673,"end":5708,"text":"ORACLE WORKFORCE DEVELOPMENT PROGRAM"}]},{"label":["Skills"],"points":[{"start":5453,"end":5462,"text":"SQL Server"}]},{"label":["Skills"],"points":[{"start":5433,"end":5436,"text":"Java"}]},{"label":["Skills"],"points":[{"start":5163,"end":5166,"text":"Java"}]},{"label":["Skills"],"points":[{"start":4660,"end":4669,"text":"JavaScript"}]},{"label":["Skills"],"points":[{"start":4654,"end":4656,"text":"CSS"}]},{"label":["Skills"],"points":[{"start":4647,"end":4650,"text":"HTML"}]},{"label":["Skills"],"points":[{"start":4255,"end":4257,"text":"CSS"}]},{"label":["Skills"],"points":[{"start":4243,"end":4252,"text":"JavaScript"}]},{"label":["Skills"],"points":[{"start":4213,"end":4216,"text":"JAVA"}]},{"label":["Tools"],"points":[{"start":3666,"end":3671,"text":"GitHub"}]},{"label":["Tools"],"points":[{"start":3657,"end":3663,"text":"Jenkins"}]},{"label":["Skills"],"points":[{"start":3652,"end":3654,"text":"CSS"}]},{"label":["Skills"],"points":[{"start":3644,"end":3647,"text":"Java"}]},{"label":["Skills"],"points":[{"start":3614,"end":3620,"text":"Jasmine"}]},{"label":["Skills"],"points":[{"start":3568,"end":3571,"text":" FTL"}]},{"label":["Skills"],"points":[{"start":3490,"end":3491,"text":"H2"}]},{"label":["Skills"],"points":[{"start":3312,"end":3318,"text":"Jasmine"}]},{"label":["Skills"],"points":[{"start":3269,"end":3271,"text":"CSS"}]},{"label":["Skills"],"points":[{"start":3262,"end":3265,"text":" FTL"}]},{"label":["Tools"],"points":[{"start":2811,"end":2816,"text":"GitHub"}]},{"label":["Tools"],"points":[{"start":2802,"end":2808,"text":"Jenkins"}]},{"label":["Skills"],"points":[{"start":2797,"end":2799,"text":"CSS"}]},{"label":["Skills"],"points":[{"start":2789,"end":2792,"text":"Java"}]},{"label":["Skills"],"points":[{"start":2759,"end":2765,"text":"Jasmine"}]},{"label":["Skills"],"points":[{"start":2713,"end":2716,"text":" FTL"}]},{"label":["Years_of_Experience"],"points":[{"start":2427,"end":2446,"text":"OCT 2014 – TILL DATE"}]},{"label":["Operating_Systems"],"points":[{"start":1380,"end":1385,"text":"Ubuntu"}]},{"label":["Operating_Systems"],"points":[{"start":1371,"end":1377,"text":"Windows"}]},{"label":["Tools"],"points":[{"start":1346,"end":1351,"text":"WinSCP"}]},{"label":["Tools"],"points":[{"start":1335,"end":1343,"text":"MS Office"}]},{"label":["Tools"],"points":[{"start":1328,"end":1332,"text":"Visio"}]},{"label":["Tools"],"points":[{"start":1317,"end":1325,"text":"SonarQube"}]},{"label":["Tools"],"points":[{"start":1308,"end":1314,"text":"Jenkins"}]},{"label":["Tools"],"points":[{"start":1302,"end":1305,"text":"Jira"}]},{"label":["Tools"],"points":[{"start":1275,"end":1279,"text":"Nexus"}]},{"label":["Tools"],"points":[{"start":1268,"end":1272,"text":"Maven"}]},{"label":["Tools"],"points":[{"start":1263,"end":1265,"text":"CVS"}]},{"label":["Tools"],"points":[{"start":1258,"end":1260,"text":"Git"}]},{"label":["Tools"],"points":[{"start":1250,"end":1255,"text":"GitHub"}]},{"label":["Skills"],"points":[{"start":1089,"end":1095,"text":"MongoDB"}]},{"label":["Skills"],"points":[{"start":1077,"end":1086,"text":"PostgreSQL"}]},{"label":["Skills"],"points":[{"start":1073,"end":1074,"text":"H2"}]},{"label":["Skills"],"points":[{"start":1066,"end":1070,"text":"MySQL"}]},{"label":["Skills"],"points":[{"start":1054,"end":1063,"text":"SQL Server"}]},{"label":["Skills"],"points":[{"start":1012,"end":1023,"text":"Shell Script"}]},{"label":["Skills"],"points":[{"start":1007,"end":1009,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":995,"end":1004,"text":"JavaScript"}]},{"label":["Skills"],"points":[{"start":990,"end":992,"text":"CSS"}]},{"label":["Skills"],"points":[{"start":978,"end":981,"text":"HTML"}]},{"label":["Skills"],"points":[{"start":953,"end":964,"text":"Web Services"}]},{"label":["Skills"],"points":[{"start":947,"end":950,"text":"J2EE"}]},{"label":["Skills"],"points":[{"start":941,"end":944,"text":"JAVA"}]},{"label":["Skills"],"points":[{"start":932,"end":938,"text":"Jasmine"}]},{"label":["Skills"],"points":[{"start":926,"end":929,"text":" FTL"}]},{"label":["Skills"],"points":[{"start":919,"end":922,"text":"Java"}]},{"label":["Skills"],"points":[{"start":906,"end":916,"text":"Spring Boot"}]},{"label":["Degree"],"points":[{"start":636,"end":658,"text":"Bachelor of Engineering"}]},{"label":["Skills"],"points":[{"start":626,"end":629,"text":"Java"}]},{"label":["Skills"],"points":[{"start":516,"end":519,"text":"Java"}]},{"label":["Email_Address"],"points":[{"start":180,"end":194,"text":"dilshob@live.in"}]},{"label":["Name"],"points":[{"start":42,"end":52,"text":" DILSHOB KK"}]},{"label":["Mobile_No"],"points":[{"start":21,"end":37,"text":"+91-855-385-3505 "}]}],"extras":null,"metadata":{"first_done_at":1564243069000,"last_updated_at":1564292863000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "Gaddam vinay\n\nEmail ID: gaddamvinay87@gmail.com\n\nPhone no:+91-7259700519\n\nProfessional Summary:\n\n Having 4+ years in the field of Analytics and advanced business analytics (Data Science) to build predictive\nframework/ML to draw insights at scale for different aspects of business.\n\n Developing and implementing advanced analytics approaches including statistical modeling, machine learning\nprinciples etc. to answer business questions & drive actionable insights using SAS, R and Python.\n\n Hands on experience in Analytics - deriving key business impact from data and in creating algorithms,\nimplementing an analytical solution based on analysis with large, complex, structured and unstructured data\nsets to provide better insights.\n\n Analyzed business problems using data from different sources to provide strategic and actionable business\ninsights.\n\n Experience and knowledge in statistical and machine learning techniques: Linear/logistic regression, Decision\ntrees, Random Forest, SVM, Clustering, NLP,Text mining, neural networks etc.\n\n Deep understanding of statistical modeling/machine learning/data mining concepts.\n Done the Model implementation by using Python API’s\n Created the ensemble models, to choose the best model by using various algorithms.\n Able to leverage a heavy dose of mathematics and applied statistics with visualization and a healthy sense of\n\nexploration.\n Team player with a ‘can do’ attitude.\n High degree of flexibility, independent and proactive working style.\n Ability to work well under pressure and on multiple and conflicting priorities.\n Strong Experience on Sql,plsql\n Strong Knowledge on Hadoop Hive,Map Reducing\n Strong Knowledge on Qlikview\n Love to have passion to learn, think out of box and hard working.\n\nEmployment History:\n\nName of the Company Designation From To\n\nPARK Information Systems Limited Software Engineer Aug 2014 April 2017\n\nInfosys Limited Technology Analyst April 2017 Till Date\n\n\n\nAcademic Qualification:\n\n ME-Master of Engineering\nEaswari Engineering College,Chennai.\n\n BE-Bachelor of Engineering\nKamban Engineering College, Thiruvannamalai.\n\nTechnical Expertise:\n\nData Science Tools :R,Python,SAS  \n\nData Visualization Tools :R, Qlikview\n\nDatabases :Oracle, Teradata\n\nProgramming Language :Python,Sql,Plsql,Unix\n\nAchievements:\n\n Performance Excellence Award (“We Appreciated”) from Present Company for outstanding contribution to\nproject.\n\nSeminar Attended:\n\n Attended seminar on Big Data at JAIN UNIVERSITY, Bangalore.\n\n\n\nProject I:\n\nProject : Churn Analysis model\n\nObjective : prediction on  churn customers.\n\nTechnologies : Hadoop server, R- Studio and Python\n\nRoles and Responsibilities\n\n Gather the data to identify the customer behavior (1.5 million records and 60 variables)\n Identify the anomalies, missing values and outlier’s treatment \n Used various techniques to treat the missing values and outliers. \n Applied various transformation techniques to normalize the data.\n Created SMOT, Balanced data techniques (over sampling and under sampling) for classifying the data\n Applied the segmentation techniques to identify various patterns of customer behavior.\n Build the various machine learning model to choose the best model\n Created various cross validation techniques (Boot strapping and K-Fold techniques)\n Applied Ensemble methods to increase the accuracy of a model\n Created various metrics to check the model performance\n\nProject II: \n\nProject :Exploratory Data analysis for Billing and Complaints Data\n\nObjective : Identify the drivers for Billing and complaints data.\n\nTechnologies : Hadoop server, R- Studio and Python\n\nRoles and Responsibilities:\n\n Identify the key metrics for Billing and complaints data (Usage information, Roaming information, network\ninformation)\n\n Did data sanity check (Missing values and Outliers data)\n Created univariate analysis for all the key metrics\n Created various data patterns for Billing and complaints data\n Applied Cohort analysis to understand customer movement from month on month\n\n\n\n Identified relation between categorical variables by using Chi-Square test.\n Identified relation between numeric variables by using the correlation techniques\n Applied various Anova methods to finding relation between the variables\n Created multi variate analysis for few variables\n Generated reports for monthly and quarterly for all key metrics variables\n\nProject III:\n\nProject  : Delinquent customer analysis\n\nObjective :  Prediction on delinquent customer\n\nTechnologies : Hadoop server, R- Studio and Python\n\nRoles and Responsibilities:\n\n Gathered the 2.5 million records data along with 90 variables\n Created data patterns by using the key metrics are like Billing information and payment information \n Applied Roll rate analysis to create the bad flag\n Did driver analysis to understand most important variables\n Built the logistic regression analysis to find who are likely to become as a delinquent \n Created metrics are like Sensitivity, Specificity, Roc curve,AUC, F1 score, recall and precision \n Applied rigid regression and lasso regression to regularize the coefficients \n\nProject IV:\n\nProject: Next best offers for banking customers\n\nObjective: Opportunity to analyze customer banking to detect opportunities for personal banker to cross\nand up sell\n\nTechnologies : Hadoop server, R- Studio and Python\n\nRoles and Responsibilities:\n\n Understanding the business problem and pulled information.\n Information in transactional systems needed to be pulled together and analyzed.\n 2.7 million daily customers events.\n Building a predictive model to identify effective customers.\n Building a recommendation engine form a specific type of information filtering system techniques that\n\nattempts to present information items that are likely of interest user \n Validating a model by using cross validation methods are like grid search and boot strapping\n By using different validation metrics are like (KS Statistics, Gini , ROC curve , sensitivity, AUC, Somers D)\n Checking the model stability at testing phases and Out of time validation\n\n\n\n Built the various models to measure the model performance and model accuracy.\n\nTextControl1","annotation":null,"extras":null,"metadata":{"first_done_at":1564046459000,"last_updated_at":1564046648000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "Skills\n\tYears of Experience\n\tProject Name\n\tDuration\n\n\tJava , core java \n\t3.2 yrs \n\tVendor Data Management System (VDMS)\nPower Supply Management System (PSMS)\nEbert Services\n\t2016 – 2019\n\n\tSpring boot \n\t2 yrs \n\tVendor Data Management System (VDMS)\nPower Supply Management System (PSMS)\n\t2017 – 2019\n\n\tMicro service \n\t6 months \n\tVendor Data Management System (VDMS)\n\t2018 – 2019\n\n\tAngular 2 \n\t1 yrs \n\tVendor Data Management System (VDMS)\n\t2018 - 2019\n\n\n\nResume\n\n                                                                                                                                         \nEmail: nagasatish400@gmail.com\nMobile : +91- 9912797538                                                                                                     GANGULA NAGA SATISH\t                    \t\t\t\t    \t                                                                                                              \n \nOBJECTIVE\n\nTo seek the challenging position in Software industry that needs innovation, creativity, dedication and enable me to continue to work in a challenging and fast paced environment, leveraging my current knowledge and fostering creativity with many learning opportunities.\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t \nPROFESSIONAL SUMMARY\n\n· Having  3+ years of development experience in Java/J2EE technologies.\n· Worked in Designing, Developing and Maintaining web based applications using SERVLETS, JSP, JDBC, Spring(Core ,ORM, MVC), HTML , JavaScript,Microservices.\n· Having Experience in HIBERNATE.\n· Having Experience in Spring Boot.\n· Having Experience in Microservices.\n· Having Experience in Restful services.\n· Having Experience in Angular2 Framework.\n· Knowledge on Angular 2\n· Using AngularJS Configurations, Controllers, Services and Filters, Directives and Validations.\n· Developed and deployed applications in Windows environment.\n· Strong programming, coding, analytical, problem solving and conceptual skills.\n· Ability to work in a fast paced, collaborative environment.\n· Experience in Using SVN for code repository and as version control mechanism.\n· Self-motivated, hardworking and passionate to learn new technologies.\n\n\n\nWORK EXPERIENCE\n\n· Currently, Working as a Software Engineer in ZENSAR Technologies, Hyderabad from March-2016 to till date \n\n\nEDUCATION\n\n· B.Tech   from  affiliated college of JNTU Kakinada.\n\nSOFTWARE SKILLS \n\nOperating System\t\t\t:  Windows Family\nProgramming Languages\t\t:  Java, J2EE\t\nIDEs\t\t\t\t:  My Eclipse\nData Base\t\t\t\t:  MySQL, Oracle\t\nWeb-Programming Language\t\t:  Servlets, JSP \nWeb-Technologies \t\t\t:  HTML, AngularJs, Angular2\nFrameworks\t\t\t\t:  Spring and ORM (Hibernate) , Spring Boot,Microservices.\nWeb Services\t\t\t\t:  Restful services\nApplication/Web Servers                            :  Apache Tomcat\nTools\t\t\t\t\t:  Maven, SVN, Junit, Jira\n\n\n\nPROFESSIONAL EXPERIENCE\n\n\n\nProject #1\n\nTitle\t\t:  Vendor Data Management System (VDMS)\nEnvironment   \t: Java,Hibernate, Angular2, Restful web services, Spring Boot, Microservices\nRole\t\t:  Developer\nTeam Size\t:  8\nObjective           :\n                                 Vendor Data Management System (VDMS) is a central server application handles Parts and Transaction information from a warehouse and to E-Commerce Application, which launches a communication medium using secure web-service. Supportive administration with Transaction reports and Graphs are handled using report manager services.\n\nResponsibilities:\n\n· Understanding user requirements and system specifications.\n· Responsible for developing the Action Classes to serve the client Requests.\n· Developed Hibernate mapping.\n· Involved in writing Hibernate Query Language (HQL) to communicate with                    \nDatabase.\n· Prepared the unit test plan and Results.\n\n\nProject #2\n\nTitle\t\t:  PSMS (Power Supply Management System)\nClient\t\t:  UTILACOR, Australia\nEnvironment   \t: Java, JSP, Spring, Hibernate, AngularJs, Restful web services\nRole\t\t:  Developer\nTeam Size\t:  8\nObjective\t:   This is an energy based web application. The energy mainly had 3 stages i.e., Generating, Transporting, Distributing. This application is interact with the distributing and utilizing. This application is distributing to 3 types of customers i.e., Residential, Commercial and Industrial. The rate plan charges are different based on the customers and regions and timings. The main purpose of this application is to select the plan according to the usage and pay the bill based on the monthly charge which is used by the customer.\n\nResponsibilities:\n\n· Analyzing the given requirement.\n· Discussing with team about requirement.\n· Prepare analyzing steps and conditions before start coding and share these conditions to team through mail.\n· Developing AngularJS (Controller, Routers, and Service), Rest Controller and data base quires for given module.\n· Implementing Junit test cases.\n· Developing Reports module with join queries for filter search operations.\n· After complete the developing module, test the module thoroughly.\n· Debugging and identifying the issues and fixing.\n\n\nProject #3\n\n Title\t              :  Ebert Services\n Client\t\t:  Ebert Constructions \n Environment   \t:  Java, JSP, Spring, AngularJs, JavaScript ,HTML, CSS\n Role\t\t:  Developer\n Team Size\t:  6\nObjective \t:  EBERT SERVICES is one stop solution for all home needs like electrician, painting, plumbing, cleaning and etc. Be it the most miniscule or the most daunting Household .Service, Application helps to serve the clients with the best quality, highly reliable and recommended Independent Professionals and Enterprise Service Providers. With EBERT SERVICES on your fingertips, all your domiciliary needs will be taken care of! The software offers an extremely quick, easy and customized search option for your requirements. It also provides a real time attendance of all service providers around you, ensuring that there is someone to address your needs always.\nEbert Construction is a professional construction company. Ebert Construction to develop and manage the entire building package .They have carried out projects from the far north to the deep South\nResponsibilities:\n\n· Understanding user requirements and system specifications.\n· Responsible for developing the Action Classes to serve the client Requests.\n· Designing and Developing code for both front and back end.\n· Prepared the unit test plan and Results.\n· Delivering the product on time line.\n\nDeclaration\n\n\tI hereby declared that the above information is true and correct to the best of my \tknowledge.\n\nDate:                                                                                                                               \nPlace: Hyderabad                                                                                               (Naga Satish Gangula)\n \n\n\t\t\t\t\t   \t\t\t\t\t                 Page 1 of 5\n\n\n\t\t\t\t\t   \t\t\t\t\t                 Page 5 of 5","annotation":null,"extras":null,"metadata":{"first_done_at":1564129843000,"last_updated_at":1564129843000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "Gopal Gupta\nData Analyst\n\nI am meritorious quick learner and i want to grow in field of Data science. I will use my creative Skills,\ntechnologies that i am familiar with and innovative ideas that will benefit the organization in the long run,\nwhich will help me in building my professional career.\n\n1. Dynamic professional with 2.11 Years of experience in the field of network management in Huawei\nTechnologies as Incident manager.\n2. Proficient in handling various issues related to Fault management.\n3. Involved in issues related to NMS(Network Management System),EMS(Elementary Management System).\n4. Presently Working in Excelr Solutions as Data analyst Intern with 6 months Experience.\n\n5.Knowledge of R and Python Programming, Data science,Machine Learning Techniques.\n\n11/2018 - Till date\n\n12/2015 - 10/2018\n\nProjects  \nData Analytics\nPrediction of battery lifetime of the laptop\n\nObjective: To identify the health of the battery which depends upon many factors, which will help the     \ncompany to estimate the warranty and warranty cost of the battery.\nTechnology used: R programming, Tableau, Excel\nEDA: Normalization, missing value treatment, visualization using tableau\nModel building: Using Random forest, Prediction\n\nPredict the resale value of Toyota corolla\n\nObjective: To find the cost of the car using variables like engine capacity, years, kilometers completed and\nother factors, so the company can estimate the price of used cars.\nTechnology used: R programming, Excel\nEDA: Normalization, missing value treatment, visualization using tableau\nModel building: Using Multiple Linear Regression\n\nData analytics using machine learning to predict the strength of the concrete.\n\nObjective: The strength of concrete is predicted by using the different components of concrete such as \ncement, slag, water etc. This model helps in finding the maximum strength of the concrete.\nTechnology used: R programming, Excel\nEDA: Normalization, missing value treatment\nModel building: Using Random forest\n\nNetwork Surveillance Engineer\n\nBangalore,Karnataka,India\n+91 9739999067/+91 9935772324\ngopal_gupta517@yahoo.com\n\n www.linkedin.com/in/GopalGupta23071989\n\nJob Objective  \n\nProfile Summary  \n\nWork experience  \n\n\n\nhttps://www.visualcv.com/kuunztuxjy4/\n\n12/2015 - 10/2018\n\nCompany: Huawei Technologies India Pvt. Ltd / Project : Zain Saudi Arabia/Aircel\nRole and Responsibility:\n\nMonitoring different types of passive and Active alarms through Huawei Tools and Equipments.\nHands on Experience on various Tools like Ows,OneFM,U2000,M2000.\nCommunicating with field supervisors and Local Operation Center for restoration of sites and\nTroubleshooting of Alarms.Providing Access to field engineer for sites.\nPerforming Quality checks and Preparing Daily incident Reports for customer.\nAuditing Reports and Incident Tickets.Maintaining Database of sites.\nEHS (Environment and health safety) related issues of Field Engineer,Technicians and Riggers.All the\nworks related to Dispatcher Teams.\n\nIncident Management\nCompany: Huawei Technologies India Pvt. Ltd\nRole and Responsibility:\n\nHandles Incidents and User Service Requests end to end.\nCoordinates activities for High and critical lncidents.\nEscalation point for Service Delivery Managers and Service Provider Incident Managers.\nContact Service Delivery Manager to discuss details of the rejected escalation.\nNotify Service Provider Incident Manager of Master ticket.\nMonitor Service levels of lncident management function.\nDiscuss prioritization with Service Provider lncident Management.\nValidate use of Knowledge Base on Escalations.\n\nProgramming Languages\nand Tool R,Python,Machine learning,NLP,Text mining\n\nHuawei Tools\nM2000, U2000,N2000\n\nIncident Reporting Tools\nIctom, Remedy, Cacti Graph, Ows, Sdm, Onefm, Netcool,NetEco\n\nApplication and\nVisualization Tools Word,Excel,PowerPoint,Outlook,Tableau\n\nSkills  \n\nAchievements  \n\nAchieved Gate Score of 365 in GATE 2013.\nAchieved Gate Score of 414 in GATE 2014.\nAchieved Gate Score of 373 in GATE 2015.\nAwarded certificate for successfully completing training from Robonest on ‘Autobot’ .\nDesign of ‘Wireless MCB Fault Detector’ under the guidance of Mrs Priyanka Mahendru.\nSuccessfully completed industrial training from HAL, Lucknow on ‘Sukohi-30’ in year 2011.\nDelivered a seminar on the topic ‘Bone Growth Stimulation’ in the year 2011.\nReceived Star Award from Huawei for Best performer for March 2018.\n\nEducation  \n\n\n\nTextControl1\nhttps://www.visualcv.com/kuunztuxjy4/\n\n08/2008 - 07/2012\n\n07/2006 - 06/2007\n\n07/2004 - 06/2005\n\nBE (Electronics and Communication)\nGautam Buddh Technical University,Lucknow UP India\nPercentage: 71.82\n\nHSC(12th)\nUPKSSV UP India\n\nPercentage: 80.17\n\nSSC(10th)\nUPKSSV UP India\n\nPercentage: 64.67","annotation":[{"label":["Degree"],"points":[{"start":4534,"end":4567,"text":"BE (Electronics and Communication)"}]},{"label":["Tools"],"points":[{"start":3772,"end":3777,"text":"NetEco"}]},{"label":["Tools"],"points":[{"start":3764,"end":3770,"text":"Netcool"}]},{"label":["Tools"],"points":[{"start":3757,"end":3761,"text":"Onefm"}]},{"label":["Tools"],"points":[{"start":3751,"end":3754,"text":" Sdm"}]},{"label":["Tools"],"points":[{"start":3747,"end":3749,"text":"Ows"}]},{"label":["Tools"],"points":[{"start":3734,"end":3744,"text":"Cacti Graph"}]},{"label":["Tools"],"points":[{"start":3726,"end":3731,"text":"Remedy"}]},{"label":["Tools"],"points":[{"start":3719,"end":3723,"text":"Ictom"}]},{"label":["Tools"],"points":[{"start":3687,"end":3691,"text":"N2000"}]},{"label":["Tools"],"points":[{"start":3681,"end":3685,"text":"U2000"}]},{"label":["Tools"],"points":[{"start":3674,"end":3678,"text":"M2000"}]},{"label":["Skills"],"points":[{"start":3648,"end":3658,"text":"Text mining"}]},{"label":["Skills"],"points":[{"start":3644,"end":3646,"text":"NLP"}]},{"label":["Skills"],"points":[{"start":3627,"end":3642,"text":"Machine learning"}]},{"label":["Skills"],"points":[{"start":3620,"end":3625,"text":"Python"}]},{"label":["Skills"],"points":[{"start":3618,"end":3618,"text":"R"}]},{"label":["Tools"],"points":[{"start":2534,"end":2538,"text":"M2000"}]},{"label":["Tools"],"points":[{"start":2528,"end":2532,"text":"U2000"}]},{"label":["Tools"],"points":[{"start":2518,"end":2520,"text":"Ows"}]},{"label":["Email_Address"],"points":[{"start":2093,"end":2116,"text":"gopal_gupta517@yahoo.com"}]},{"label":["Mobile_No"],"points":[{"start":2078,"end":2091,"text":"+91 9935772324"}]},{"label":["Mobile_No"],"points":[{"start":2063,"end":2076,"text":"+91 9739999067"}]},{"label":["Skills"],"points":[{"start":1079,"end":1079,"text":"R"}]},{"label":["Skills"],"points":[{"start":746,"end":761,"text":"Machine Learning"}]},{"label":["Skills"],"points":[{"start":733,"end":744,"text":"Data science"}]},{"label":["Skills"],"points":[{"start":713,"end":718,"text":"Python"}]},{"label":["Skills"],"points":[{"start":707,"end":707,"text":"R"}]},{"label":["Years_of_Experience"],"points":[{"start":328,"end":337,"text":"2.11 Years"}]},{"label":["Skills"],"points":[{"start":88,"end":99,"text":"Data science"}]},{"label":["Name"],"points":[{"start":0,"end":10,"text":"Gopal Gupta"}]}],"extras":null,"metadata":{"first_done_at":1564130613000,"last_updated_at":1564130613000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "Gurpreet Kaur\ngurpreetkaurit26@gmail.com ||linkedin\n+91-6361101256\n\n\n\nOBJECTIVE\n\nTo work as a challenging Java Professional, who would like to utilize her technical knowledge and at the same time, ready to master some new technologies for the development of the application.\n\n\n\nCAREER SUMMARY\n\n\n·  2.9 years’ experience in analysis, design, development, documentation, implementing, testing and integration of scalable, multi-tier distributed enterprise business applicationsdeveloped using technologies Java/ J2EE.\n· Strong Knowledge on Application Development using Software Development\nLife Cycle SDLC using various methodologies like Agile/Scrum.\n·  Strong experience in working with J2EE technologies like Hibernate, JDBC, WEB SERVICES and STRUTS2.\n· Strong Experience in spring framework such as Spring MVC.\n· Good knowledge in OOPS concept.\n· Strong knowledge of various database such as Oracle and MySQL.\n· Strong knowledge on test driven development (TDD) using Mockito framework.\n· Hands on experience on build and continuous integration tools such as Jenkins, Maven.\n· Experience in designing front-end interfaces using Html, CSS, Angular JS and JavaScript.\n· Hands on experience in using Tomcat, Oracle Web Logic servers.\n· Hands on experience in using Eclipse, Visual Studio, Toad and Oracle SQL developer.\n· Experience in using GitBlit for managing, viewing, and serving Git repositories.\n· Experience in using Putty.\n· Good communication skills with ability to grasp new things quickly.\n· Good team player with interpersonal skills.\n\n\nTECHNICAL SKILLS\n\n\nOperating System – Windows 10, Windows 7, Windows 8\nLanguages – Java/J2EE, JSP, PL/SQL.\nDatabase – Oracle, MySQL.\nVersion Control Tools –SVN, Git, TortoiseGit\nWeb Technology-Html, CSS, JavaScript, Angular JS, JQuery\n  Bug Tracking Tool – Jira, HP QC, VSTS, Mantis.\n  Build Tools-Maven\n  Source Control Tool-Git, SVN, Tortoise Git\n  IDE- Eclipse, Visual Studio, IBM Rational Application Developer\nWeb Application Server - Apache Tomcat\nOther Software-JUnit, Sonar, Mockito.\n\n\n\nWORK EXPIRIENCE\n\n\nSenior Systems Engineer                                                                              July 2016 - Present\nInfosys Limited, Gurgaon\n\nJava/J2EE Developer\n\nClient – Telstra(July 2016-Aug 2018)\n\nDescription : This project is internally used by Telstra for the configuration changes required by Telecom Industry to provide proper services to their customers.\n\nRESPONSIBILITIES –\n\n· Understanding the requirement and functionality of the system\n· Unit testing of all the layers by mocking the external layers using Mockito framework.\n· Responsible for coding Action Classes using Struts 2.\n· Implementing the Spring Framework for selected modules\n· Responsible for Different Defect Fixes.\n· Responsible for coding of DAO classes and hibernate for accessing the Database.\n· Writing UI and different server side validations for the owned use cases using AngularJS, JavaScript, JQuery and JSP.\n· Sanity Testing and Bug fixing.\n· Deployment of the latest code from GIT to the server or SIT environment through Jenkins and putty.\n· Mentoring new joiners in the team.\n· Ensuring smooth delivery of the project.\n· Responsible for writing SQL queries for the newly created tables and modifying the old ones.\n\nTechnologies Used:\n· Core Java, Spring MVC, JQuery, MySQL, SVN, Eclipse IDE, Angular JS, TortoiseGit, Visual Studio, SQL, VSTS, Hibernate, Mockito, HP-QC, Web Services.\n\nClient – TDSCPC India (Sep 2018-Present)\n\nDescription : It is web site development project of Income Tax. This website is developed for filling TDS and TCS Certificate, 26AS and Form 13, refund.\n\nRESPONSIBILITIES –\n\n· Understanding the requirement and functionality of the system\n· Responsible for Different Defect Fixes.\n· Responsible for coding of DAO classes and hibernate for accessing the Database.\n· Writing UI and different server side validations for the owned use cases using JavaScript, JQuery and JSF.\n· Sanity Testing and Bug fixing.\n· Mentoring new joiners in the team.\n· Ensuring smooth delivery of the project.\n· Responsible for writing SQL queries for the newly created tables and modifying the old ones.\n\nTechnologies Used:\n· Core Java, Spring MVC, JQuery, JSF, MySQL, SVN, IBM Rational Application Developer, Oracle, Hibernate, JIRA,Mantis, Web Services.\n\n\t\t\nEDUCATION/ACADEMICS\n\n\n\nEducational Qualification\tUniversity/College/School\tYear\t    Marks\n\n\nBachelor of Technology (IT)A.K.T.U/Pranveer Singh Institute2012-2016\t85.5%\n\tOf Technology\n\n12th  CBSE/Harmilap Mission School,2010-2012\t79.6%\nKanpur\n\nCBSE/Harmilap Mission School,\t2010\t9 CGPA\n10thKanpur\n\n\n\n\n\nCO-CURRICULAR ACTIVITIES / ACHIEVEMENTS\n\n· Was nominated for the best debutant award in my project unit\n· Awarded with an Insta Award as a token of appreciation for various contributions to my project.\n· Awarded with Certificate of Appreciation and medal for achieving 6th rank in the University.\n\nINTERESTS / HOBBIES\n\n· Music\t\n· Travelling\n· Photography \t\t\n\nPERSONAL INFORMATION\n\n       Date of Birth -          26thOctober 1993\n       Nationality -             Indian\n       Gender-                    Female\n       Languages known - English, Punjabi, Hindi","annotation":[{"label":["Degree"],"points":[{"start":4412,"end":4438,"text":"Bachelor of Technology (IT)"}]},{"label":["Skills"],"points":[{"start":4294,"end":4299,"text":"Mantis"}]},{"label":["Skills"],"points":[{"start":4270,"end":4275,"text":"Oracle"}]},{"label":["Tools"],"points":[{"start":4229,"end":4231,"text":"SVN"}]},{"label":["Skills"],"points":[{"start":4222,"end":4226,"text":"MySQL"}]},{"label":["Skills"],"points":[{"start":4209,"end":4214,"text":"JQuery"}]},{"label":["Skills"],"points":[{"start":4191,"end":4194,"text":"Java"}]},{"label":["Skills"],"points":[{"start":3940,"end":3945,"text":"JQuery"}]},{"label":["Skills"],"points":[{"start":3928,"end":3937,"text":"JavaScript"}]},{"label":["Skills"],"points":[{"start":3412,"end":3418,"text":"Mockito"}]},{"label":["Tools"],"points":[{"start":3395,"end":3398,"text":"VSTS"}]},{"label":["Tools"],"points":[{"start":3362,"end":3372,"text":"TortoiseGit"}]},{"label":["Skills"],"points":[{"start":3350,"end":3359,"text":"Angular JS"}]},{"label":["Tools"],"points":[{"start":3332,"end":3334,"text":"SVN"}]},{"label":["Skills"],"points":[{"start":3325,"end":3329,"text":"MySQL"}]},{"label":["Skills"],"points":[{"start":3317,"end":3322,"text":"JQuery"}]},{"label":["Skills"],"points":[{"start":3299,"end":3302,"text":"Java"}]},{"label":["Skills"],"points":[{"start":2958,"end":2960,"text":"JSP"}]},{"label":["Skills"],"points":[{"start":2947,"end":2952,"text":"JQuery"}]},{"label":["Skills"],"points":[{"start":2935,"end":2944,"text":"JavaScript"}]},{"label":["Skills"],"points":[{"start":2587,"end":2593,"text":"Mockito"}]},{"label":["Skills"],"points":[{"start":2215,"end":2218,"text":"J2EE"}]},{"label":["Skills"],"points":[{"start":2210,"end":2213,"text":"Java"}]},{"label":["Skills"],"points":[{"start":2033,"end":2039,"text":"Mockito"}]},{"label":["Skills"],"points":[{"start":2026,"end":2030,"text":"Sonar"}]},{"label":["Skills"],"points":[{"start":2019,"end":2023,"text":"JUnit"}]},{"label":["Skills"],"points":[{"start":1990,"end":2002,"text":"Apache Tomcat"}]},{"label":["Tools"],"points":[{"start":1886,"end":1897,"text":"Tortoise Git"}]},{"label":["Tools"],"points":[{"start":1881,"end":1883,"text":"SVN"}]},{"label":["Tools"],"points":[{"start":1876,"end":1878,"text":"Git"}]},{"label":["Tools"],"points":[{"start":1848,"end":1852,"text":"Maven"}]},{"label":["Tools"],"points":[{"start":1826,"end":1831,"text":"Mantis"}]},{"label":["Tools"],"points":[{"start":1820,"end":1823,"text":"VSTS"}]},{"label":["Tools"],"points":[{"start":1813,"end":1817,"text":"HP QC"}]},{"label":["Tools"],"points":[{"start":1807,"end":1810,"text":"Jira"}]},{"label":["Skills"],"points":[{"start":1778,"end":1783,"text":"JQuery"}]},{"label":["Skills"],"points":[{"start":1766,"end":1775,"text":"Angular JS"}]},{"label":["Skills"],"points":[{"start":1754,"end":1763,"text":"JavaScript"}]},{"label":["Skills"],"points":[{"start":1748,"end":1751,"text":" CSS"}]},{"label":["Skills"],"points":[{"start":1743,"end":1746,"text":"Html"}]},{"label":["Tools"],"points":[{"start":1716,"end":1726,"text":"TortoiseGit"}]},{"label":["Tools"],"points":[{"start":1711,"end":1713,"text":"Git"}]},{"label":["Tools"],"points":[{"start":1706,"end":1708,"text":"SVN"}]},{"label":["Skills"],"points":[{"start":1676,"end":1680,"text":"MySQL"}]},{"label":["Skills"],"points":[{"start":1668,"end":1673,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":1649,"end":1654,"text":"PL/SQL"}]},{"label":["Skills"],"points":[{"start":1644,"end":1646,"text":"JSP"}]},{"label":["Skills"],"points":[{"start":1638,"end":1641,"text":"J2EE"}]},{"label":["Skills"],"points":[{"start":1633,"end":1636,"text":"Java"}]},{"label":["Operating_Systems"],"points":[{"start":1611,"end":1617,"text":"Windows"}]},{"label":["Operating_Systems"],"points":[{"start":1600,"end":1606,"text":"Windows"}]},{"label":["Operating_Systems"],"points":[{"start":1588,"end":1594,"text":"Windows"}]},{"label":["Tools"],"points":[{"start":1385,"end":1387,"text":"Git"}]},{"label":["Tools"],"points":[{"start":1342,"end":1344,"text":"Git"}]},{"label":["Skills"],"points":[{"start":1298,"end":1303,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":1208,"end":1213,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":1157,"end":1166,"text":"JavaScript"}]},{"label":["Skills"],"points":[{"start":1142,"end":1151,"text":"Angular JS"}]},{"label":["Skills"],"points":[{"start":1136,"end":1139,"text":" CSS"}]},{"label":["Skills"],"points":[{"start":1131,"end":1134,"text":"Html"}]},{"label":["Skills"],"points":[{"start":1071,"end":1075,"text":"Maven"}]},{"label":["Skills"],"points":[{"start":971,"end":977,"text":"Mockito"}]},{"label":["Skills"],"points":[{"start":906,"end":910,"text":"MySQL"}]},{"label":["Skills"],"points":[{"start":895,"end":900,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":688,"end":691,"text":"J2EE"}]},{"label":["Skills"],"points":[{"start":510,"end":513,"text":"J2EE"}]},{"label":["Skills"],"points":[{"start":504,"end":507,"text":"Java"}]},{"label":["Years_of_Experience"],"points":[{"start":298,"end":306,"text":"2.9 years"}]},{"label":["Skills"],"points":[{"start":106,"end":109,"text":"Java"}]},{"label":["Mobile_No"],"points":[{"start":52,"end":65,"text":"+91-6361101256"}]},{"label":["Email_Address"],"points":[{"start":14,"end":39,"text":"gurpreetkaurit26@gmail.com"}]},{"label":["Name"],"points":[{"start":0,"end":12,"text":"Gurpreet Kaur"}]}],"extras":null,"metadata":{"first_done_at":1564205988000,"last_updated_at":1564205988000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "CURRICULUM VITAE\nJyoti Ranjan Mohanty                                             \nSenior Consultant at Oracle SSI                                       \n\n\n\t   Professional Summary\n\t\n\n\t· Currently working with Oracle Solutions Services India as a Senior Consultant.\n· Having 7 Years of Experience in top IT MNCs on various projects Development, Enhancement,POC and Customization projects .\n· Having  3 years of relevant    experience on  below aspects of Hadoop ecosystem - Spark (Spark-SQL, Pyspark) , HDFS, Hive, Pig, Sqoop,Oracle  big  data cloud  service(Cloudera), AWS EMR – (S3,Redshift) environment, Python .\n\n· Having 4 years of IT experience in other areas like Java, SQL, PL/SQL, Web services (REST and SOAP) , Oracle cloud products and services etc.\n· Experience in data analysis, data model design, web application development, testing,  deployment etc..Understanding, testing technical and functional issues, debugging and solving them.\n\n· Have very good understanding of software development life cycles along with Agile methodology.\n· Have used operating systems like Windows, Linux and Unix.\n\n· Very good in consulting, onsite offshore interaction, customer interaction, bidding/estimation, leading team, presentation and interest to travel. Was involved in recruitment activities.\n\n· Good interpersonal and communication skills, commitment, result oriented professional.\n\n· Hardworking with a quest and zeal to learn new technologies and undertake challenging tasks.\n\n\n\t  Technical Skills\n\t\n\n\tFramework/Languages    :      Spark 2.2 (Pyspark,Spark-SQL), Hive, Pig, Sqoop,Oozie,\n                                                   Python,HDFS File system,S3 File System, AWS-Redshift,     \n\n                                                   Java, SQL/PLSQL, MySQL,Web-services, Java script\n\nDatabases\n\n     :      MySQL, Oracle, DBCS, SQL sever\n\nTools\n\n\n     :      GIT, SVN, SSH Client, Putty,Soap UI, SQL Developer, PVCS,QC\n\n                                                    ,  \n\nGUI Tools\n\n     :      Pycharm ,Jupyter notebook, JDeveloper ,Eclipse ,HUE\n\nCloud technologies           :      Oracle cloud services, AWS\n\nServers                               :      Weblogic server 11g and 12c, JCS , SOA server, Tomcat\n\n\n\n\n\t  Professional  Experience     \n\t\n\n\t· Currently working as Senior Consultant in Oracle SSI since May 2016. Currently working as a Spark developer.\n\n· Worked as ADF and SOA Technical Consultant in Tech Mahindra since March  2015 to May 2016.\n\n· Worked as ADF Technical Consultant for IGATE Global Solutions since July 2014 to March 2015.\n\n· Worked as ADF & OAF Technical Consultant for Wipro Technologies from February 2012 to July 2014.\n\n\n\tEducation\n\t· BE in Computer science from Sapthagiri college of Engineering, Anna University Coimbatore  with 80.0%\n· Intermediate from CHSE, Orissa with 75.6%\n· 10th from HSC, Orissa with 76.5%\n\n\n\n1.  World Health Organization(WHO)\nDescription:  \n\nWorld Health Organization is an UN health care organization which is solely responsible for standardizing healthcare across world and helping the countries in overcoming difficult situations like disease outbreaks. \nTeam Size: 12\nRole:\nWorked as a senior consultant from start to end of the project. Analysis of Requirement specification, Preparing Technical Design documentation. Was actively involved in customer interaction for clarification, presentation/demo and project estimation. Got spot awards recognition for quality project delivery.\n\nProject details:\n\nThe IT entity is focused on building out the cloud-based architecture (Amazon S3, Redshift supported via EMR/Spark) and populating the S3 data lake landing and staging environments with required information housed together.Each of the campaign data included various kinds of data e.g. campaign location info , diesease outbreaks info, symptoms, number of deaths, doctors/staff applied for position, interviewed, employed ,campaign cost involved, effectiveness metric etc.\n S3 Object Storage to provide the underlying data lake enablement. It will  include three architecture hops: Landing, Staging and Warehouse.\n\nLanding Zone – Data is mainly consumed from different sources (HR, Payroll, Taleo, Custom application DB)  as a CSV files in S3 location.\n\nStaging Zone – The preprocessor job used for data analyzing and cleansing based on S2T doc from Landing Zone to Staging.\n\nWarehouse – Dimension and Fact structure was created on S3 bucket after applying transformations on top of staging data. Applying CDC logic to find insert, update, delete and unchanged record, distributed across History, Current Backup and Current dimension.\n\nRedshift – Tables will be created on top of S3 History dimension for the end system.\n\nSkills: Spark SQL, Pyspark, Python\n\nEnvironment: AWS EMR, S3 and Redshift\n2.  eHealth NSWH, Sydney\nDescription:  \n\neHealth is the IT division of NSW health, which is a government organization present in Sydney, NSW. \nTeam Size: 10\nRole:\nCurrently working as a spark developer. Was actively involved in customer interaction for clarification, presentation/demo, providing training, supporting the onsite team on issues and project estimation. \n\nProject details:\nCloud data lake is maintained in Hive Database and downstream applications such as PI, PING consume data in different formats like JSON, structured data etc. as per requirement. The main aim of project is to setup a generalized framework and act as an anchor to ingest data from multiple sources and provide insights of commercial value to the users.\nData Ingestion – Data is mainly consumed from different sources (Oracle RDBMS and different entities , Third party vendors) as a flat files moved from edge node to hdfs.\n\nRaw Layer – The external table is created on top of flat files without any normalization.The managed table is created from the external table by applying proper metadata based on the mapping document.\n\nCurated Layer – On top of the raw layer, we will introduce a data quality rule framework layer which will check data quality for rules such as null check, duplicate check, count check. special character check etc. by reading data in external data layer, fed from the external sources as it is, and applying the rules on it. The records passing the screening will be ingested into the system data lake for further processing.\n\nSDS – In Service Data Store, the JSON files (API: Member, Benefits, Claims) will be generated from the curated layer based on the member and their eligibility coverage which will be sent to end system. SDS automation script helps in testing each object from a JSON file and produce test report. Using Python script, it automatically does count check and data comparison check for each object. Mapping Dictionary checks makes sure the attribute naming matched the mapping document and API swagger.\n\nRejected Data - The data rejected in data quality framework will be sent to the user with rejection reasons for correction. Also, an audit will be maintained to track the number of records passing or failing from it.\n\nSkills: Hive, Spark SQL, Pyspark, Shell\n\nEnvironment: Cloudera (Oracle big data cloud service)\n\n3.  Metro Cash And Carry, Russia\nDescription:  \nMetro Cash And Carry is a whole sale organization and for recruitment and managing their IT operation they use various oracle cloud products. \nTeam Size: 3\nRole\nWorked as a hadoop developer for various data transformation and analysis.\n\nProject details:\nObjective: It is basically an Ab Inisio ETL tool to hadoop migration project. All information with respect to billing, asset,sales,revenue, address,footfalls, products and site related information was extracted from different source systems and fed to CRM systems to increase customer satisfaction, Customer Order placing and Customer Order Management. This also enabled Metro to get insights from data applying many machine learning algorithms. Followed by the analysis many decisions were taken like based on purchases made providing offers, better membership cards ,notifications for product offer  to good customers etc\nData Ingestion Framework- This framework would also take care of putting data from RDBMS system to HDFS filestystem using sqoop . Then hive tables were created for data analysis for end systems which uses hadoop. At the same time the processed data after PIG ETL process was also exported back to oracle RDBMS .These data are specifically used by end systems which still uses oracle RDBMS.\nSkills: Hive, Apache Pig, Oozie, Sqoop, Shell\n\nEnvironment: Cloudera (Oracle Big Data Cloud Service)\n4.  Biassyala / External Assignment Management System(Swedbank)\nDescription: \n\nBiassyala is a PaaS extension hosted on JCS (java cloud services) provides the solution for automated Request generation and approval process. Swedbank had paper based approval system which was replaced by our system. \nTeam Size: 3\nRole:\n\n        Worked on Developing Dynamic Request generation and document approval. Historical Data load, Scheduled processes. Done the built and Deployment of application in the development environments during each release. Fixed many critical issues, when asked to be deployed in the short period of time. Used technologies like Fusion HCM, Web Proxies, JCS, ADF 12c, ADF-BC, ADF Rich Faces, Task flows, Data Control, Java, JSF, Oracle12c.\n5.  Equity bank, Kenya\n\nDescription:  \n\nEquity bank is a big bank in Kenya. I was responsible for developing banking application using BPM, ADF etc. I was involved in creating custom ADF screens which were used in Human tasks in BPM approval process.\nTeam Size: 15\nRole\nWorked as a senior consultant in designing, analysis of requirement specification, preparing technical design documentation, development of UIs in ADF. Basically, I used BPM data controls as backend and SOAP web services are used for some data retrieval\n\n6.   Accounting and Corporate Regulatory Authority(ACRA), Singapore\n Description:\n\n          The Accounting and Corporate Regulatory Authority (ACRA) is the national regulator of business entities and public accountants in Singapore. ACRA also plays the role of a facilitator for the development of business entities and the public accountancy profession.\nTeam Size: 12\n\nRole:\nWorked as a java and ADF Developer. Involved in fresh development and Defect fixing activities. Responsible for quality deliverables.\n\n7.  Cisco(Cisco_CVCM_QTC_CCW_ASE_OM), California\n\nDescription:\n\n     In cisco I was working as a SOA and BAM developer. Used technologies like SOA 11g ,12c and BAM 11g and 12c. SOA was used to integrate e-business suite order management system and push data into BAM objects. I was also responsible for creating BAM data objects, real time Reports, Alerts, export import BAM objects etc.\nTeam Size: 4\nRole:\nWorked as a middleware developer and BAM developer. Involved in fresh development and lead the team. Responsible for quality deliverables.\n8.   CISCO Advanced Services Project (CISCO-CSSD-AS-PROJECT), California\nDescription:\n\n          The project was basically about accounting the revenue i.e. creating budgets and recognizing revenues.\n\nTeam Size: 15\n\nRole:\nAnalyzing Functional document and creating technical document. The project is based on Oracle Projects module which is combination of modules like Project management, resource management, Billing, Costing etc. Have worked on creating custom ADF screens and used oracle 11g as back end for the same. Used java, SQL queries, PL/SQL. Worked on developing custom OAF pages from scratch and have done enhancements on existing applications. \n\tPersonal \n\nInformation\n\t· Name                     : Jyoti Ranjan Mohanty\n\n· Date of Birth          :  29th June 1990\n\n· Marital Status         :  Married\n\n· Current Address      :  Flat 301, Jeevanadiganga apartment, Gururaja   \n\n                                             Layout, Doddanekundi, Marthahalli, Bengaluru,   \n\n                                             Karnataka, India, Pin 560037\n\n· Permanent Address :   c/o- Maheswar Mohanty ,\n                                Bhitaradiga patna, Bhuban,\n                                Dhenkanal, Orissa, India, pin-759017 \n\n· Languages known    :  English, Hindi, Oriya\n\n\n\n\nDeclaration:\n\nI hereby declare that the above-mentioned information is correct up to my knowledge and I bear the   responsibility for the correctness of the above-mentioned particulars.\n\nPlace: Bengaluru\n\nJyoti Ranjan Mohanty\n PROJECT PROFILE�\n�","annotation":[{"label":["Name"],"points":[{"start":12389,"end":12408,"text":"Jyoti Ranjan Mohanty"}]},{"label":["Name"],"points":[{"start":11608,"end":11627,"text":"Jyoti Ranjan Mohanty"}]},{"label":["Skills"],"points":[{"start":11205,"end":11210,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":9302,"end":9307,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":9291,"end":9294,"text":"Java"}]},{"label":["Skills"],"points":[{"start":8527,"end":8532,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":8490,"end":8494,"text":"Sqoop"}]},{"label":["Skills"],"points":[{"start":8483,"end":8487,"text":"Oozie"}]},{"label":["Skills"],"points":[{"start":8478,"end":8480,"text":"Pig"}]},{"label":["Skills"],"points":[{"start":8465,"end":8468,"text":"Hive"}]},{"label":["Skills"],"points":[{"start":8166,"end":8169,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":7109,"end":7114,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":7070,"end":7076,"text":"Pyspark"}]},{"label":["Skills"],"points":[{"start":7059,"end":7063,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":7053,"end":7056,"text":"Hive"}]},{"label":["Skills"],"points":[{"start":6630,"end":6635,"text":"Python"}]},{"label":["Skills"],"points":[{"start":5595,"end":5600,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":5212,"end":5215,"text":"Hive"}]},{"label":["Skills"],"points":[{"start":4767,"end":4769,"text":"AWS"}]},{"label":["Skills"],"points":[{"start":4746,"end":4751,"text":"Python"}]},{"label":["Skills"],"points":[{"start":4737,"end":4743,"text":"Pyspark"}]},{"label":["Skills"],"points":[{"start":4726,"end":4730,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":3606,"end":3610,"text":"Spark"}]},{"label":["Degree"],"points":[{"start":2697,"end":2718,"text":"BE in Computer science"}]},{"label":["Skills"],"points":[{"start":2374,"end":2378,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":2324,"end":2329,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":2135,"end":2137,"text":"AWS"}]},{"label":["Skills"],"points":[{"start":2112,"end":2132,"text":"Oracle cloud services"}]},{"label":["Tools"],"points":[{"start":1939,"end":1940,"text":"QC"}]},{"label":["Tools"],"points":[{"start":1934,"end":1937,"text":"PVCS"}]},{"label":["Tools"],"points":[{"start":1919,"end":1931,"text":"SQL Developer"}]},{"label":["Tools"],"points":[{"start":1910,"end":1916,"text":"Soap UI"}]},{"label":["Tools"],"points":[{"start":1904,"end":1908,"text":"Putty"}]},{"label":["Tools"],"points":[{"start":1892,"end":1901,"text":"SSH Client"}]},{"label":["Tools"],"points":[{"start":1887,"end":1889,"text":"SVN"}]},{"label":["Tools"],"points":[{"start":1882,"end":1884,"text":"GIT"}]},{"label":["Skills"],"points":[{"start":1851,"end":1859,"text":"SQL sever"}]},{"label":["Skills"],"points":[{"start":1845,"end":1848,"text":"DBCS"}]},{"label":["Skills"],"points":[{"start":1837,"end":1842,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":1830,"end":1834,"text":"MySQL"}]},{"label":["Skills"],"points":[{"start":1794,"end":1804,"text":"Java script"}]},{"label":["Skills"],"points":[{"start":1780,"end":1791,"text":"Web-services"}]},{"label":["Skills"],"points":[{"start":1774,"end":1778,"text":"MySQL"}]},{"label":["Skills"],"points":[{"start":1763,"end":1771,"text":"SQL/PLSQL"}]},{"label":["Skills"],"points":[{"start":1757,"end":1760,"text":"Java"}]},{"label":["Skills"],"points":[{"start":1686,"end":1697,"text":"AWS-Redshift"}]},{"label":["Skills"],"points":[{"start":1670,"end":1683,"text":"S3 File System"}]},{"label":["Skills"],"points":[{"start":1653,"end":1668,"text":"HDFS File system"}]},{"label":["Skills"],"points":[{"start":1646,"end":1651,"text":"Python"}]},{"label":["Skills"],"points":[{"start":1588,"end":1592,"text":"Oozie"}]},{"label":["Skills"],"points":[{"start":1582,"end":1586,"text":"Sqoop"}]},{"label":["Skills"],"points":[{"start":1577,"end":1579,"text":"Pig"}]},{"label":["Skills"],"points":[{"start":1571,"end":1574,"text":"Hive"}]},{"label":["Skills"],"points":[{"start":1559,"end":1567,"text":"Spark-SQL"}]},{"label":["Skills"],"points":[{"start":1551,"end":1557,"text":"Pyspark"}]},{"label":["Skills"],"points":[{"start":1540,"end":1544,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":721,"end":726,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":671,"end":674,"text":"Java"}]},{"label":["Skills"],"points":[{"start":607,"end":612,"text":"Python"}]},{"label":["Skills"],"points":[{"start":570,"end":572,"text":"AWS"}]},{"label":["Skills"],"points":[{"start":526,"end":531,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":520,"end":524,"text":"Sqoop"}]},{"label":["Skills"],"points":[{"start":515,"end":517,"text":"Pig"}]},{"label":["Skills"],"points":[{"start":509,"end":512,"text":"Hive"}]},{"label":["Skills"],"points":[{"start":503,"end":506,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":492,"end":498,"text":"Pyspark"}]},{"label":["Skills"],"points":[{"start":481,"end":489,"text":"Spark-SQL"}]},{"label":["Skills"],"points":[{"start":474,"end":478,"text":"Spark"}]},{"label":["Years_of_Experience"],"points":[{"start":275,"end":281,"text":"7 Years"}]},{"label":["Skills"],"points":[{"start":210,"end":215,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":104,"end":109,"text":"Oracle"}]},{"label":["Name"],"points":[{"start":17,"end":36,"text":"Jyoti Ranjan Mohanty"}]}],"extras":null,"metadata":{"first_done_at":1564141403000,"last_updated_at":1564141403000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "K. SOMASHEKHAR\n\n +919490784246    Email: -kalmangissomu@gmail.com \n\nObjective:\n\nTo seek a challenging position in an organization that provides me an opportunity to pursue career in field of Data\nScience & Big data Analytics and provide me with global exposure to excel in my work so as to make contribution\ntowards growth of the organization.\n\nCORE COMPETENCIES \n\nI have around 3 years of experience on Data Analytics. \n\nI have been working on different Algorithms include Machine\nLearning techniques, NLP (Natural language processing), Linear &\nLogistic Regression analysis, Segmentation, Decisions trees, Cluster\nanalysis and factor analysis, Natural language processing, Time Series\nAnalysis, Artificial Neural Networks (ANN), K-Nearest Neighbour,\nK-Means algorithm, Random Forests Algorithm, Sentimental analysis.\n\nMajor Contribution Towards: MACHINE LEARNING Algorithms\n(Predictive analytics)\n\nTECHNICAL SKILLSET\n\n Data analysis Using various machine learning algorithms \n Proficient in using tools such as R, Python (Numpy, Pandas, Matplotlib, Scikit Learn)\n Data visualization using R, Python, Tableau.\n Predictive analytics and NLP\n Platforms like Linux/UNIX, and Virtual Machine Environments\n Experience with SQL and databases like Oracle, Teradata.\n Experience with Hadoop cluster for Big Data analytics.\n Well versed in big data analysis using pig, Hive and Map Reduce\n\nSystems Analysis\n\nMachine Learning/Deep Learning\n\n/NLP\n\nPredictive Analytics\n\nR/Python/SAS programming\n\n\n\n                                                              Work Experience \n\n Working as a Senior Analyst with Capgemini from August 2015 to Till date.\n\nProjects:\nCustomer value management Analysis:\n\nTools: SAS, Python, Machine learning, Decision trees, Random forest, Predicative\nmodeling, Logistic regression, Proc SQL, SAS Macros, Excel.\n\n Duration: Feb 2018-Till Date\n\n1. End to End project management & execution - front end business experience,\ndealing with clients and understanding their needs as well as hands on in\ndata extraction and statistical model development and validation \n\na. CVM Team: Developed a Logistic regression model to identify potential\ncustomers who has propensity to churn. The model has reduced the churn\nrate by 1.5% o Developed a Logistic regression model to identify\npotential customers who can migrate to another plan. Scoring of the\nmodel has helped the marketers in targeting right set of customer and\nincreased campaign performance.\n\nb. Fraud Analytics - Based on customer behavior and tenure, identified\nvarious customer segments for never pay fraud, developed statistical\nmodel for each identified segment, which resulted in more than € 270K\nbenefit\n\nc. Cohort Analysis: Developed a dashboard which identifies the customer\nmovement from segment to segment (High to Low Value), explaining reason\nbehind the movement. which has helped marketing team in quick decision\nmaking and campaign designing o Inactivity Analysis: Identified the\ntriggers for structural inactivity. Helped the business in preventing\ncustomer from becoming inactive and came up with suggestion to make\ncustomer active again. It has resulted in increased in revenue by\n€3Mn.\n\n\n\nPropensity model for customer response model\nTools: SAS, Predicative modeling, Logistic regression, Proc SQL, SAS Macros,\nExcel.\n\n Duration: Feb 2017-Jan 2018\n\nResponsibilities:\n\n Business object: We would like to build a propensity model, who will\nrespond for a product.\n\n Phase1: Performed Exploratory Data Analysis, Data Cleaning, Features\nscaling and Features engineering.\n\n Applied the Proc and Data steps to analyse and create the data step\n Performed Data sanitization, Missing value treatment, outlier treatment.\n Automated the Code for data processing by using the SAS macros.\n Phase 2: Created Dummy variables for Categorical variables and done the\n\nBinning variable creation for Continuous variables.\n Performed Statistics -Descriptive statistics, Hypothesis testing, ANOVA.\n Performed feature selection by picking the most predictive features from\n\nthe model.\n Used variable reduction techniques to drop the in-significant variables\n\n(multicollinearity).\n Divided the data into training and validation datasets.\n Phase 3: Built Response model at customer’s level (by using Logistic\n\nregression).\n Used P value for finding out the fitness of the model.\n Used Boosting and Bagging techniques to further improve the accuracy of\n\nthe algorithm.\n Finally provided data insights and recommendations for the model. \n       Customer life time value:\n\nCustomer value analysis:\n\nObjective: Identifying the discounted value of future profits generated by customer,\nhere profits which includes costs and revenue.\n\nTools: R, SAS, statistical methods, Excel.\n\nDuration: Aug 2015-Jan 2017\n\nContribution:\n\n Understand client’s requirements and objectives of the project\n Understanding the business problem and converting the same into a data\n\nproblem\n Processing, cleansing and verifying the integrity of data used for\n\nanalysis in python\n\n\n\n Identifying the proportion of direct purchases and indirect purchase\ncustomers\n\n Identifying the purchase rate.\n Identifying the life time, the period over which customer is maintaining\n\nhis or her relationship with company\n Identifying the monetary value\n Applied the passion distribution to find out purchase rate\n Built a decision tree model to find out price estimation by using anova\n\nmethod\n Built some machine learning (Random forest, Naive Bayes) and advanced\n\nanalytics (Logistic regression and Decision trees) to predict to purchasing\nprobability of a customers.\n\n Migrated Code from R to SAS tool.\n Well versed with R and SAS coding.\n\nEDUCATIONAL QUALIFICATION\n\n1.M.B.A. Finance & Marketing from SHRIDHAR UNIVERSITY, PILANI, RAJASTHAN in\n\n2015.\n\n       2. B.B.M. Dr.Jyothirmayi Degree College, Adoni in 2013 and secured 61%.\n\n       3.Intermediate Sri Chaitanya Junior College, Vijayawada in 2008 and secured\n\n74%.\n\n       4.SSC from St.Joseph’s English Medium High School, Adoni in 2005 and secured\n65%\n\nTextControl1","annotation":[{"label":["Degree"],"points":[{"start":5727,"end":5753,"text":".M.B.A. Finance & Marketing"}]},{"label":["Tools"],"points":[{"start":5686,"end":5688,"text":"SAS"}]},{"label":["Skills"],"points":[{"start":5680,"end":5680,"text":"R"}]},{"label":["Tools"],"points":[{"start":5651,"end":5653,"text":"SAS"}]},{"label":["Skills"],"points":[{"start":5646,"end":5646,"text":"R"}]},{"label":["Tools"],"points":[{"start":5531,"end":5549,"text":"Logistic regression"}]},{"label":["Tools"],"points":[{"start":4752,"end":4756,"text":"Excel"}]},{"label":["Tools"],"points":[{"start":4726,"end":4728,"text":"SAS"}]},{"label":["Skills"],"points":[{"start":4723,"end":4723,"text":"R"}]},{"label":["Tools"],"points":[{"start":3767,"end":3769,"text":"SAS"}]},{"label":["Tools"],"points":[{"start":3310,"end":3314,"text":"Excel"}]},{"label":["Tools"],"points":[{"start":3298,"end":3307,"text":"SAS Macros"}]},{"label":["Tools"],"points":[{"start":3288,"end":3295,"text":"Proc SQL"}]},{"label":["Tools"],"points":[{"start":3267,"end":3285,"text":"Logistic regression"}]},{"label":["Tools"],"points":[{"start":3244,"end":3264,"text":" Predicative modeling"}]},{"label":["Tools"],"points":[{"start":3240,"end":3242,"text":"SAS"}]},{"label":["Tools"],"points":[{"start":2264,"end":2282,"text":"Logistic regression"}]},{"label":["Tools"],"points":[{"start":2118,"end":2136,"text":"Logistic regression"}]},{"label":["Tools"],"points":[{"start":1835,"end":1839,"text":"Excel"}]},{"label":["Tools"],"points":[{"start":1823,"end":1832,"text":"SAS Macros"}]},{"label":["Tools"],"points":[{"start":1813,"end":1820,"text":"Proc SQL"}]},{"label":["Tools"],"points":[{"start":1792,"end":1810,"text":"Logistic regression"}]},{"label":["Skills"],"points":[{"start":1713,"end":1718,"text":"Python"}]},{"label":["Tools"],"points":[{"start":1708,"end":1710,"text":"SAS"}]},{"label":["Tools"],"points":[{"start":1478,"end":1480,"text":"SAS"}]},{"label":["Skills"],"points":[{"start":1471,"end":1476,"text":"Python"}]},{"label":["Skills"],"points":[{"start":1469,"end":1469,"text":"R"}]},{"label":["Skills"],"points":[{"start":1442,"end":1444,"text":"NLP"}]},{"label":["Skills"],"points":[{"start":1426,"end":1438,"text":"Deep Learning"}]},{"label":["Skills"],"points":[{"start":1409,"end":1424,"text":"Machine Learning"}]},{"label":["Skills"],"points":[{"start":1285,"end":1298,"text":"Hadoop cluster"}]},{"label":["Skills"],"points":[{"start":1257,"end":1264,"text":"Teradata"}]},{"label":["Skills"],"points":[{"start":1249,"end":1254,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":1226,"end":1228,"text":"SQL"}]},{"label":["Operating_Systems"],"points":[{"start":1163,"end":1172,"text":"Linux/UNIX"}]},{"label":["Skills"],"points":[{"start":1142,"end":1144,"text":"NLP"}]},{"label":["Tools"],"points":[{"start":1106,"end":1112,"text":"Tableau"}]},{"label":["Skills"],"points":[{"start":1098,"end":1103,"text":"Python"}]},{"label":["Skills"],"points":[{"start":1095,"end":1095,"text":"R"}]},{"label":["Skills"],"points":[{"start":1054,"end":1065,"text":"Scikit Learn"}]},{"label":["Skills"],"points":[{"start":1042,"end":1051,"text":"Matplotlib"}]},{"label":["Skills"],"points":[{"start":1034,"end":1039,"text":"Pandas"}]},{"label":["Skills"],"points":[{"start":1027,"end":1031,"text":"Numpy"}]},{"label":["Skills"],"points":[{"start":1019,"end":1024,"text":"Python"}]},{"label":["Skills"],"points":[{"start":1016,"end":1016,"text":"R"}]},{"label":["Skills"],"points":[{"start":951,"end":977,"text":"machine learning algorithms"}]},{"label":["Skills"],"points":[{"start":798,"end":817,"text":"Sentimental analysis"}]},{"label":["Skills"],"points":[{"start":772,"end":795,"text":"Random Forests Algorithm"}]},{"label":["Skills"],"points":[{"start":753,"end":769,"text":"K-Means algorithm"}]},{"label":["Skills"],"points":[{"start":732,"end":750,"text":"K-Nearest Neighbour"}]},{"label":["Skills"],"points":[{"start":698,"end":729,"text":"Artificial Neural Networks (ANN)"}]},{"label":["Skills"],"points":[{"start":676,"end":695,"text":"Time Series\nAnalysis"}]},{"label":["Skills"],"points":[{"start":609,"end":644,"text":"Cluster\nanalysis and factor analysis"}]},{"label":["Skills"],"points":[{"start":592,"end":606,"text":"Decisions trees"}]},{"label":["Skills"],"points":[{"start":578,"end":589,"text":"Segmentation"}]},{"label":["Skills"],"points":[{"start":539,"end":575,"text":"Linear &\nLogistic Regression analysis"}]},{"label":["Skills"],"points":[{"start":504,"end":506,"text":"NLP"}]},{"label":["Years_of_Experience"],"points":[{"start":380,"end":386,"text":"3 years"}]},{"label":["Email_Address"],"points":[{"start":42,"end":65,"text":"-kalmangissomu@gmail.com"}]},{"label":["Mobile_No"],"points":[{"start":18,"end":30,"text":"+919490784246"}]},{"label":["Name"],"points":[{"start":0,"end":13,"text":"K. SOMASHEKHAR"}]}],"extras":null,"metadata":{"first_done_at":1564046591000,"last_updated_at":1564047287000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "RESUME\n\nKanhei Charan Padhy\nExperience Summary\n\n· Having  4 +  Years of experience Development of web application using Java / J2EE technologies, Spring, Hibernate.\n· Good knowledge of Object Oriented Programming  Concepts.\n· Experience in development of object-oriented business applications using Java, JDBC, Servlets, JSP, Spring  and Hibernate,Spring Boot, Spring Cloud ,Microservices.\n· Implementation of  RESTful API in application development.\n\n· Excellent Communication and Organizational skills with zeal to learn new technologies.\n· Capable of analyzing the problem and thereby providing an appropriate solution.\n· Having passion for shipping quality high-performance code.\nEducational  Qualification\n\n· B.Tech  in Information and Technology from Biju Patnaik University of  Technology (Odisha) in 2012.\nExperience Profile\n\n· Working as Senior Software Engineer at L&T Technology Services from Jan 2018 to Till Date.\n· Worked as Software Engineer at Tech Mahindra  from June 2015 to Dec 2017.\nSkill Set\n\n\tSKILL\n\tEXPERIENCED\n\tAWARENESS\n\n\tLanguages\n\tJAVA\n\t\n\n\tOperating Systems\n\tWindows\n\tLinux\n\n\tFrameWorks\n\tSpring, Hibernate,SpringBOOT, Spring Cloud,Microservices\n\tOracle \n\n\tDistributed\n\nTechnologies\n\tJAXP, JAXB, JAX-WS, JAX-RS, (RESTful),JSON\n\t\n\n\tJEE Technologies\n\tServlet , JSP\n\t\n\n\tJSE Technologies\n\tJDBC\n\t\n\n\tClient Technologies\n\tHtml, JavaScript, CSS , XML\n\tAjax, JQuery\n\n\tServers\n\tWebshaper,Tomcat, Weblogic,\n\t\n\n\tDatabase  Softwares\n\tOracle 11g\n\tMySQL,SQL\n\n\tTools\n\tSVN, Maven,JUnit,Log4J,SoapUI\n\tMockito,Powermockito\n\n\tIDEs\n\tEclipse,STS\n\tNetBeans\n\n\n\nProject Summary\n\n# Project1 :\nTitle\n                    :  KeplerLake\nEnvironment      : JAVA, Spring Boot, Spring Cloud ,Microservices Technology.\nClient                      :  Intel Technology India Pvt. Ltd ,Bangalore\nDuration                :  Oct 2018 to April 2019\nRole\n                   :  Senior Software Engineer\nTeam Size              :   6\nDescription:      \n\n                      A System that provides a trusted execution environment to facilitate collaborative computation involving confidential data and software from different parties who want to share but not disclose their assets.\nRoles & Responsibilities:\nThere are basically 5 components they are \n\n1)Central Function\n\n2)Data Provider Gateway\n\n3)Trust Orchestrator\n\n4)Gluster Fs\n\n5)Trust Execution Environment\n· So I as a developer worked on Central Function component. In this component there are 3 services running i.e. OAuth(For Security),HTConder(job scheduler) ,Docker(Container). \n\n· So out of these 3 services I have worked on Oauth2 implementation basically OAuth (Open Authorization) is an open standard for token-based authentication and authorization on the Internet allows an end user's account information to be used by third-party services, such as Facebook, without exposing the user's password.\n# Project2 :\nTitle\n                    :  FEMTO DESIGN & DEBUG TOOL\nEnvironment\n:  JAVA, Linux, Spring, Spring Cloud Spring Boot, MySQL, webservices (SOAP/REST),Micro Services Technology,Eclipse\nClient                      :  Samsung,Banaglore\n\nDuration                :  April 18-Sep 18\nRole\n                   :   Senior Software Engineer\nTeam Size              :   15\nDescription:              The Objective of this project is to design and develop the Femto Debugging Tools (FDT) which will aid the Samsung SME’s to analyse and identify the root cause in different scenarios in an efficient manner.\nRoles & Responsibilities:\n· Messaging: Services must handle requests from the application’s clients. Furthermore, services must sometimes collaborate to handle those requests. They must use an inter-process communication protocol. Use asynchronous messaging for inter-service communication. Services communicating by exchanging messages over messaging channels. Examples of asynchronous messaging technologies .\n· I have developed RABBITMQ as messaging queue and as cloud server I have implemented EUREKA server and ZUUL proxy, Fault tolerance(Hystrix Netflix),Load balancing(Ribbon Netflix), using Spring Cloud.\n· I was working with Offline Packet Processing Tool where I devloped to parse log file and read the data from log file and store it into some local file where user can analyse fully of private side network and public side network and also user analyse TR-069 call flows.\n#Project3:\n\nTitle\n                    :  RBT (Ring Back Tone)\nEnvironment      : JAVA,Linux,Servlet,Jsp,PL/SQL,Spring, Hibernate,Oracle10g,Design Patterns, JSP, SVN, Eclipse ,Tomcat Server, Maven.\nClient                      :  Huawei Technologies Co., Ltd, Bangalore\nDuration                :  Jan 18-Mar 18\nRole\n                   :   Senior Software Engineer\nTeam Size              :   15\nDescription:              RBT  has become one of  the most popular mobile services and  it’s now common    place to hear popular songs emanating from mobile phones rather than the monotonous bleeps and rings of yesteryear.\n   Roles & Responsibilities:\n· Worked for USSD(Unstructured Supplementary Service Data) Module and created procedures and functions using PL/SQL\n\n· Designed LLD And UT for the requirements document.\n#Project4:\nTitle   \n\n   : Vehicle Rental Application\nEnvironment                       : Core Java, Jsp, SpringMVC, Hibernate, XML, Apache Tomcat 7\nClient                                : Tech Mahindra Business Services,Bangalore\nDuration                          : Jul 2015-Dec 2017.\nRole\n                             :  Software Engineer\nTeam Size                        : 7\nDescription:   \nIt is a web application which allows customer to book a vehicle online for the travel.The application is composed of two services Customer service and Admin service. In customer service a customer is allowed to create its account and  can book vehicle from different available categories with options to cancel booking.In Admin service the admin person can keep track of the booked vehicles and can also generate report for amount earned in  a month by all the booked vehicles and can see the list of booked vehicle for a particular duration.\nRoles & Responsibilities:\n· Developed the Controller classes and DAO classes using Spring DAO module\n· Developed the Java Classes using Spring AOP and Spring IOC Module.\n\n· Involved in Funneling of exceptions across the layers and displaying appropriate messages to the user.\n\n· Involved in proper association of entities and cascade settings using JPA/Hibernate.\n\n· Handled  input data validation using Spring validation framework.\n\n· Usage of resource bundle for internationalization.\nPERSONAL DETAILS\n Name\n\n:     Kanhei Charan Padhy\n Date of Birth\n:     01st May 1991.\n\n Gender\n\n:     Male \n Marital Status\n\n:     Married\n Passport                                                               :     NA\n Languages Known\n\n\n:    English, Hindi, Oriya\nDECLARATION\n\nI hereby declare that all the above information is true to the best of my knowledge.\n\nDate: \n\nPlace: Bangalore                                                                     \n(Kanhei Charan Padhy)","annotation":null,"extras":null,"metadata":{"first_done_at":1564243962000,"last_updated_at":1564292546000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "RESUME\n\nKanhei Charan Padhy\nE-mail: kanheipadhy1991@gmail.com\nMob. No.:+91-8971871366 , 7008567383\nExperience Summary\n\n· Having  4 +  Years of experience Development of web application using Java / J2EE technologies, Spring, Hibernate.\n· Good knowledge of Object Oriented Programming  Concepts.\n· Experience in development of object-oriented business applications using Java, JDBC, Servlets, JSP, Spring  and Hibernate,Spring Boot, Spring Cloud ,Microservices.\n· Implementation of  RESTful API in application development.\n\n· Excellent Communication and Organizational skills with zeal to learn new technologies.\n· Capable of analyzing the problem and thereby providing an appropriate solution.\n· Having passion for shipping quality high-performance code.\nEducational  Qualification\n\n· B.Tech  in Information and Technology from Biju Patnaik University of  Technology (Odisha) in 2012.\nExperience Profile\n\n· Working as Senior Software Engineer at L&T Technology Services from Jan 2018 to Till Date.\n· Worked as Software Engineer at Tech Mahindra  from June 2015 to Dec 2017.\nSkill Set\n\n\tSKILL\n\tEXPERIENCED\n\tAWARENESS\n\n\tLanguages\n\tJAVA\n\t\n\n\tOperating Systems\n\tWindows\n\tLinux\n\n\tFrameWorks\n\tSpring, Hibernate,SpringBOOT, Spring Cloud,Microservices\n\tOracle \n\n\tDistributed\n\nTechnologies\n\tJAXP, JAXB, JAX-WS, JAX-RS, (RESTful),JSON\n\t\n\n\tJEE Technologies\n\tServlet , JSP\n\t\n\n\tJSE Technologies\n\tJDBC\n\t\n\n\tClient Technologies\n\tHtml, JavaScript, CSS , XML\n\tAjax, JQuery\n\n\tServers\n\tWebshaper,Tomcat, Weblogic,\n\t\n\n\tDatabase  Softwares\n\tOracle 11g\n\tMySQL,SQL\n\n\tTools\n\tSVN, Maven,JUnit,Log4J,SoapUI\n\tMockito,Powermockito\n\n\tIDEs\n\tEclipse,STS\n\tNetBeans\n\n\n\nProject Summary\n\n# Project1 :\nTitle\n                    :  KeplerLake\nEnvironment      : JAVA, Spring Boot, Spring Cloud ,Microservices Technology.\nClient                      :  Intel Technology India Pvt. Ltd ,Bangalore\nDuration                :  Oct 2018 to April 2019\nRole\n                   :  Senior Software Engineer\nTeam Size              :   6\nDescription:      \n\n                      A System that provides a trusted execution environment to facilitate collaborative computation involving confidential data and software from different parties who want to share but not disclose their assets.\nRoles & Responsibilities:\nThere are basically 5 components they are \n\n1)Central Function\n\n2)Data Provider Gateway\n\n3)Trust Orchestrator\n\n4)Gluster Fs\n\n5)Trust Execution Environment\n· So I as a developer worked on Central Function component. In this component there are 3 services running i.e. OAuth(For Security),HTConder(job scheduler) ,Docker(Container). \n\n· So out of these 3 services I have worked on Oauth2 implementation basically OAuth (Open Authorization) is an open standard for token-based authentication and authorization on the Internet allows an end user's account information to be used by third-party services, such as Facebook, without exposing the user's password.\n# Project2 :\nTitle\n                    :  FEMTO DESIGN & DEBUG TOOL\nEnvironment\n:  JAVA, Linux, Spring, Spring Cloud Spring Boot, MySQL, webservices (SOAP/REST),Micro Services Technology,Eclipse\nClient                      :  Samsung,Banaglore\n\nDuration                :  April 18-Sep 18\nRole\n                   :   Senior Software Engineer\nTeam Size              :   15\nDescription:              The Objective of this project is to design and develop the Femto Debugging Tools (FDT) which will aid the Samsung SME’s to analyse and identify the root cause in different scenarios in an efficient manner.\nRoles & Responsibilities:\n· Messaging: Services must handle requests from the application’s clients. Furthermore, services must sometimes collaborate to handle those requests. They must use an inter-process communication protocol. Use asynchronous messaging for inter-service communication. Services communicating by exchanging messages over messaging channels. Examples of asynchronous messaging technologies .\n· I have developed RABBITMQ as messaging queue and as cloud server I have implemented EUREKA server and ZUUL proxy, Fault tolerance(Hystrix Netflix),Load balancing(Ribbon Netflix), using Spring Cloud.\n· I was working with Offline Packet Processing Tool where I devloped to parse log file and read the data from log file and store it into some local file where user can analyse fully of private side network and public side network and also user analyse TR-069 call flows.\n#Project3:\n\nTitle\n                    :  RBT (Ring Back Tone)\nEnvironment      : JAVA,Linux,Servlet,Jsp,PL/SQL,Spring, Hibernate,Oracle10g,Design Patterns, JSP, SVN, Eclipse ,Tomcat Server, Maven.\nClient                      :  Huawei Technologies Co., Ltd, Bangalore\nDuration                :  Jan 18-Mar 18\nRole\n                   :   Senior Software Engineer\nTeam Size              :   15\nDescription:              RBT  has become one of  the most popular mobile services and  it’s now common    place to hear popular songs emanating from mobile phones rather than the monotonous bleeps and rings of yesteryear.\n   Roles & Responsibilities:\n· Worked for USSD(Unstructured Supplementary Service Data) Module and created procedures and functions using PL/SQL\n\n· Designed LLD And UT for the requirements document.\n#Project4:\nTitle   \n\n   : Vehicle Rental Application\nEnvironment                       : Core Java, Jsp, SpringMVC, Hibernate, XML, Apache Tomcat 7\nClient                                : Tech Mahindra Business Services,Bangalore\nDuration                          : Jul 2015-Dec 2017.\nRole\n                             :  Software Engineer\nTeam Size                        : 7\nDescription:   \nIt is a web application which allows customer to book a vehicle online for the travel.The application is composed of two services Customer service and Admin service. In customer service a customer is allowed to create its account and  can book vehicle from different available categories with options to cancel booking.In Admin service the admin person can keep track of the booked vehicles and can also generate report for amount earned in  a month by all the booked vehicles and can see the list of booked vehicle for a particular duration.\nRoles & Responsibilities:\n· Developed the Controller classes and DAO classes using Spring DAO module\n· Developed the Java Classes using Spring AOP and Spring IOC Module.\n\n· Involved in Funneling of exceptions across the layers and displaying appropriate messages to the user.\n\n· Involved in proper association of entities and cascade settings using JPA/Hibernate.\n\n· Handled  input data validation using Spring validation framework.\n\n· Usage of resource bundle for internationalization.\nPERSONAL DETAILS\n Name\n\n:     Kanhei Charan Padhy\n Date of Birth\n:     01st May 1991.\n\n Gender\n\n:     Male \n Marital Status\n\n:     Married\n Passport                                                               :     NA\n Languages Known\n\n\n:    English, Hindi, Oriya\nDECLARATION\n\nI hereby declare that all the above information is true to the best of my knowledge.\n\nDate: \n\nPlace: Bangalore                                                                     \n(Kanhei Charan Padhy)","annotation":[{"label":["Name"],"points":[{"start":7103,"end":7121,"text":"Kanhei Charan Padhy"}]},{"label":["Name"],"points":[{"start":6673,"end":6691,"text":"Kanhei Charan Padhy"}]},{"label":["Skills"],"points":[{"start":6560,"end":6565,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":6509,"end":6517,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":6307,"end":6312,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":6292,"end":6297,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":6239,"end":6244,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":5336,"end":5344,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":5325,"end":5330,"text":"Spring"}]},{"label":["Tools"],"points":[{"start":4596,"end":4600,"text":"Maven"}]},{"label":["Tools"],"points":[{"start":4567,"end":4569,"text":"SVN"}]},{"label":["Skills"],"points":[{"start":4562,"end":4564,"text":"JSP"}]},{"label":["Skills"],"points":[{"start":4535,"end":4540,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":4525,"end":4533,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":4517,"end":4522,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":4498,"end":4504,"text":"Servlet"}]},{"label":["Operating_Systems"],"points":[{"start":4492,"end":4496,"text":"Linux"}]},{"label":["Skills"],"points":[{"start":4487,"end":4490,"text":"JAVA"}]},{"label":["Skills"],"points":[{"start":4121,"end":4132,"text":"Spring Cloud"}]},{"label":["Skills"],"points":[{"start":3036,"end":3041,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":3023,"end":3034,"text":"Spring Cloud"}]},{"label":["Skills"],"points":[{"start":3015,"end":3020,"text":"Spring"}]},{"label":["Operating_Systems"],"points":[{"start":3008,"end":3012,"text":"Linux"}]},{"label":["Skills"],"points":[{"start":3002,"end":3005,"text":"JAVA"}]},{"label":["Skills"],"points":[{"start":1756,"end":1768,"text":"Microservices"}]},{"label":["Skills"],"points":[{"start":1742,"end":1753,"text":"Spring Cloud"}]},{"label":["Skills"],"points":[{"start":1729,"end":1734,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":1723,"end":1726,"text":"JAVA"}]},{"label":["Tools"],"points":[{"start":1588,"end":1599,"text":"Powermockito"}]},{"label":["Tools"],"points":[{"start":1580,"end":1586,"text":"Mockito"}]},{"label":["Tools"],"points":[{"start":1572,"end":1577,"text":"SoapUI"}]},{"label":["Tools"],"points":[{"start":1566,"end":1570,"text":"Log4J"}]},{"label":["Tools"],"points":[{"start":1560,"end":1564,"text":"JUnit"}]},{"label":["Tools"],"points":[{"start":1554,"end":1558,"text":"Maven"}]},{"label":["Tools"],"points":[{"start":1549,"end":1551,"text":"SVN"}]},{"label":["Skills"],"points":[{"start":1518,"end":1523,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":1382,"end":1385,"text":"JDBC"}]},{"label":["Skills"],"points":[{"start":1356,"end":1358,"text":"JSP"}]},{"label":["Skills"],"points":[{"start":1346,"end":1352,"text":"Servlet"}]},{"label":["Skills"],"points":[{"start":1319,"end":1322,"text":"JSON"}]},{"label":["Skills"],"points":[{"start":1309,"end":1317,"text":"(RESTful)"}]},{"label":["Skills"],"points":[{"start":1301,"end":1306,"text":"JAX-RS"}]},{"label":["Skills"],"points":[{"start":1293,"end":1298,"text":"JAX-WS"}]},{"label":["Skills"],"points":[{"start":1287,"end":1290,"text":"JAXB"}]},{"label":["Skills"],"points":[{"start":1281,"end":1284,"text":"JAXP"}]},{"label":["Skills"],"points":[{"start":1244,"end":1249,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":1229,"end":1241,"text":"Microservices"}]},{"label":["Skills"],"points":[{"start":1216,"end":1227,"text":"Spring Cloud"}]},{"label":["Skills"],"points":[{"start":1204,"end":1213,"text":"SpringBOOT"}]},{"label":["Skills"],"points":[{"start":1194,"end":1202,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":1186,"end":1191,"text":"Spring"}]},{"label":["Operating_Systems"],"points":[{"start":1166,"end":1170,"text":"Linux"}]},{"label":["Operating_Systems"],"points":[{"start":1157,"end":1163,"text":"Windows"}]},{"label":["Skills"],"points":[{"start":1129,"end":1132,"text":"JAVA"}]},{"label":["Degree"],"points":[{"start":785,"end":790,"text":"B.Tech"}]},{"label":["Skills"],"points":[{"start":446,"end":458,"text":"Microservices"}]},{"label":["Skills"],"points":[{"start":432,"end":443,"text":"Spring Cloud"}]},{"label":["Skills"],"points":[{"start":419,"end":424,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":409,"end":417,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":397,"end":402,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":392,"end":394,"text":"JSP"}]},{"label":["Skills"],"points":[{"start":382,"end":388,"text":"Servlet"}]},{"label":["Skills"],"points":[{"start":376,"end":379,"text":"JDBC"}]},{"label":["Skills"],"points":[{"start":225,"end":233,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":217,"end":222,"text":"Spring"}]},{"label":["Years_of_Experience"],"points":[{"start":129,"end":138,"text":"4 +  Years"}]},{"label":["Mobile_No"],"points":[{"start":88,"end":97,"text":"7008567383"}]},{"label":["Mobile_No"],"points":[{"start":71,"end":84,"text":"+91-8971871366"}]},{"label":["Email_Address"],"points":[{"start":36,"end":60,"text":"kanheipadhy1991@gmail.com"}]},{"label":["Name"],"points":[{"start":8,"end":26,"text":"Kanhei Charan Padhy"}]}],"extras":null,"metadata":{"first_done_at":1564212678000,"last_updated_at":1564212678000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "Kishore Kumar R\n\nNo.34, Sri SriNidhi Nilaya, 7th main, 3rd cross,\nRaghavendra Nagar, R.M.Nagar,\nBangalore, Karnataka 560016 India\nMobile: 8548855784\nEmail: kishore.kumar6r@gmail.com\nLinkedIn: https://www.linkedin.com/in/kishore-kumar-r-a5762616b/\n\nMotivated and Smart working professional with skills in Hadoop & Spark.\n \nHighly motivated and enterprising professional with good knowledge of Hadoop - Bigdata. Able to use own initiative, working alone or as part of a team under pressure to meet deadlines and objectives. Excellent communication and organizational skills, a strong work ethic and determination to succeed. Enthusiastic learner, eager to meet new challengers and get ahead in the IT industry.\n\nSummary\n\nAround 6.4 years of experience in IT out of which having 5 years of experience in big data technologies.\n\n· Deep expertise in Hadoop ecosystem- Spark, Spark Data frames, Spark SQL, Spark Streaming (POC), HDFS, Hive, Sqoop, Flume, Oozie, Pig, Impala, Python, Hbase, Map Reduce, Kafka and Cassandra (Datastax training completed).\n· Multiple Frameworks built and supported for vendors.\n· Worked on importing and exporting data from different databases like oracle, mysql into HDFS and hive/hbase using SQOOP.\n· Implemented real time streaming applications involving json data using FLUME and KAFKA.\n· Proven ability to excel in fast paced development environment using latest frameworks/tools- GIT, Jenkins, SVN.\n· Loading and analyzing flat files from LFS to HDFS and working knowledge in Linux commands.\n· Successfully designed, developed and implemented big data projects in production environment.\n· Good exposure in following all the process in a production environment like change management, incident management etc. Expertise in handling development.\n· Designed and implemented process to read transactional logs of RDBMS into HDFS in real time using KAFKA & CAMUS (POC).\n\n\n\n\n\n\nSKILLS\n\n\tBig Data\n\tHadoop, Map Reduce, HDFS, Hive, Sqoop, Flume, Oozie, Pig, Spark, Kafka, Impala, AWS, Camus, Confluent Platform, Cloudera, Horton Works, Hbase, Cassandra (training completed)\n\n\tDatabase\n\tMysql, Hive, Oracle, DB2, SQL Server\n\n\tLanguages\n\tUnix Shell, Python, SQL\n\n\tDevOps Tools\n\tGithub, Jenkins, SVN, Anisible\n\n\tOther Tools/Utilities\n\tWinSCP, Putty, Autosys, VM Workstation, PyCharm.\n\n\n\n\nPROFESSIONAL EXPERIENCE\n\nProject Name: US Bank Client Sep '16 to Present\nCompany: Accenture Solutions Private Limited, Bangalore \nRole: Hadoop Developer\nResponsibilities:\n· Implemented process to generate oozie xml for Shell, hive, hdfs actions during runtime.\n· Data ingestion from multiple (Flat files && RDBMS) sources for CHATBOT application.\n· Built multiple Frameworks for processing Files, sql's, validations, file extractions sent to downstream.\n· Data Offload from Teradata, Oracle and SQL server using SQOOP to HDFS and access via Hive/Impala for Client reporting.\n· Analytics queries developed in Spark-data frames (python)/ JSON flattening using pyspark.\n· Accelerate app (innovation app to bring data in real time) Working on Fake News detection innovation and social media - user network clustering.\n· Implemented Real time streaming applications involving json data using FLUME and KAFKA.\n· Designed and implemented process to read transactional logs of RDBMS into HDFS in real time using KAFKA & CAMUS (POC).\n\nProject Name: Enterprise Data Lake (US Retail) Nov '15 to Sep ’16 \nCompany: Accenture Solutions Private Limited, Mumbai \nRole: Hadoop Developer\nResponsibilities:\n· Independently worked on user stories for Detokenization in Hive, Hive performance optimization and Multiple Flume Agents.\n· Loading and analyzing flat files from LFS to HDFS and working knowledge in Linux commands.\n· Worked on importing and exporting data from different databases like Teradata, MySQL into HDFS and hive/hbase using SQOOP.\n\n\nProject Name: US Bank Client Jan '13 to Aug ’15 \nCompany: TCS, Bangalore\nRole: Mainframe/Hadoop Developer\nResponsibilities: \n· Active part of End to End design, development and deployment projects for validating files received from different client systems.\n· Unit Test preparations and Unit testing and documentation in QC. Requirement analysis and Post deployment support process.\n· Maintenance and support of front end applications java, jsp.\n· Proof of Concepts to find duplicate images/feature extraction stored in hdfs based on hash values.\n· 360 degree customer centric view Proof of concept using neo4j, Sentimental analysis of customer feedback.\n\nSignificant Highlights\n\n· Awarded Accenture Centre of Excellence (ACE)-Future Skills DEC 2018.\n· Awarded Star Performer of the Month FEB 2018 for exceeded expectations in the deliverables and mentored above and beyond expectations.\n· Awarded Accenture Centre of Excellence (ACE)-Pathfinder AUG 2016, highest level of accreditation in Accenture, for delivery of multiple critical and complex Hadoop releases in less time.\n· Appreciations from client and Accenture leadership on multiple occasions for outstanding performance & defect free project deliveries.\n· Active member of Innovation team with prototype reaching top 100 in Accenture global.\n· Independently worked on proof of concepts for tools such as Accelerate APP within Accenture.\n· Active member CSR community within Accenture and organized many events at DU/IG levels.\n\nAcademics\n\n\tDegree\n\tUniversity\n\t% Marks\n\n\tBE\n\tCMRIT (VTU)\n\t81%\n\n\tII PUC\n\tSt Joseph’s PUC\n\t85%\n\n\t10th\n\tITI Vidya Mandir\n\t89%\n\n\n\nHobbies/Interests\n\n· Upgrade skills in Hacker Rank/geeksforgeeks etc.\n· Setup multi node Hadoop cluster in AWS and Proof of Concepts in AWS.\n· Active part of Charity/CSR events in Accenture.\n· Innovation and updating the concepts and skills in Github and Gaming.","annotation":[{"label":["Tools"],"points":[{"start":5697,"end":5702,"text":"Github"}]},{"label":["Skills"],"points":[{"start":5589,"end":5591,"text":"AWS"}]},{"label":["Skills"],"points":[{"start":5560,"end":5562,"text":"AWS"}]},{"label":["Skills"],"points":[{"start":5542,"end":5547,"text":"Hadoop"}]},{"label":["Degree"],"points":[{"start":5368,"end":5369,"text":"BE"}]},{"label":["Skills"],"points":[{"start":4885,"end":4890,"text":"Hadoop"}]},{"label":["Skills"],"points":[{"start":3927,"end":3932,"text":"Hadoop"}]},{"label":["Skills"],"points":[{"start":3803,"end":3806,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":3794,"end":3796,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":3665,"end":3668,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":3604,"end":3608,"text":"Flume"}]},{"label":["Skills"],"points":[{"start":3561,"end":3564,"text":"Hive"}]},{"label":["Skills"],"points":[{"start":3555,"end":3558,"text":"Hive"}]},{"label":["Skills"],"points":[{"start":3459,"end":3464,"text":"Hadoop"}]},{"label":["Skills"],"points":[{"start":3286,"end":3289,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":2914,"end":2918,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":2852,"end":2857,"text":"Impala"}]},{"label":["Skills"],"points":[{"start":2847,"end":2850,"text":"Hive"}]},{"label":["Skills"],"points":[{"start":2827,"end":2830,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":2801,"end":2803,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":2790,"end":2795,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":2442,"end":2447,"text":"Hadoop"}]},{"label":["Tools"],"points":[{"start":2293,"end":2299,"text":"PyCharm"}]},{"label":["Tools"],"points":[{"start":2277,"end":2290,"text":"VM Workstation"}]},{"label":["Tools"],"points":[{"start":2268,"end":2274,"text":"Autosys"}]},{"label":["Tools"],"points":[{"start":2261,"end":2265,"text":"Putty"}]},{"label":["Tools"],"points":[{"start":2253,"end":2258,"text":"WinSCP"}]},{"label":["Tools"],"points":[{"start":2219,"end":2226,"text":"Anisible"}]},{"label":["Tools"],"points":[{"start":2214,"end":2216,"text":"SVN"}]},{"label":["Tools"],"points":[{"start":2205,"end":2211,"text":"Jenkins"}]},{"label":["Tools"],"points":[{"start":2197,"end":2202,"text":"Github"}]},{"label":["Skills"],"points":[{"start":2177,"end":2179,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":2169,"end":2174,"text":"Python"}]},{"label":["Skills"],"points":[{"start":2157,"end":2166,"text":"Unix Shell"}]},{"label":["Skills"],"points":[{"start":2133,"end":2142,"text":"SQL Server"}]},{"label":["Skills"],"points":[{"start":2128,"end":2130,"text":"DB2"}]},{"label":["Skills"],"points":[{"start":2120,"end":2125,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":2114,"end":2117,"text":"Hive"}]},{"label":["Skills"],"points":[{"start":2107,"end":2111,"text":"Mysql"}]},{"label":["Skills"],"points":[{"start":2064,"end":2072,"text":"Cassandra"}]},{"label":["Skills"],"points":[{"start":2057,"end":2061,"text":"Hbase"}]},{"label":["Skills"],"points":[{"start":2043,"end":2054,"text":"Horton Works"}]},{"label":["Skills"],"points":[{"start":2033,"end":2040,"text":"Cloudera"}]},{"label":["Skills"],"points":[{"start":2013,"end":2030,"text":"Confluent Platform"}]},{"label":["Skills"],"points":[{"start":2006,"end":2010,"text":"Camus"}]},{"label":["Skills"],"points":[{"start":2001,"end":2003,"text":"AWS"}]},{"label":["Skills"],"points":[{"start":1993,"end":1998,"text":"Impala"}]},{"label":["Skills"],"points":[{"start":1986,"end":1990,"text":"Kafka"}]},{"label":["Skills"],"points":[{"start":1979,"end":1983,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":1974,"end":1976,"text":"Pig"}]},{"label":["Skills"],"points":[{"start":1967,"end":1971,"text":"Oozie"}]},{"label":["Skills"],"points":[{"start":1960,"end":1964,"text":"Flume"}]},{"label":["Skills"],"points":[{"start":1953,"end":1957,"text":"Sqoop"}]},{"label":["Skills"],"points":[{"start":1947,"end":1950,"text":"Hive"}]},{"label":["Skills"],"points":[{"start":1941,"end":1944,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":1928,"end":1938,"text":" Map Reduce"}]},{"label":["Skills"],"points":[{"start":1921,"end":1926,"text":"Hadoop"}]},{"label":["Skills"],"points":[{"start":1911,"end":1918,"text":"Big Data"}]},{"label":["Skills"],"points":[{"start":1851,"end":1854,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":1476,"end":1479,"text":"HDFS"}]},{"label":["Tools"],"points":[{"start":1424,"end":1426,"text":"SVN"}]},{"label":["Tools"],"points":[{"start":1415,"end":1421,"text":"Jenkins"}]},{"label":["Skills"],"points":[{"start":1192,"end":1195,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":1006,"end":1014,"text":"Cassandra"}]},{"label":["Skills"],"points":[{"start":996,"end":1000,"text":"Kafka"}]},{"label":["Skills"],"points":[{"start":983,"end":993,"text":" Map Reduce"}]},{"label":["Skills"],"points":[{"start":977,"end":981,"text":"Hbase"}]},{"label":["Skills"],"points":[{"start":969,"end":974,"text":"Python"}]},{"label":["Skills"],"points":[{"start":961,"end":966,"text":"Impala"}]},{"label":["Skills"],"points":[{"start":956,"end":958,"text":"Pig"}]},{"label":["Skills"],"points":[{"start":949,"end":953,"text":"Oozie"}]},{"label":["Skills"],"points":[{"start":942,"end":946,"text":"Flume"}]},{"label":["Skills"],"points":[{"start":935,"end":939,"text":"Sqoop"}]},{"label":["Skills"],"points":[{"start":929,"end":932,"text":"Hive"}]},{"label":["Skills"],"points":[{"start":923,"end":926,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":900,"end":904,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":895,"end":897,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":889,"end":893,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":870,"end":874,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":863,"end":867,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":845,"end":850,"text":"Hadoop"}]},{"label":["Years_of_Experience"],"points":[{"start":726,"end":734,"text":"6.4 years"}]},{"label":["Skills"],"points":[{"start":392,"end":397,"text":"Hadoop"}]},{"label":["Skills"],"points":[{"start":313,"end":317,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":304,"end":309,"text":"Hadoop"}]},{"label":["Email_Address"],"points":[{"start":156,"end":180,"text":"kishore.kumar6r@gmail.com"}]},{"label":["Mobile_No"],"points":[{"start":138,"end":147,"text":"8548855784"}]},{"label":["Name"],"points":[{"start":0,"end":14,"text":"Kishore Kumar R"}]}],"extras":null,"metadata":{"first_done_at":1564242189000,"last_updated_at":1564242673000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "Skills\n\tYears of Experience\n\tProject Name\n\tDuration\n\n\tPython\n\t4 Years\n\tLima chatbot,Wells Fargo\n\t2015-2019\n\n\tMachine Learning\n\t4 Years\n\tLima chatbot,Wells Fargo\n\t2015-2019\n\n\tText Mining\n\t4 Years\n\tLima chatbot,Wells Fargo\n\t2015-2019\n\n\tSql\n\t4 Years\n\tLima chatbot,Wells Fargo\n\t2015-2019\n\n\tNLP\n\t3 Years\n\tWells Fargo\n\t2015-2018\n\n\n \n\nKODI PRAKASH SENAPATI\nMob# +91–75698 23183\nE-mail: kdatascientist@gmail.com\n\nOBJECTIVE\n\nSeeking a challenging position as a highly reliable professional in Data science (Machine Learning & Deep Learning) algorithms, DWH to improve company performance in Different Domain by employing the best practices learnt till now through diverse experiences. Looking for a challenging & rewarding job opportunity which can utilize my existing technical/managerial skill set and helps me in emerging as a competitive professional while working with a team of inspired people in a dynamic workplace\n\nPROFESSIONAL BACKGROUND\n\nHaving 7+ years of diversified IT experience in analysis, design and development of Data science (Machine Learning , Deep Learning & Computer Vision),Big Data applications(HDFS, Map-Reduce, Hive and Spark), Natural language processing ,Oracle 10g/11g, Data Warehousing using Informatica Powercenter 8.x/9.x/10.x, TeradataV2R6 and MDM.\n\nDATA SCIENCE EXPERIENCE\n\n· Understanding Business Problem Using Python and Bigdata concepts to solve business Problems and successful implementations of solutions.\n· Possess expertise in Data science (Machine Learning & Deep Learning) using Python.\n· Possess expertise in implementation of Regression and classification algorithms (Linear Regression, Multiple Liner Regression, Logistic Regression, KNN, Naive Bayes classifier, Decision Tree, Random Forest, SVM, K-Means, Hierarchical clustering, Gradient descent, ANN, CNN, RNN , Computer vision, Natural language processing and Recommendation systems, LSTM, Time series analysis ) using scikit learn and tensor flow libraries.\n· Possess expertise in implementation of chatbot using Google DialogFlow and strong knowledge in NLP using Spacy.\n· Knowledge in Bigdata concepts (HDFS, MAPREDUCE, HIVE and PYSPARK)\n· Good in statistical and mathematical knowledge\n· Strong knowledge of Banking, Retail and healthcare domain.\n· Knowledge in visualizing the data using Tableau\n· Knowledge in implementation of blockchain.\n\nDATAWAREHOUSE EXPERIENCE\n\n· Possess expertise in Database as well as Data warehousing concepts, skilled at progressing from problem statement to well documented design\n· Strong ETL Knowledge of Data Integration, Master Data Management, Data Profiling and Data quality processes\n· Experience in working with Powercenter web service\n· Experience in working with using Informatica partitioning and PDO\n· Experience in working with complex mappings using Mapplets, expressions, XML parser transformation, routers, lookups, aggregators, filters, updatestatergy, SQL, joiners and union transformations in Informatica\n· Experience in Data Warehouse Development life cycle, Dimensional Modeling, Repository Management and Administration, Implementation of STAR, Snowflake Schemas\n\n\n\n\nEMPLOYMENT HISTORY\n\n· Worked as Process Developer in Data Science & Data Warehousing at GENPACT PVT LTD for client  WELLS FARGO since AUG 2011 till FEB 2019\n\nEDUCATIONAL QUALIFICATION\n\n· M.C.A (Master in Computer Application) from A.V.I.T College, CHENNAI, DEEMED University, CHENNAI – 2010 \n\nTECHNICAL SKILLS\n\n· Operating Systems      :   MS Windows, UNIX\n· Language\t\t    :  SQL, PL/SQL, python and R programming\n· Scripting \t\t    :  Unix/Shell Scripting\n· Analytics Tools             :   Jupyter notebook, spyder\n· ETL Tools                      :  Informatica Powercenter 8.x/9.x/10.x\n· Scripting Language      :  Python, Unix, PERL\n· Job Scheduling tools    :  Autosys\n\nPROJECT DETAILS\n\n\t1.\n\tProjects\n\t:\n\tLima chatbot\n\n\t\n\tClient\n\t:\n\tWellsFargo\n\n\t\n\tDomain\n\t:\n\tBanking\n\n\t\n\tDuration\n\t:\n\tJan 2018 – Till date\n\n\t\n\tTechnology\n\t:\n\tPython, Jupyter notebook, scikit learn and tensor flow libraries ,Keras,Theano, oracle, Anaconda package, Google API,Google Dialogflow, Google Cloud\n\n\t\n\tRole\n\t:\n\tTeam Lead\n\n\n\nChatterBot is a machine-learning based conversational dialog engine build in Python which makes it possible to generate responses based on collections of known conversations. The language independent design of ChatterBot allows it to be trained to speak any language\nAn untrained instance of ChatterBot starts off with no knowledge of how to communicate. Each time a user enters a statement, the library saves the text that they entered and the text that the statement was in response to. As ChatterBot receives more input the number of responses that it can reply and the accuracy of each response in relation to the input statement increase. The program selects the closest matching response by searching for the closest matching known statement that matches the input, it then returns the most likely response to that statement based on how frequently each response is issued by the people the bot communicates with\nFor building the Bot we used Google's natural language understanding developer framework for building conversational experiences. DialogFlow needs to be trained on the dataset to attain a machine learning capability which understands the intent and context of what a user says in order to respond in the most useful way.\nDialogFlow lets you build conversational interfaces on top of your products and services by providing a powerful natural language understanding (NLU) engine to process and understand natural language input.\n\n\nResponsibilities:\n\n•\tDone exploratory analysis and took inferences by visualization the data.\n•\tMade exploratory analysis and cleansed the data.\n•\tPerformed below are the data preprocessing steps.\n\tTokenization\n\tNoise Removal\n\tNoun Phrase Extraction\n\tPart-of-speech Tagging\n\tWords Inflection and Lemmatization\n\tN-grams\n\tSpelling Correction\t\n\tSemantic Analysis\n\tSentiment Analysis\n•\tPerformed below are the Training Phases using Google DialogFlow.\n\tContent Recognition\n\tContent Level Division\n\tEntities\n\tEvents Generation\n\tSynonym Tuning\n\tResponse Training\n•\tUsing Google cloud (GCI) performed Bot preparation\n\n\t2.\n\tProjects\n\t:\n\tCDD,LOANIQ,REALM,INFOLEASE, Loan defaulter prediction, Customer feedback analysis\n\n\t\n\tClient\n\t:\n\tWells Fargo\n\n\t\n\tDomain\n\t:\n\tInvestment Banking\n\n\t\n\tDuration\n\t:\n\tSept 2015 – Jan 2018\n\n\t\n\tTechnology\n\t:\n\tPython, Jupyter notebook, scikit learn and tensor flow libraries ,Keras,Theano, oracle, Anaconda package, Matlab,Google API\n\n\t\n\tRole\n\t:\n\tModule Lead\n\n\n\nDescription:\n\nWells Fargo & Company is an American multinational banking and financial services holding company which is headquartered in San Francisco, California, with \"hubquarters\" throughout the country. It is the fourth largest bank in the U.S. by assets and the largest bank by market capitalization. Wells Fargo is the second largest bank in deposits, home mortgage servicing, and debit cards\nCustomer Due Diligence (CDD) prediction:\n CDD information comprises the facts about a customer that should enable an organization to assess the extent to which the customer exposes it to a range of risks. These risks include money laundering and terrorist financing. Primary goal of CDD enables Wells Fargo to know its customer understand the nature and purpose of customer relationship to develop a customer risk profile and reasonably predict the types of transaction in which a customer is likely to engage and determine when transition are potentially suspicious. Organizations need to ‘know their customers’ for a number of reasons:\n\n· to comply with the requirements of relevant legislation and regulation\n· to help the firm, at the time the due diligence is carried out, to be reasonably certain that the customers are who they say they are, and that it is appropriate\n· to provide them with the products or services requested\n· to guard against fraud, including impersonation and identity fraud\n· to help the organization to identify, during the course of a continuing relationship, what is unusual and to enable the unusual to be examined;\n· if unusual events do not have a commercial or otherwise straightforward rationale they may involve money laundering, fraud, or handling criminal or terrorist property\n· to enable the organization to assist law enforcement, by providing available\n· Information on customers being investigated following the making of a suspicion report to the Financial Intelligence Unit (FIU)\nResponsibilities\n \n· Doing Customer interaction and understanding the requirement\n·  Getting the requirements from client and distributing the task among the team members.\n· Done exploratory analysis and took inferences by visualization the data\n· Improvised the quality of data by removing inconsistent data, missing values and label the data using feature scaling\n· Removed insignificant variables using Dimensionality reduction technique\n· Used Gradient descent algorithm to minimizing the cost function\n· Performed Random Forest, Decision Tree, Logistic Regression, and KNN to find fraud customers\n· Optimized the accuracy parameters with 95% accuracy with using Random Forest\n· Explored the data using different visualization techniques \n\nLoan IQ Prediction.\n\nLoanIQ allows you to instantly identify high-risk loans and reduce overall default exposure. The LoanIQ and Market Risk Scores assist you in making a review decision based on the level of collateral risk associated with a loan. Whether you need to assess one loan or thousands, LoanIQ delivers the answers you need fast to make the most informed loan decisions. Primary goal of Loan IQ prediction is\n• Easy-to-use and designed to work with your existing loan systems and processes \n• Fast-track low risk loans to automated approval processes \n• Select the highest risk loans for quality control and due diligence \n• Evaluate existing portfolios retroactively for loan quality comparisons \n• Monitor portfolio performance to aid with loss mitigation and retention programs \n• Investor protection with insurance option \n• The industry’s only patent-pending collateral risk predictive model \n• Loan scoring based on the widest range of predictors \n• Instant identification of property over-valuation \n• Filter out high-risk loans by drilling down on red-flagged indicators to assess quality of     loans     against your underwriting criteria\n• Instantly receive risk metrics for due diligence\n\nResponsibilities\n\n· Done exploratory analysis and took inferences by visualization the data\n· Doing Customer interaction and understanding the requirement \n· Getting the requirements from client and distributing the task among the team members\n· Improvised the quality of data by removing inconsistent data, missing values and label the data using feature scaling\n· Removed insignificant variables using Dimensionality reduction technique\n· Used kernel SVM for nonlinear separable data\n\n· Performed artificial neural networks (ANN)  because of including requiring less formal statistical training, ability to implicitly detect complex nonlinear relationships between dependent and independent variables, ability to detect all possible interactions between predictor variables, and the availability of multiple training\n· Performed algorithms like KNN, NAÏVE BAYES, Random forest and obtained optimal sensitivity and specificity with logistic regression\n· Optimized the accuracy parameters with 87% of accuracy\n·  Explored the data using different visualization techniques\n\nINFOLEASE prediction\nWells Fargo providing the loans to customer for keeping the collateral as a lease. We need to identify the highest risk loans for quality control and due diligence based on collateral value\nREALM prediction\nWells Fargo providing the loans to real state customers. We need to identify Instant identification of property or collateral over-valuation\nLoan Defaulter prediction\n\n Built a model to predict whether the banking customer will repay the loan amount or not\n\nResponsibilities\n\n· Done exploratory analysis and took inferences by visualization the data\n· Improvised the quality of data by removing inconsistent data, missing values, outliers\n· Removed insignificant variables using Dimensionality reduction technique\n· Optimized the accuracy parameters with 82% of accuracy\n· Explored the data using different visualization techniques  \n· Used algorithms like KNN, NAÏVE BAYES and obtained optimal sensitivity and specificity with logistic regression\n\nCustomer feedback analysis\n\nPrimary & secondary research for customer satisfaction & service delay using Natural language processing\n\nResponsibilities\n\n· Done exploratory analysis and took inferences by visualization the data\n· Removed insignificant variables using Dimensionality reduction technique\n· Made exploratory analysis and cleansed the data\n· Performed Natural language processing (NLP) using NLTK , NAÏVE BAYES, Logistic regression for customer feedback analysis\n· Explored the data using different visualization techniques\n\n\t3.\n\tProject\n\t:\n\tCMSO(Commercial Managed Service Outsourcing)\n\n\t\n\tClient\n\t:\n\tShire\n\n\t\n\tDomain\n\t:\n\tpharmaceutical\n\n\t\n\tDuration\n\t:\n\tSept 2014 – Sept 2015\n\n\t\n\tTechnology\n\t:\n\tInformatica Powercenter 9.1,Netezza,UC4 & UNIX(putty)\n\n\t\n\tRole\n\t:\n\tSoftware Engineer\n\n\n\nDescription:\n\n          Shire Plc is an Irish-headquartered global specialty biopharmaceutical company. Originating in the United Kingdom with a large operational base in the United States, its brands and products include Vyvanse, Adderall XR, Intuniv, Lialda, Pentasa, Fosrenol, Replagal, Elaprase, VPRIV, Firazyr and Dermagraft.basicaly shire manufacture the products for ADHD(Neural disorder),GI(Gastro insentient ) and RD(rare disease).basically we are working with products commonly known as drugs or medicines. Sales Data is logically organized information about an executed sales transaction.  A sales transaction is executed when the order has been filled and the customer has been billed for the product.  We are getting the sales transaction information daily, weekly, monthly, quarterly basics from different vendors and finally we are loading data into EDWARD then publishing into mart.\n\nResponsibilities:\n\n· Supporting onsite and offshore development team and helping them conceptualize the business processes\n· Preparing SQL statement for quality check\n· Created Complex Mapping and tuned them for better performance\n· Created Informatica mappings using Mapplets, Source Qualifier, Expression, Lookup (connected and unconnected), Aggregate, Update Strategy, Joiner, Normalizer, java Transformation, XML parser transformation, sorter and Filter transformations\n· Experienced with coordinating cross-functional teams, project management and presenting technical ideas to diverse groups & Proven ability to implement technology based solutions for business problems\n· Used the Informatica Designer to develop processes for data profiling, data extracting, cleansing, data scrubbing, data transforming, integrating, and loading data into the work area (staging)\n· Scheduling the workflow and running the workflow using third party tool UC4\n· Defect Management and Bug Fixing\n\n\n\t4.\n\tProject\n\t:\n\tIBM Watson Healthcare\n\n\t\n\tClient\n\t:\n\tIBM\n\n\t\n\tDuration\n\t:\n\tAUG 2011 –  AUG 2014\n\n\t\n\tTechnology\n\t:\n\tInformatica Powercenter 9.1,UNIX(Reflection) & Teradata V2R6\n\n\t\n\tRole\n\t:\n\tSoftware Developer\n\n\n\nDescription\n\nIBM announced that Watson software system's first commercial application would be for utilization management decisions in lung cancer treatment at Memorial Sloan Kettering Cancer Center, New York City, in conjunction with health insurance company WellPoint. This Project is all about the rehousing of their current existing project into Teradata platform. Previously Client was using Oracle Data base for storing their information. Client  have relied on our analytics for the insights and strategies. They need to make or enable the best decisions for their patients, clients, beneficiaries, members, partners, customers, and consumers, with our healthcare-specific expertise and solutions in managing complex and disparate health-related data. They needed a way to migrate Healthcare reporting data from many systems to a single system of record\nEnterprise Data Warehouse and Research Depot (EDWard) is sourced directly from Operational Systems for most of the sources. Main purpose of EDWard is to act as a source to provide a structure, one source of truth, to meet the different reporting and analysis needs of the company. Primary goal of EDWard is to\n· Source data directly from Operational Sources\n· Apply Transformation rules consistently across sources\n· Load data with a latency of <  24 hrs from source\n· Act as one source for Enterprise reporting and analysis purpose\n· Provide opportunity to retire regional Data Warehouses\n· Claims, Membership, Product & Provider data loaded for all core source systems\n· Build out other Key Strategic data including Member Month, Consumer Profile and Groupers etc\nEdward’s key value to IBM Watson is in the evolution of information assets by transforming disparate data to value-added information\n\nResponsibilities:\n.\n· Worked in data Extraction, Transformation and Loading from source to target system using BTEQ, FastLoad and MultiLoad\n· Created appropriate indexes depend on the table situation and requirement\n· Did data reconciliation in various source systems and in Teradata\n· Involved in loading of data into Teradata from legacy systems and flat files using BTEQ, FAST Load scripts\n· Performance tuning, including collecting statistics, analyzing explains & determining which tables needed statistics\n· Extensively used ETL to load data from Oracle and Flat files to Data Warehouse\n· Extensively used Informatica client tools-Source Analyzer, Warehouse designer, Mapping designer, Mapplet Designer, Transformation Developer, Informatica Repository Manager, Facets application\n· Created Informatica mappings using Source Qualifier, Expression, Lookup (connected and unconnected), Aggregate, Update Strategy, Joiner, Normalizer and Filter transformations\n· Experienced with coordinating cross-functional teams, project management and presenting technical ideas to diverse groups & Proven ability to implement technology based solutions for business problems\n\nStrength\n\n· Hard Working and Sincere\n· Good decision making and analytical skills\n· Able to handle people in a very efficient way\n· Good process knowledge, Quick learner, Team player\n\n\nPersonal Details\n\nLinguistic Proficiency\t:   English, Hindi, Oriya, Telugu\nHobbies\t\t \t:   Motivational videos, Music, Reading, Spend time in Internet \nLanguage \t\t:   English, Hindi, Telugu, Odia\n\nDeclaration\n\nI hereby declare that the above information furnished is true to the best of my knowledge and belief\n\n1\n\n1","annotation":null,"extras":null,"metadata":{"first_done_at":1564142149000,"last_updated_at":1564205448000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "Curriculum Vitae\n\t          KPM Mounika\n\n\n\n                                                                                     KPM Mounika\n\tObjective\n\n\nTo work in a pragmatic way, where I can show my talent and enhance my skills to meet the required goals and    objectives of the organization with full integrity and zest.\n\n\tWork Summary\n\n\nA technocrat with 3 years and 6 months of experience in Advanced Analytics, Model Building, Predictive Analysis and data visualization.Presently working with HCL Technologies, as a Data Analyst. An energetic, self-motivated team member with excellent communication and presentation skills always ready for new technologies and challenges.\n\n\tStatistical and Analytical Skills\n\n\n· Have clear understanding of machine learning concepts.\n· Worked on Linear Regression, Logistic Regression, Decision Tree, Random forest,KNN and SVM algorithms using Python.\n· Worked on unsupervised learning algorithms using K-means clustering, hierarchical clustering and DB Scan.\n· Worked on Dimensionality reductions techniques like PCA.\n· Worked on NLP using nltk package.\n· Data visualization using seaborn in Python.\n· Feature engineering in Python. Missing value and outlier handling, Transforming variable, Creating new variables, Reshaping data using packages like pandas,numpy for regression and classification models.\n \n\tTechnical Skills\n\n\nProgramming Languages : Python,SQL\nTools Used                          : Anaconda, Jupyter Notebook\nHardware Platforms         : Data Science Virtual Machine, Windows Server 2012 R2.\n\n\tProfessional Experience\n\n\nProject- Deutsche Bank Advanced Analytics\nClient : Deutsche Bank\nTools : Anaconda,Jupyter , SQL\nTeam Size : 3\nOrganization : HCL Technologies\n\n\nRoles and Responsibilities\n· Understanding the business problems.\n· Analyze large datasets and Data Cleaning.\n· Calculated the optimum interest rate for the bank customers applying for loan.\n· Performed regression to suggest the amount of loan to be given the customers depending on their previous behaviour.\n· Performed classification for HR attrition to estimate the people that will be leaving the company.\n· Performed regression to estimate the credit card limit.\n· Building the models on the datasets and testing the performance of the models using Cross Validation.\n· Communicate the results of analyses, modeling, and tests through data visualization.\n\nProject- CareOregon Data Analytics\nClient : CareOregon\nTools : Anaconda,Jupyter,SQL\nTeam Size : 3\nOrganization: HCL Technologies\nRoles and Responsibilities\n· Understanding the business problems.\n· Analyze large datasets and Data Cleaning.\n· Worked on the Insurance plan clustering according to the customers.\n· Worked on the clustering problem on which kind of customers to be targeted for contacting regarding the insurance plan.\n· Worked on Customer Churn  to analyse which customers will cancel their contracts in near furture.\n· Communicate the results of analyses, modeling, and tests through data visualization.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\tEducation\n\n\t\n\tQualification\n\tCollege\n\t Board/University\n\tYear of passing\n\tPercentage\n\n\tB.E. (E.C.E)\n\tSRKR Engineering College\n\tAndhra University\n\t       2015\n\t         82.6\n\n\tIntermediate(12th)\n\tNarayana Junior College\n\tBoard of Intermediate Education\n\t       2011 \n\t         95.8\n\n\tS.S.C(10th)\n\tLittle Flower School\n\tBoard of Secondary Education\n\t       2009\n\t         89.9\n\n\n\n\n\n\n\tPersonal Profile\n\n\nFather’s Name\t\t:  Katakam Jogi Bala Venkateswara Rao\nDate of Birth\t\t\t:  24-07-1994\nSex\t\t\t\t:  Female\nMarital Status\t\t:  Single\nNationality\t\t\t:  Indian\t\nLanguages\t\t\t:  Telugu, English, Hindi.\n\n\tContact Details\n\n\n\n  Mobile : 9980333101\t\t\t\tMail.ID : kpmmounica@gmail.com\n\n I hereby declare that the information furnished above is true to the best of my knowledge.\n\n Date: \t\t      \t\t\t\t                           KPM Mounika\n\n\n\t\n\tPage 3 of 3\n\tDated: 04/14/19","annotation":[{"label":["Name"],"points":[{"start":3832,"end":3842,"text":"KPM Mounika"}]},{"label":["Email_Address"],"points":[{"start":3671,"end":3690,"text":"kpmmounica@gmail.com"}]},{"label":["Mobile_No"],"points":[{"start":3647,"end":3656,"text":"9980333101"}]},{"label":["Degree"],"points":[{"start":3111,"end":3122,"text":"B.E. (E.C.E)"}]},{"label":["Skills"],"points":[{"start":2466,"end":2468,"text":"SQL"}]},{"label":["Tools"],"points":[{"start":2458,"end":2464,"text":"Jupyter"}]},{"label":["Tools"],"points":[{"start":2449,"end":2456,"text":"Anaconda"}]},{"label":["Skills"],"points":[{"start":1674,"end":1676,"text":"SQL"}]},{"label":["Tools"],"points":[{"start":1664,"end":1670,"text":"Jupyter"}]},{"label":["Tools"],"points":[{"start":1655,"end":1662,"text":"Anaconda"}]},{"label":["Tools"],"points":[{"start":1454,"end":1469,"text":"Jupyter Notebook"}]},{"label":["Tools"],"points":[{"start":1444,"end":1451,"text":"Anaconda"}]},{"label":["Skills"],"points":[{"start":1402,"end":1404,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":1395,"end":1400,"text":"Python"}]},{"label":["Skills"],"points":[{"start":1301,"end":1305,"text":"numpy"}]},{"label":["Skills"],"points":[{"start":1294,"end":1299,"text":"pandas"}]},{"label":["Skills"],"points":[{"start":1235,"end":1256,"text":"Creating new variables"}]},{"label":["Skills"],"points":[{"start":1212,"end":1232,"text":"Transforming variable"}]},{"label":["Skills"],"points":[{"start":1168,"end":1173,"text":"Python"}]},{"label":["Skills"],"points":[{"start":1135,"end":1140,"text":"Python"}]},{"label":["Skills"],"points":[{"start":1083,"end":1086,"text":"nltk"}]},{"label":["Skills"],"points":[{"start":1073,"end":1075,"text":"NLP"}]},{"label":["Skills"],"points":[{"start":1056,"end":1058,"text":"PCA"}]},{"label":["Skills"],"points":[{"start":886,"end":891,"text":"Python"}]},{"label":["Skills"],"points":[{"start":857,"end":859,"text":"KNN"}]},{"label":["Skills"],"points":[{"start":843,"end":855,"text":"Random forest"}]},{"label":["Skills"],"points":[{"start":828,"end":840,"text":"Decision Tree"}]},{"label":["Skills"],"points":[{"start":807,"end":825,"text":"Logistic Regression"}]},{"label":["Skills"],"points":[{"start":788,"end":804,"text":"Linear Regression"}]},{"label":["Skills"],"points":[{"start":749,"end":764,"text":"machine learning"}]},{"label":["Years_of_Experience"],"points":[{"start":360,"end":379,"text":"3 years and 6 months"}]},{"label":["Name"],"points":[{"start":128,"end":138,"text":"KPM Mounika"}]},{"label":["Name"],"points":[{"start":28,"end":38,"text":"KPM Mounika"}]}],"extras":null,"metadata":{"first_done_at":1564045235000,"last_updated_at":1564045235000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "3\n\n\nVINAY KUMAR\n\nSoftware Engineer                                             E-Mail: developer.vinay.28@gmail.com\n   Mobile No. :   +91-9480390679 /      \n                           +91-9631479899 \n\n                                                                     \nObjective:\n\n     Aim to be associated with a Progressive organization that gives me the scope to share my knowledge and skills in accordance with the latest trends and be a part of team that dynamically works towards the growth of organization and gives the satisfaction thereof.\nProfessional Profile:\n\n· Having 3  years of Total diversified IT experience in Developing JAVA and J2EE   \n      Applications.\n· Hands on Experience in Spring Framework.\n· Hands on Experience in Hibernate Framework.\n· Proven Experience in J2EE Technologies (JDBC, JSP, Servlets).\n· Hands on Experience in Web Services\n· Hands on Experience in Micro Services using Spring Boot.\n· Good Knowledge in Angular JS.\nWork Experience:\n\n· Software Developer in Dynpro India Pvt Ltd, Currently Working for Our Client IBM India Pvt Ltd in client Location, Bangalore\n· Software Engineer in WiFi Networks Pvt Ltd, Bangalore from  July 2016  to  May 2018\n\nTechnical Skills:\n\n\tJava Technologies :\n\tJava, JSP, Servlets, JDBC, Spring, Hibernate, Micro Services, Web Services\n\n\tFrameworks :\n\tHibernate, Spring with Spring Boot\n\n\tWeb Technologies :\n\tHTML, Java script, Ajax, CSS, Angular Js\n\n\tDatabases :\n\tOracle12g , MySQL, MongoDB\n\n\tWeb Server :\n\tTomcat, Weblogic12 c\n\n\tRepository:\n\tGIT\n\n\tBuilding Tool:\n\tMaven.\n\n\tTesting Tools:\n\tJunit\n\n\tIDE :\n\tNetBeans, Eclipse, Spring Tool Suite(STS)\n\n\tOS :\n\tLinux , Windows\n\n\n\n\n\n\nProjects:\n\n· Project #1 : 304160b\n           Technologies         :           Java, Micro Services with Spring Boot, Json, MongoDB,            \n                                                        \n           Platform                 :           Linux (Kubernets Cluster)\n           Role                         :          Developer\n           Team size               :          8\n           Client                      :          AT&T\n\nDescription: The Functionality of 304160b is, here we are collecting data from the different sources through EGF (Event Generation Framework) when any change event will occurs in different Source level.\n304160b Modules Include:\n· CDC (Change Data Capture) is capturing the new Event Occurred in different Source level.\n· Doc (Document Object Constructor) is listening to CDC event through Kafka and it will prepared Json Data according the CDC event  which was occurred in different  source level for different domains and publish to respective domains DMAPP.\n· Different Domains Micro Services will subscribe to respective DMAPP Messages and according to event it will perform different operation and store into respective mongo collection.\n· AssetInventoryCache will receive all the domains data and put it into the Mongo collection.\nResponsibilities: \n· Involved in the Client requirements discussion & Analysis \n· Coordination with client, coding and end to end testing. \n· Involved in all project planning & designing.\n· Involved in Daily Scrum Meetings and status calls for project\n\n· Project #2 : 302266\n           Technologies         :           Java, Micro Services with Spring Boot, Json, MongoDB,            \n                                                        \n           Platform                 :           Linux (Kubernets Cluster)\n           Role                         :          Developer\n           Team size               :          8\n           Client                      :          AT&T\n\nDescription: The Functionality of 302266, We are  retrieving  the topology data for different Assets from the MongoDB collection for Billing System (BilledServiceInventoryCache).\n\n302266 Modules Include:\n· CDC (Change Data Capture) is capturing the new Event Occurred in Source level.\n· Doc (Document Object Constructor) is listening to CDC event through Kafka and it will prepared Json Data according the CDC event  which was occurred in source level for respective Asset domain and publish to DMAPP.\n· Asset will subscribe the event and store into MongoDB collection.\n· BilledServiceInventoryCache call the AssetTopology api to collect the topology information’s.\n \nResponsibilities: \n· Involved in the Client requirements discussion & Analysis \n· Coordination with client, coding and end to end testing. \n· Involved in all project planning & designing.\n· Involved in Daily Scrum Meetings and status calls for project.\n\n· Project #3 : TATA USP(Universal Subscription Platform)\nTechnologies \t               :\t Java, JSP/Servlets, Hibernate, Mysql, XML, Tomcat \nPlatform\t\t:\t Linux (Cent OS 6)\nRole\t\t               :\t Developer\nTeam size\t\t:\t 3\n Client \t\t               :\t Tata Tele Services Ltd.\n\nDescription:  WiFi Universal Subscription Platform has innovative features for the subscribers to subscribe through IVR, SMS, and USSD. This also has the provision for user to manage his/her activation/ deactivation, charging and renewal information for VAS services. The same can be done on portal where user should register to manage his account.\nUSP Modules Include:\n· OBD Platform - For free outbound calling, promotion module for CRBT marketing activities (with advanced call patch application and integrated dedicated charging proxy).\n· Customer Care IVR- To handle all subscribers’ calls related to CRBT queries and monitoring.\n· SMS Application for Promotions, subscription reminders, notifications etc.\n· USSD Application for Promotions, subscription, notifications etc.\n· Customer Care Web Interface for handling customer queries and deactivation requests. And Automatic daily report as per the formats shared by client for all the modules.\n\nUniversal Subscription Platform Features:\n· Activation/ Deactivation of VAS service.\n· Dynamic menu creation.\n· SMS / USSD notifications to subscriber on service status.\n· White list content/service for specific subscribers based on his/her profile.\nResponsibilities: \n· Involved in the Client requirements discussion & Analysis.\n· Coordination with client. \n· Involved in Overall project planning, designing & coding.\n· Designing of Database for modules.\n\n· Project #4 : COLLECT CALL\nTechnologies \t:\t Java, JSP/Servlets, Hibernate, MariaDB, Tomcat \nPlatform\t\t:\t Linux (Cent OS 6)\nRole\t\t               :\t Developer\nTeam size\t\t:\t 3\n Client \t\t               :\tEmirates Telecommunication Group Company\n                                               ( Etisalat  Afghanistan)\nDescription:  WiFi Collect Call is a very nice product/service for the subscribers. Incase if A party having zero balance but they can talk with to B party. User can manage his/her activation/deactivation, charging, renewal information via IVR, USSD, and SMS. \nCollect Call Modules Include:\n· IVR- To handle all subscribers’ calls related to Collect Call service activation/deactivation and monitoring.\n· SMS Application for Billing, Subscription status, notifications etc.\n· USSD Application to handle all subscribers for Collect Call service activation/deactivation.\n· Customer Care Web Interface for handling customer queries and deactivation requests. And reports as per the formats shared by client for all the modules.\nCollect Call Platform Features:\n· Activation/ Deactivation of Collect Call service.\n· Dynamic menu creation.\n· SMS / USSD / IVR notifications to subscriber on service status.\nResponsibilities: \n· Involved in the Client requirements discussion & Analysis \n· Coordination with client, coding and end to end testing. \n· Involved in all project planning & designing.\n· Designing of Database for modules.\n· Involved in sprint planning Meetings.\n· Involved in Daily Scrum Meetings and status calls for project.\n\n· Project #5 : SMSC\nTechnologies \t               :\t Java, JSP/Servlets, Hibernate, Mysql, Tomcat \nPlatform\t\t:\t Linux (Ubuntu)\nRole\t\t               :\t Developer\nTeam size\t\t:\t 3\nClient \t\t               :\tSomtel Somalia (Hargeisa, Mogadishu, Punt land)\nDescription: WiFi SMSC is a product for Somtel active subscribers for sending Bulk SMS\nSMSC Modules Include:\n· Web Interface for creating campaign and bulk uploading.\n· Reports as per the formats shared by client for the modules.\n \nSMSC Platform Features:\n· Creating unique campaign for bulk uploading active subscriber mobile number for sending SMS. \nResponsibilities: \n· Involved in the Client requirements discussion & Analysis \n· Coordination with client, coding and end to end testing. \n· Involved in all project planning & designing.\n· Designing of Database for modules.\n· Involved in Daily Scrum Meetings and status calls for project.\n· Project #6 : Just4U\n    Technologies \t               :\t Java, JSP/Servlets, Hibernate, Mysql, Tomcat \n    Platform\t\t:\t Linux (Ubuntu)\n    Role\t\t               :\t Developer\n    Team size\t\t:\t 3\n    Client \t               :\tSomtel Somalia\n                                               (Hargeisa, Mogadishu, Punt land)\nDescription: It is an API integration which is hosted in MTN cloud server for promoting VAS services to the active subscriber through IVR and OBD (OBD Platform – Outbound bulk calling promotion ).For client MTN Liberia.\nResponsibilities: \n· Involved in the Client requirements discussion & Analysis \n· Coordination with client, coding and end to end testing. \n· Involved in all project planning & designing.\n· Designing of Database for modules.\n· Involved in sprint planning Meetings.\n· Involved in Daily Scrum Meetings and status calls for project.\n\nEducation:\n\n· MCA From T.John Institute Of Technology, Bangalore (Karnataka) in  2015.\n\n  Personal Interest And Activities:\n\nExtracurricular:\n· Participated in Project Expo - 2015 held on 9th April at our college T.John Institute Of Technology, bengaluru on project title “Online University” during  My 6th Sem of MCA and won First prize.\n\nHobbies:\n· Watching News, Watching Discovery Shows, Internet Surfing, Watching movies, Listening music, Cooking, Outing.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  Personal Profile:\n\n\nFathers’ Name            :  Manoj Kumar\nMother’s Name          :  Shaila Devi\nDate of Birth               :  28th July 1990\nCurrent Address         :  Jay Bheema Nagar, 1st Main, 1st Cross\n                                         BTM 1st Stage, Bengaluru, Karnataka 560068\n\nPermanent Address: C Block Q.No-493 Bhuli Nagar\n\t\t              Dhanbad, Jharkhand-828104\n\nLanguage Known      :  Hindi, English\nMobile No.                 :  +91-9480390679 / +91-9631479899\n\n\n      I hereby declare that the above information is true to the best of my knowledge.\n\n\nPlace: Bangalore                                                                                 Best regards       \nDate:                                                                                                          (vinay kumar)","annotation":[{"label":["Mobile_No"],"points":[{"start":10472,"end":10485,"text":"+91-9631479899"}]},{"label":["Mobile_No"],"points":[{"start":10455,"end":10468,"text":"+91-9480390679"}]},{"label":["Degree"],"points":[{"start":9832,"end":9835,"text":"MCA "}]},{"label":["Degree"],"points":[{"start":9532,"end":9535,"text":"MCA "}]},{"label":["Operating_Systems"],"points":[{"start":8770,"end":8774,"text":"Linux"}]},{"label":["Skills"],"points":[{"start":8745,"end":8750,"text":"Tomcat"}]},{"label":["Skills"],"points":[{"start":8727,"end":8735,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":8717,"end":8724,"text":"Servlets"}]},{"label":["Skills"],"points":[{"start":8713,"end":8715,"text":"JSP"}]},{"label":["Skills"],"points":[{"start":8707,"end":8710,"text":"Java"}]},{"label":["Operating_Systems"],"points":[{"start":7868,"end":7872,"text":"Linux"}]},{"label":["Skills"],"points":[{"start":7847,"end":7852,"text":"Tomcat"}]},{"label":["Skills"],"points":[{"start":7829,"end":7837,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":7819,"end":7826,"text":"Servlets"}]},{"label":["Skills"],"points":[{"start":7815,"end":7817,"text":"JSP"}]},{"label":["Skills"],"points":[{"start":7809,"end":7812,"text":"Java"}]},{"label":["Operating_Systems"],"points":[{"start":6318,"end":6322,"text":"Linux"}]},{"label":["Skills"],"points":[{"start":6297,"end":6302,"text":"Tomcat"}]},{"label":["Skills"],"points":[{"start":6277,"end":6285,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":6267,"end":6274,"text":"Servlets"}]},{"label":["Skills"],"points":[{"start":6263,"end":6265,"text":"JSP"}]},{"label":["Skills"],"points":[{"start":6257,"end":6260,"text":"Java"}]},{"label":["Operating_Systems"],"points":[{"start":4682,"end":4686,"text":"Linux"}]},{"label":["Skills"],"points":[{"start":4661,"end":4666,"text":"Tomcat"}]},{"label":["Skills"],"points":[{"start":4638,"end":4646,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":4628,"end":4635,"text":"Servlets"}]},{"label":["Skills"],"points":[{"start":4624,"end":4626,"text":"JSP"}]},{"label":["Skills"],"points":[{"start":4618,"end":4621,"text":"Java"}]},{"label":["Skills"],"points":[{"start":4157,"end":4163,"text":"MongoDB"}]},{"label":["Skills"],"points":[{"start":3717,"end":3723,"text":"MongoDB"}]},{"label":["Operating_Systems"],"points":[{"start":3416,"end":3420,"text":"Linux"}]},{"label":["Skills"],"points":[{"start":3290,"end":3296,"text":"MongoDB"}]},{"label":["Skills"],"points":[{"start":3271,"end":3281,"text":"Spring Boot"}]},{"label":["Skills"],"points":[{"start":3251,"end":3264,"text":"Micro Services"}]},{"label":["Skills"],"points":[{"start":3245,"end":3248,"text":"Java"}]},{"label":["Skills"],"points":[{"start":2670,"end":2683,"text":"Micro Services"}]},{"label":["Operating_Systems"],"points":[{"start":1899,"end":1903,"text":"Linux"}]},{"label":["Skills"],"points":[{"start":1773,"end":1779,"text":"MongoDB"}]},{"label":["Skills"],"points":[{"start":1754,"end":1759,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":1734,"end":1747,"text":"Micro Services"}]},{"label":["Skills"],"points":[{"start":1728,"end":1731,"text":"Java"}]},{"label":["Operating_Systems"],"points":[{"start":1636,"end":1642,"text":"Windows"}]},{"label":["Operating_Systems"],"points":[{"start":1628,"end":1632,"text":"Linux"}]},{"label":["Tools"],"points":[{"start":1563,"end":1567,"text":"Junit"}]},{"label":["Tools"],"points":[{"start":1538,"end":1542,"text":"Maven"}]},{"label":["Tools"],"points":[{"start":1516,"end":1518,"text":"GIT"}]},{"label":["Skills"],"points":[{"start":1488,"end":1499,"text":"Weblogic12 c"}]},{"label":["Skills"],"points":[{"start":1480,"end":1485,"text":"Tomcat"}]},{"label":["Skills"],"points":[{"start":1456,"end":1462,"text":"MongoDB"}]},{"label":["Skills"],"points":[{"start":1449,"end":1453,"text":"MySQL"}]},{"label":["Skills"],"points":[{"start":1437,"end":1445,"text":"Oracle12g"}]},{"label":["Skills"],"points":[{"start":1411,"end":1420,"text":"Angular Js"}]},{"label":["Skills"],"points":[{"start":1406,"end":1408,"text":"CSS"}]},{"label":["Skills"],"points":[{"start":1400,"end":1403,"text":"Ajax"}]},{"label":["Skills"],"points":[{"start":1387,"end":1397,"text":"Java script"}]},{"label":["Skills"],"points":[{"start":1381,"end":1384,"text":"HTML"}]},{"label":["Skills"],"points":[{"start":1347,"end":1357,"text":"Spring Boot"}]},{"label":["Skills"],"points":[{"start":1335,"end":1340,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":1324,"end":1332,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":1295,"end":1306,"text":"Web Services"}]},{"label":["Skills"],"points":[{"start":1279,"end":1292,"text":"Micro Services"}]},{"label":["Skills"],"points":[{"start":1268,"end":1276,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":1260,"end":1265,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":1254,"end":1257,"text":"JDBC"}]},{"label":["Skills"],"points":[{"start":1244,"end":1251,"text":"Servlets"}]},{"label":["Skills"],"points":[{"start":1239,"end":1241,"text":"JSP"}]},{"label":["Skills"],"points":[{"start":1233,"end":1236,"text":"Java"}]},{"label":["Skills"],"points":[{"start":1212,"end":1215,"text":"Java"}]},{"label":["Skills"],"points":[{"start":915,"end":925,"text":"Spring Boot"}]},{"label":["Skills"],"points":[{"start":894,"end":907,"text":"Micro Services"}]},{"label":["Skills"],"points":[{"start":856,"end":867,"text":"Web Services"}]},{"label":["Skills"],"points":[{"start":820,"end":827,"text":"Servlets"}]},{"label":["Skills"],"points":[{"start":815,"end":817,"text":"JSP"}]},{"label":["Skills"],"points":[{"start":809,"end":812,"text":"JDBC"}]},{"label":["Skills"],"points":[{"start":746,"end":754,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":703,"end":708,"text":"Spring"}]},{"label":["Years_of_Experience"],"points":[{"start":583,"end":590,"text":"3  years"}]},{"label":["Mobile_No"],"points":[{"start":184,"end":197,"text":"+91-9631479899"}]},{"label":["Mobile_No"],"points":[{"start":134,"end":147,"text":"+91-9480390679"}]},{"label":["Email_Address"],"points":[{"start":87,"end":114,"text":"developer.vinay.28@gmail.com"}]},{"label":["Name"],"points":[{"start":4,"end":14,"text":"VINAY KUMAR"}]}],"extras":null,"metadata":{"first_done_at":1564294114000,"last_updated_at":1564294114000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "LOPAMUDRA DASH\nContact No.: +91 7829623653\nE-Mail: \nlina.lopamudra@gmail.com\n\n                                                                    Senior ETL Developer \n                                                               (Informatica Power Center)\nPROFILE SUMMARY\n\n· 5.9 years of ETL and Data Integration experience in developing Informatica Power Center codes for Data Extraction, Data Transformation, Data Loading, Data Quality.\n· Experience in all the phases of Data warehouse life cycle involving Requirement Analysis, Design, Coding, Testing, Deployment, and Post-deployment support.\n· Profound indulgence in Requirement gathering and analysis phase. \n· Integrating various data systems like Salesforce Veeva, SAP CRM, Work Day, ECC, Siebel, AMD, Mendix etc.\n· Curating and cleansing data from different sources like Oracle, SQL server, SAP HANA and non-relational sources like flat files.\n· Experience in creating Transformations (Joiner, Sorter, Aggregator, Expression, Lookup, Router, Filter, Update Strategy, Sequence Generator etc) and Mappings using Informatica Designer and processing tasks using Workflow Manager to move data from multiple sources into targets.\n· Worked on file transfers, job scheduling and error handling using Unix shell scripting.\n· Extensively worked on Developing and Debugging, Performance Tuning, identifying and resolving performance bottlenecks in various phases of SDLC for the ETL codes.\n· Worked profoundly on Change Data Capture techniques.\n· Experience on Salesforce integration with Informatica Power Center and Informatica Cloud.\n· Testing the developed codes for optimal efficiency and zero defect along with supporting UAT, FUT , Production Deployment and Hypercare period post deployment. \n· Main areas of expertise are analyzing issues, developing and supporting Data Warehousing and ETL Integration projects as a Team Leader or an individual contributor.\n· Reflective domain knowledge on Pharmaceutical/Medical sector, gained over experience of serving market leaders like Boehringer Ingelheim and Medtronic.\n· Self-motivated and able to set effective priorities to achieve immediate and long-term goals and meet operational deadlines. \nEMPLOYMENT DETAILS\n\n\n   Sep’ 16 - Till Date with Medtronic India Pvt. Ltd. \n   Jun’ 27 - Sep’ 16 – Capgemini India Pvt. Ltd.\n\nEDUCATION\n\n\n· B.Tech (Civil Engineering) – C.V.Raman College of Engineering, B.P.U.T, Odisha – 8.4 CGPA.\n· 12th (Mathematics, Physics, Chemistry) – Kalinga Bharati Residential College, Odisha – 81.1%.\n· 10th – Saraswati Vidya Mandir, Cuttack, Odisha  – 91.4%.\n\n\nTECHNICAL SKILLS\n\n   ETL            :  Informatica Power Center 10.2.0/10.1.1/9.6.1/9.1.0,Power Center Mapping Architect for Visio, \n                          Informatica Data Quality, Informatica Cloud Services\n   Tools         :  Veeva, Jira, Tivoli Workload Scheduler ,Toad for Oracle, Sql  Developer, Putty, WinSCP, Workbench , \n                          SAP HANA Studio, Microsoft SQL Server Management Studio, Demand Tracking System, \n                          Service  Now, Tableau\n   Databases :  Oracle, SAP HANA, SQL Server\n   Platforms  :  Microsoft Windows, Linux, SFDC\n   Scripting   :  Unix shell scripting, SQL, Batch scripting\n   Domain Knowledge/Experience   : Pharmaceutical/Medical\n  \n\n\n\n\n\n\n\n\n\nPROJECT SUMMARY\n\nEMPLOYER:                          MEDTRONIC\n\nTitles:                                      Sunshine, AMD Integration, Agile Map, LDD (Local Discovery Directory),\n                                                  Open Orders, Process Mining , Cornerstone, Fieldglass Integration, \n                                                  Enhancements and fixes on multiple projects \nDesignation:                          IT Developer\nDuration:                                Sep’ 16 –  Present\nTeam Size:                             4\nTools:                                      Informatica Power center 9.6.1,10.1.1,10.2.0, Unix, Tivoli\nDatabase:                               Oracle 11g, Oracle ExaData, SAP HANA, Sql Server\n\n\n   ROLES & RESPOSIBILITIES\n\n\n· Understanding of the Source System, Business requirement.\n· Developing the codes to extract, transform and load the data from various source systems to different targets using Informatica Power Center Mapping Designer, Power Center Workflow Manager, Power Center Workflow Monitor and Informatica cloud services.\n· Integrating different data systems.\n· Testing and analyzing the codes in case of failure and performing unit testing.\n· Developing and testing UNIX and Batch scripts for ease of work.\n· Tune the codes for better performance and reducing bottlenecks.\n· Code deployment for different non-production environments and keeping the environments and codes deployment-ready before production move.\n· Resolving issues and supporting during UAT,FUT phases.\n· Timely delivery of modules with best possible efficiency.\n· Supporting the codes post production deployment during hyper-care period.\n· Smooth hand-shake with support team.\n· Having the documentation ready e.g. Unit Testing Doc, SOP for support team with change details, Technical Specification documents update, Lessons learnt document for future references etc.\n· Providing KT to the new team members in the team and be the SME.\n· Indulgence in end to end processes.\n· Knowledge sharing sessions within the team to have understanding of other project member’s assignment.\n· Capturing the important information over video sessions.\n· Understanding the functional overview of projects.\n· Connecting the dots to know the bigger picture of each assignment by collaborating with other teams .\n\n\nEMPLOYER:                          CAPGEMINI INDIA PVT. LTD.\n\nTitles:                                      NxGn CRM Global Roll Out , ODS VDS support(STL L3), BI Audit Project, ODS EU,\n                                             Boehringer_BI_CHC Animal Health Cust First\nClient:                                      Boehringer Ingelheim\nDesignation:                          Consultant P2\nDuration:                                Jun’ 13 –  Sep’ 16\nTeam Size:                              50\nTools:                                       Informatica Power center 9.1.0/9.6.1, Unix, Toad, Sql Developer, Jira, Apex Data loader\nDatabase:                                Oracle 10g/11g\n\n\n\n  ROLES & RESPOSIBILITIES\n\n· Understanding the scope of  work and putting together the business requirements.\n· Developing the ETL,unix,sql codes to integrate data from various systems.\n· Testing and analyzing the codes in case of failure and performing unit testing.\n· Timely and effective delivery of modules aiming for defect free resolutions.\n· Understanding the functional overview of project to develop domain insight.\n· Resolving production defects raised by Users and L2 Team before SLA breach during tenure of L3 support.\n· Involved in Peer review and analyzing the data issue tickets and transferring the knowledge to Users with functionality handled for the issues. \n· Working Knowledge of VM Service Manager - BISM Tool for tickets.\n\n\n\n\n\n· Worked on developing CR’s based on User Requirements and released to Production with the timelines.\n· Working profoundly on code enhancements.\n· Implementing the oracle wallet into the shell scripts to hide the DB passwords and parameterization of user names and passwords for FTP.\n· Manual Testing of Oracle Procedures/Scripts exclusively for a project on Veeva integration.\n· Worked as Module Lead for Veeva Integration for region EMEA.\n· Preparing and provide delivery estimates and responsible for distribution of tasks.\n· Develop and modifying the Informatica codes from source till landing, landing to staging and stg to ODS and ODS to VEEVA (Salesforce).\n· Support the users for user acceptance testing.\n· Published various Documents such as Source to Target Mapping (STM), Play Book, Migration Documents etc.\n· Helping the testing team with UAT loads.\n· Fixing the JIRA bugs raised during UAT.\n\n\nACHIVEMENTS & RECOGNITIONS\n\n· Received Spot Awards twice from Capgemini for good performance and before-time delivery.\n· Applauded with “Star Award”  in Capgemini for the dedicated effort in “Boehringer-Ingelhiem” projects.\n· Received token of appreciations from team leads and managers for being an active team player and continuous efforts for building up a collaborated friendly work environment.\n· Recognized with “Rising Star” award for taking up responsibilities as a Module Lead with less years of experience in IT and more in the projects of client “Boehringer-Ingelheim”.\n· Sincere and pushing beyond the line efforts were awarded with early promotion as “Consultant” during the service period in Capgemini. \n· Implemented Microsoft Visio for Power Center to bulk create informatica codes which has saved time tremendously bringing down the cost incurred for projects.\n· Much appreciated by the users and managers for best documentation practices, Shift-Left approach and analysis skills.\n· Storing the available information and knowledge in form of videos to help the new joiners adhere to the processes and project details easily and effectively.\n· Rewarded with 500 Medtronic points for capabilities towards issues identification and fixtures.\n\n\n\n\nDeclaration\n\n\tI declare that the information mentioned above is true to the best of my knowledge.\n\n\nDate :\nPlace: Bangalore                                                                                                                                                                     Lopamudra Dash","annotation":[{"label":["Tools"],"points":[{"start":7466,"end":7470,"text":"Veeva"}]},{"label":["Tools"],"points":[{"start":7419,"end":7423,"text":"Veeva"}]},{"label":["Skills"],"points":[{"start":7364,"end":7369,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":6437,"end":6439,"text":"ETL"}]},{"label":["Skills"],"points":[{"start":6292,"end":6297,"text":"Oracle"}]},{"label":["Tools"],"points":[{"start":6228,"end":6231,"text":"Jira"}]},{"label":["Skills"],"points":[{"start":4245,"end":4268,"text":"Informatica Power Center"}]},{"label":["Skills"],"points":[{"start":4015,"end":4022,"text":"SAP HANA"}]},{"label":["Skills"],"points":[{"start":3999,"end":4004,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":3987,"end":3992,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":3214,"end":3228,"text":"Batch scripting"}]},{"label":["Skills"],"points":[{"start":3209,"end":3211,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":3187,"end":3206,"text":"Unix shell scripting"}]},{"label":["Operating_Systems"],"points":[{"start":3164,"end":3167,"text":"SFDC"}]},{"label":["Operating_Systems"],"points":[{"start":3157,"end":3161,"text":"Linux"}]},{"label":["Operating_Systems"],"points":[{"start":3148,"end":3154,"text":"Windows"}]},{"label":["Skills"],"points":[{"start":3110,"end":3119,"text":"SQL Server"}]},{"label":["Skills"],"points":[{"start":3100,"end":3107,"text":"SAP HANA"}]},{"label":["Skills"],"points":[{"start":3092,"end":3097,"text":"Oracle"}]},{"label":["Tools"],"points":[{"start":3068,"end":3074,"text":"Tableau"}]},{"label":["Tools"],"points":[{"start":3054,"end":3065,"text":"Service  Now"}]},{"label":["Tools"],"points":[{"start":3003,"end":3024,"text":"Demand Tracking System"}]},{"label":["Tools"],"points":[{"start":2963,"end":3000,"text":"Microsoft SQL Server Management Studio"}]},{"label":["Tools"],"points":[{"start":2946,"end":2960,"text":"SAP HANA Studio"}]},{"label":["Tools"],"points":[{"start":2907,"end":2915,"text":"Workbench"}]},{"label":["Tools"],"points":[{"start":2899,"end":2904,"text":"WinSCP"}]},{"label":["Tools"],"points":[{"start":2892,"end":2896,"text":"Putty"}]},{"label":["Tools"],"points":[{"start":2876,"end":2889,"text":"Sql  Developer"}]},{"label":["Tools"],"points":[{"start":2859,"end":2873,"text":"Toad for Oracle"}]},{"label":["Tools"],"points":[{"start":2832,"end":2856,"text":"Tivoli Workload Scheduler"}]},{"label":["Tools"],"points":[{"start":2826,"end":2829,"text":"Jira"}]},{"label":["Tools"],"points":[{"start":2819,"end":2823,"text":"Veeva"}]},{"label":["Skills"],"points":[{"start":2772,"end":2797,"text":"Informatica Cloud Services"}]},{"label":["Skills"],"points":[{"start":2746,"end":2769,"text":"Informatica Data Quality"}]},{"label":["Skills"],"points":[{"start":2677,"end":2716,"text":"Power Center Mapping Architect for Visio"}]},{"label":["Skills"],"points":[{"start":2626,"end":2649,"text":"Informatica Power Center"}]},{"label":["Skills"],"points":[{"start":2608,"end":2610,"text":"ETL"}]},{"label":["Degree"],"points":[{"start":2339,"end":2364,"text":"B.Tech (Civil Engineering)"}]},{"label":["Skills"],"points":[{"start":1845,"end":1847,"text":"ETL"}]},{"label":["Skills"],"points":[{"start":1539,"end":1562,"text":"Informatica Power Center"}]},{"label":["Skills"],"points":[{"start":1429,"end":1431,"text":"ETL"}]},{"label":["Skills"],"points":[{"start":1253,"end":1272,"text":"Unix shell scripting"}]},{"label":["Skills"],"points":[{"start":852,"end":859,"text":"SAP HANA"}]},{"label":["Skills"],"points":[{"start":840,"end":842,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":832,"end":837,"text":"Oracle"}]},{"label":["Tools"],"points":[{"start":718,"end":722,"text":"Veeva"}]},{"label":["Skills"],"points":[{"start":340,"end":363,"text":"Informatica Power Center"}]},{"label":["Skills"],"points":[{"start":290,"end":292,"text":"ETL"}]},{"label":["Years_of_Experience"],"points":[{"start":277,"end":285,"text":"5.9 years"}]},{"label":["Skills"],"points":[{"start":232,"end":255,"text":"Informatica Power Center"}]},{"label":["Skills"],"points":[{"start":153,"end":155,"text":"ETL"}]},{"label":["Email_Address"],"points":[{"start":52,"end":76,"text":"lina.lopamudra@gmail.com\n"}]},{"label":["Mobile_No"],"points":[{"start":28,"end":41,"text":"+91 7829623653"}]},{"label":["Name"],"points":[{"start":0,"end":13,"text":"LOPAMUDRA DASH"}]}],"extras":null,"metadata":{"first_done_at":1564141017000,"last_updated_at":1564141017000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "Mangesh Vilas Khapre\t\t\t\t\t\t\n5.6 years of experience in IT industry\nContact No:-8087752115/8999937585, 0721-2570850\nEmail-ID:-mangesh.v.khapre@gmail.com\t\t\t\t\n\nObjective:-\nTo help implement the right cloud based solutions for the clients which speeds and eases their processes, also focusing on innovative offerings within the organization which allows me to grow, learn and lead.\n\nHighlights:-\n· 5+ years Salesforce development experience with 5 End-to-End and 3 enhancement/ Lightning migration projects\n· 7x Salesforce certified & 130+ Trailhead badges, including PD2, Service, Communities, Field Service Lightning cloud and super badges\n· 3 Career awards which includes Accenture’s highest rated ACE and Syntel’s value award Stretch.\n· 3 Integration, 2 Lightning Development projects experience with service cloud and communities\n· Experience in leading 5 resources in deliverables by assigning, guiding and prioritizing tasks ensuring quality.\n· InstaConnect: - Approachable, ability to view from others perspective, motivate and work in a collaborative team environment.\n· 360° view of the Technology-Industry-Business-Domain\n\nSkills and Competencies:-\n· Programmatic-Apex classes, triggers, Batch classes, Queueable classes, Test Classes, Salesforce APIs\n· Automation- Process Builder, Workflows, Validation rules, Formulas, Reports & Dashboards\n· Front-End- Visualforce, HTML5, Bootstrap\n· UX- JavaScript, jQuery\n· Frameworks- Lightning. AngularJS\n· Deployment- Force.com Migration (Ant), Change Sets, Packages\n· Integration- Web Services using Rest Api with OAuth 2.0 and Delegated Authentication, Apex Rest Services, JSON\n· Development tools- Eclipse IDE, Developer Console, Notepad++\n· Data Maintenance- Data Loader, Data Import wizard, dataloader.io\n· Project Management- Agile using JIRA, Agile Accelerator, Design and Functional Documentation.\n· Third-Party Tools- Workbench, Postman, Checkmarx, Conga Composer\n\nOrganization worked with:-\n· Accenture India Pvt. Ltd. : August 2017 – Present\n· Cognizant Technology Solutions : April 2016 – July 2017\n· Syntel Ltd. : November 2013 –  March 2016\n\nProject Experience:-\n\tMedical Information System – Phase 2\n\n\tSkills Used:-\n\tLightning, Apex, Service Cloud Communities\n\n\tBrief:-\n\tCustomer Portal to login queries and Internal Lightning pages development.\n\n\tKey Contributions:-\n\t· Developing Lightning Components\n· Designing and discussing flow of the portal with UI developer\n· Estimating and assigning tasks to the junior developers\n\n\n\n\n\n\n\n\tMedical Information System\n\n\tSkills Used:-\n\tApex, Visualforce, Integration, Leadership, Conga Composer\n\n\tBrief:-\n\tCustomized application based on Service cloud to answer customer queries about life sciences products. \n\n\tKey Contributions:-\n\t· Guided and trained junior developers implementing the right design.\n· Integration with Veeva Vault using rest api\n· Visualforce development for public facing site.\n\n\n\n\tEnterprise Model (Top Healthcare-Payer US)\n\n\tSkills Used:-\n\tSFDC, Apex, Visualforce, JavaScript, JSON\n\n\tBrief:-\n\tLarge payer plans processing project which includes many written Batches, Triggers, Workflows, and Integrations through third party. This system is highly scaled.\n\n\tKey Contributions:-\n\t· Worked on complex business requirements and identifying the right design approach \n· Integration with another Salesforce Org for Plan Sell status and Account and Contact Info.\n\n\n\n\tBusiness Bank Implementation\n\n\tSkills Used:-\n\tSFDC, Apex, Visualforce, Lightning, Integration, Aura Framework\n\n\tBrief:-\n\tA platform where Business Bank can perform their daily tasks. This project is focused on Lightning experience to the users with some customized components such as cards and dashboards. It also had integrations with external systems such as Customer Systems and Appointment Booking System.\n\n\n\tKey Contributions:-\n\t· Created custom lightning components\n· Integrated salesforce with Appointment Booking System using Rest.\n· Post deployment data upload using Data Loader.\n\n\n\n\tStrategic Introducer System\n\n\tSkills Used:-\n\tSFDC, Apex, Visualforce, AngularJS, Service Console, Bootstrap\n\n\tBrief:-\n\tA public facing community portal for the introducers to submit referrals developed in AngularJS and Bootstrap. It also had other users using standard salesforce functionalities such as Business Development Manager (BDM), Call Center Agent (CCA) etc. \n\n\tKey Contributions:-\n\t· Created/Enhanced Visualforce components with AngularJS and Bootstrap\n· Worked on Service Console user module with triggers, workflows, validation rules.\n\n\n\n\tGuided Reference System\n\n\tSkills Used:-\n\tSFDC, Apex, Visualforce, JQuery, Bootstrap\n\n\tBrief:-\n\tA platform to map HCC and ICD codes for Medicare Advantage Plans. Users of the system are Admin, Medical coder, and Auditor/Reviewer and Payer auditor/Reviewer.\n\n\n\tKey Contributions:-\n\t· Created custom history tracking and exception handling\n· Customized search functionality\n· Involved in data modelling decisions\n\n\n \n\tFall Risk Assessment\n\n\tSkills Used:-\n\tSFDC, Apex, Visualforce, JQuery, HTML5\n\n\tBrief:-\n\tApplication to track old age patient’s tendency to fall from the hospital bed by using fall scale models.\n\n\tKey Contributions:-\n\t· Worked with triggers and validations\n· Added behavior to Visualforce pages using Jquery\n· Worked on point and click feature in Salesforce\n\n\n\n\n\tPlans Star Rating\n\n\tSkills Used:-\n\tSFDC, Apex, Visualforce, JQuery, HTML5\n\n\tBrief:-\n\tAn application where the business users can view the performance of the health plans and make decisions on them.\n\n\tKey\nContributions:-\n\t· Created Visualforce pages and components with Apex controllers\n· Defined sharing settings for the users\n· Developed a batch for complex business logic for the rating of plans.\n\n\n\nEducation:-\n1. 2009 -2013 B.E. Information Technology Aggregate of 8 Semesters- 72.23%  from SGB Amravati University \n1. 2008-2009 H.S.C 78.50 % from Maharashtra State Board.\n1. 2006-2007 S.S.C. 76.61 % from Maharashtra State Board\n\nAchievements and Certifications:-\n1. Salesforce Certified Platform Developer 1 and 2, Administrator and Service and Sales Cloud Consultant.\n1. Won Kudos award in Syntel for display of efficient work in first year.\n1. Won Value award Stretch given for above expectation performance in the second year\n1. Won Accenture’s highly prestigious ACE award, in the first year working with the organization.\n1. Gained appreciations from top managers for creating and demonstrating capabilities which led to acquiring new projects.\n1. Earned TOP rank in class in 2nd, 3rd and 5th Semesters.\n1. Won 3rd Prize in National Level Sports Engineering Exhibition- Presentation.\n1. Selected in Top 20 students in Apropos Gumption Scholarship & Talent Search. (Maharashtra).\n1. Microsoft Certified Technology Associate-Software Development Fundamentals.\n\nInterests:-\n1. Reading Books\n1. Psychology, Philosophy, Brands and Businesses\n\nLanguages Known:-\n1. English\n1. Hindi\n1. Marathi\n1. French (Basics)\n\nPersonal Information:-\n1. Date of Birth- 30th  October 1991\n1. Gender- Male\n1. Passport Number- L1804781\n1. Passport Validity up to- 14/05/2023\n1. Current Location- Pune\n\nPermanent Address:-\nC/o Ashok Tehare, Narendra Colony,\nNear Vimal Nagar, Near Farshi Stop, Amravati, Maharashtra, India, 444606\n\nOther Connects:-\nmangesh.khapre@hotmail.com\nMangesh.khapre@yahoo.com\nAbout.me/mangesh_khapre\nhttps://in.linkedin.com/in/mangeshkhapre\nPage 1 of 3","annotation":null,"extras":null,"metadata":{"first_done_at":1564122029000,"last_updated_at":1564122029000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "TextControl1\n\nmansakaurk93@gmail.com\n\n8179468784\n\nBangalore, India\n\nData Scientist and Machine Learning\nEngineer\n\nMachine Learning Engineer in health care domain with specialization in NLP, Deep Learning\n\nWORK EXPERIENCE\n\n09/2017 – Present\n\nData Scientist\nIQVIA\n\nBuilt Neural Machine Translation framework using Deep\nLearning Models\nDesigned a Predictive Engine for Patient Dropout in a\nClinical Trial\nBuilt a Sentimental/Feedback Analysis algorithm to\nclassify the polarity of the opinion expressed by the\ncustomer’s feedback on the services provided by\nQuintiles.\nBuilt data set for Electronic Data Capture and Subject\nData Tabulation Model (SDTM) as per CDISC (Clinical\nData Interchange Standards consortium) standard to\nfacilitate model development and business reporting.\n\n08/2015 – 09/2017\n\nSoftware Engineer\nTechMahindra\n\nWorked on Entity Aggregation and Navigation (EAN)\ninitiative which enabled us to quickly and efficiently\nperform navigation, linkage and accurate error\nresolution and report generation across the gamut of\nour families of entities.\nDesigned a database for retail domain.\n\nEDUCATION\n\n08/2011 – 04/2015\n\nElectronics and Communication\nV R Siddhartha Engineering College\n\nGPA - 7.5\n\n08/2009 – 04/2011\n\nScience\nSri Chaitanya Educational Institutions\n\n96%\n\n06/2008 – 04/2009\n\nSSC\nAtkinson Hr. Secondary School\n\n93%\n\nSKILLS & COMPETENCIES\n\nData Science Machine Learning NLP\n\nDeep Learning Python scripting Spotfire\n\nSQL Agile Methodology\n\nACHIEVEMENTS & CERTIFICATES\n\nIQVIA Hackathon Champion\nWinner and Lead Presenter of IQVIA Hackathon 2018.\n\nNatural Language Processing Nanodegree\n(09/2018 – 01/2019)\nNatural Language Processing Nanodegree program from Udacity.\n\nDeep Learning Specialization (08/2018 – 01/2019)\nSpecialization in Deep Learning from Coursera\n\nData Science Specialization (01/2018 – 06/2018)\nSpecialization in Data Science using python from Data Camp.\n\nBadminton Championship\nWinners of Doubles and Mixed double Badminton Corporate\ntournament\n\nCricket Corporate Tournamet (03/2018 – Present)\nWinners of Women s Cricket Tournament .\n\nLANGUAGES\nEnglish\n\nHindi\n\nPunjabi\n\nTelugu\n\nINTERESTS\n\nMansa kaur\nKochhar\n\nAchievements/Tasks\n\nAchievements/Tasks\n\nBadminton Cricket Reading","annotation":null,"extras":null,"metadata":{"first_done_at":1564137942000,"last_updated_at":1564138983000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "Resume\nNeha Rastogi\nMobile: +91 9990352441\nEmail:neharastogi077@gmail.com\n\n\nObjective:\nTo seek the challenging position in Software industry that needs innovation, creativity, dedication and enable me to continue to work in a challenging and fast paced environment, leveraging my current knowledge and fostering creativity with many learning opportunities\n\nProfile Summary:\n\n· Working as Senior Software Engineer, API development, Rest API Spring boot, Microservices, Hibernate , Redis.\n· Worked as a developer in complete software development lifecycle of a multi-tier application based upon J2EE standards and NEI development, BST, Catalog, Order Management using Core JAVA, J2EE.\n· Having 7.6 years of experience in object-oriented analysis, design, and development.\n· Open Source development background- Pursuing industry best practices and advanced technologies in current project by following weblogs, forums, mailing lists and open source projects. \n· Brings excellent work ethic and motivation to the workplace.\nProfessional Experience\t\n\nCurrent Employer:\tBroctagon Fintech Group \nCurrent Designation:\tSenior Software Engineer\nCurrent Role:\t\t\tSoftware Developer.\nTotal Experience:\t\t7.6 Years.\n\nSkill Sets\t\n\nProgramming Languages and Technologies\n· Unix \n· Core Java, Multithreading \n· Spring Boot , Hibernate \n· HTML\n· MySQL\n· Microservices \n· RestAPI\n· RabbitMQ\n· Comptel Instantlink\n· Comptel BST\n· Comptel NEI\n· Comptel Catalog\n· Comptel Order Management\nDevelopment IDEs\n· Eclipse.\n· NetBean\nWeb Servers, Containers and J2EE Application Servers\n· Apache Tomcat 6.0\nDatabases\n· SQL\n· PL/SQL\n· Redis(No SQL)\n\n\n\n\n\n\n\nVersioning Tool\n\n· CVS\n· Clear Case\n· Mercurial\n· Git Hub\n\n\nExperience  \n\n· 4/2017 – \nBroctagon Fintech Group\nSenior Software Engineer\n           Project:   Trading Platform\n                            Technology: Spring Boot, Hibernate, MySQL, Rabbit MQ, Redis, Rest services\n                             Team Size: 4\n                             Duration: April 2017 – present\n                             Role: Developer\n                             Department: R & D\n            Description: A Trading Platform is a software through which investors and traders can open, close and manage market positions. A Trading platform is the software that allows investors and traders to place trades and monitor accounts .Oftentimes, trading platform will incorporate market analysis software as well, whereby traders and investors can chart the markets and perform stock screens. \n            Responsibilities: \n· Requirement  and designing document\n· Analysis and effort estimation\n· Designing , implementation, Unit testing \n\n\n· 11/2013 –4/2017  \nComptel Communication India Private Limited, Noida\nSolution Specialist\nProject 1: \tNEI DEVELOPER\nTechnology: Core Java\nTeam Size: 2.\nDuration: November 2013 – Apr-2017\nRole: Developer\nDescription: Comptel, itself is a very big name in telecom domain. We provide many solution to service providers. As Solution specialist we are supposed to develop API that can connect our product with Network elements like HLR, PCRF, IN, and HSS etc. These network elements communicate in various protocols and Java is the main language used for development.\n\nResponsibilities: \n· Requirement elicitation and design documents.\n· Detailed analysis estimating effort.\n· Onsite communication to update requirements in Conceptual Document.\n· Coding, Unit Testing, Integration Testing and Code Review.\n· Manual Testing of NE interface.\n· Client Interaction\n· BST Implementation\n· Catalog Implementation\n· Order Management\n\nClients: \n· MCI IRAN (Developer)\n· NAWRAS  (Developer)\n· TELEFONICA COLOMBIA (Developer , NEI, BST)\n· STC  (Developer NEI , BST, OrderManagment)\n· Sabafone  (Developer NEI, BST)\n· Bharti, India (Developer ,  BST)\n· PTCL  (Developer , BST, NEI)\n· Robi Bangladesh  (Developer  IL8.2)\n· PLDT-Phillppnes (Developer, BST, Order Management)\n\n\n· 10/2012 – 10/2013.\nBirlasoft India Ltd, Noida\nSoftware Engineer \nProject 2: \tCPF\nClient: CPF (Close Premium Finance), U.K\nTechnology: Java, spring 2.5, Hibernate MyEclipse 10, Oracle SQL Developer, CVS\nTeam Size: 6.\nDuration: May 2013 – 10/2013\nRole: Developer and Tester.\nDescription: CPF is the leading provider of insurance finance for the general insurance market in the United Kingdom and Ireland. Premium Finance is for both business and individuals, it helps business and individuals pay their insurance premiums by spreading the cost over the regular installments.\nThe application deals with premium financing and it captures all the details of the stakeholders i.e. brokers, underwriters etc. It also has the functionality of uploading, approving various types of loans, depending upon the client requirements in conjunction with the company’s business plans.\n\nResponsibilities: \n· Requirement elicitation and design documents.\n· Detailed analysis estimating effort.\n· Coding, Unit Testing, Integration Testing and Code Review.\n· Manual Testing of Web application.\n\nProject 3: \tDATA MAPPER\nClient: GESBI\nTechnology: JAVA, J2EE, SQL and PLSQL / Net beans, Eclipse, Putty, spring, Hibernate, SQL Developer,       Tomcat  \nTeam Size: 8.\nDuration: Oct 2012 – April 2013\nRole: Developer and Tester.\nDescription: This is an independent product. This product provides various services that include file uploading, Delite cases raised by customers and transactional sms viewing and complaints this is based on MVC 2 architecture. Struts 1.0 framework, Spring, Hibernate, XML, ANT, Core Java, JDBC, SQL and HTML.\n\nResponsibilities: \n· Requirement elicitation and design documents.\n· Detailed analysis estimating effort.\n· Onsite communication to update requirements in Conceptual Document.\n· Coding, Unit Testing, Integration Testing and Code Review.\n· Manual Testing of Web application.\n\n·  02/2012– 10/2013.\nCavisson System , Noida\nSoftware Engineer\n\nProject 4:               Netstorm the capacity analysis tool\n\nDescription: NetStorm is a testing tool, which can test the capacity of a web application by emulating web-based real world users accessing the web application. It generates the load and creates the log. On the basis of these logs, the capacity of the Server can be determined. Capacity refers to the load which server can hold in. For example if there is a rediff server and the response of the same needs to be marked when 2K users access it at the same point of time, then the information about the number of users that are responded in time, number of users that are in waiting queue and the ones that have been discarded can be composed with the help of this tool. \nNetStorm emulates real users by simulating Web browser.\n\n\nResponsibilities: \n· Creating design documents.\n· Detailed analysis estimating effort.\n· Coding, Unit Testing, Integration Testing and Code Review.\n\n\n02/2011 – 08/2011\nCavisson System, Noida\nSystems Engineer Trainee.\n·  Training at Cavisson System.\n\n\nPersonal Skills\n              Good Problem solving abilities, verbal and written communication skills, ability to deal with people, willingness to learn and a hard worker. I have strong analytical and logical skills. My work interest shows in progressive and innovative environment.\nLeadership qualities, excellent client interaction and great presentation skills. \n\n\nQualification\n\n· Completed MCA with an exposure of implementing software programming skills in designing & developing various systems.\n· Skilful in working on Core JAVA and JAVA Swings by using Windows and Linux.\n· Completed various projects during my Organizational Curriculum.\n· Excellent interpersonal, communication skills with proficiency at grasping new technical concepts quickly and utilizing the same in a productive manner. \n· Participated in various sports event at school level & college level.\n\n\n\nPersonal Details \n\nFather’s name: Mr. Rajeev Rastogi\nDate of Birth: 28 July, 1987\nPermanent Address: 341-D Regal shipra sun city Indirapuram, near shipra mall, Ghaziabad\nHobbies: Reading Novels, Listening songs","annotation":[{"label":["Degree"],"points":[{"start":7302,"end":7304,"text":"MCA"}]},{"label":["Skills"],"points":[{"start":5519,"end":5522,"text":"HTML"}]},{"label":["Skills"],"points":[{"start":5511,"end":5513,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":5494,"end":5502,"text":"Core Java"}]},{"label":["Skills"],"points":[{"start":5473,"end":5481,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":5111,"end":5113,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":5100,"end":5108,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":5059,"end":5061,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":5049,"end":5051,"text":"SQL"}]},{"label":["Tools"],"points":[{"start":4110,"end":4112,"text":"CVS"}]},{"label":["Skills"],"points":[{"start":4095,"end":4097,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":4064,"end":4072,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":2779,"end":2787,"text":"Core Java"}]},{"label":["Skills"],"points":[{"start":1863,"end":1867,"text":"MySQL"}]},{"label":["Skills"],"points":[{"start":1852,"end":1860,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":1839,"end":1849,"text":"Spring Boot"}]},{"label":["Tools"],"points":[{"start":1675,"end":1681,"text":"Git Hub"}]},{"label":["Tools"],"points":[{"start":1663,"end":1671,"text":"Mercurial"}]},{"label":["Tools"],"points":[{"start":1650,"end":1659,"text":"Clear Case"}]},{"label":["Tools"],"points":[{"start":1644,"end":1646,"text":"CVS"}]},{"label":["Skills"],"points":[{"start":1604,"end":1616,"text":"Redis(No SQL)"}]},{"label":["Skills"],"points":[{"start":1595,"end":1600,"text":"PL/SQL"}]},{"label":["Skills"],"points":[{"start":1589,"end":1591,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":1441,"end":1464,"text":"Comptel Order Management"}]},{"label":["Skills"],"points":[{"start":1423,"end":1437,"text":"Comptel Catalog"}]},{"label":["Skills"],"points":[{"start":1409,"end":1419,"text":"Comptel NEI"}]},{"label":["Skills"],"points":[{"start":1395,"end":1405,"text":"Comptel BST"}]},{"label":["Skills"],"points":[{"start":1373,"end":1391,"text":"Comptel Instantlink"}]},{"label":["Skills"],"points":[{"start":1362,"end":1369,"text":"RabbitMQ"}]},{"label":["Skills"],"points":[{"start":1352,"end":1358,"text":"RestAPI"}]},{"label":["Skills"],"points":[{"start":1335,"end":1347,"text":"Microservices"}]},{"label":["Skills"],"points":[{"start":1327,"end":1331,"text":"MySQL"}]},{"label":["Skills"],"points":[{"start":1320,"end":1323,"text":"HTML"}]},{"label":["Skills"],"points":[{"start":1307,"end":1315,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":1293,"end":1303,"text":"Spring Boot"}]},{"label":["Skills"],"points":[{"start":1275,"end":1288,"text":"Multithreading"}]},{"label":["Skills"],"points":[{"start":1264,"end":1272,"text":"Core Java"}]},{"label":["Operating_Systems"],"points":[{"start":1256,"end":1259,"text":"Unix"}]},{"label":["Years_of_Experience"],"points":[{"start":692,"end":700,"text":"7.6 years"}]},{"label":["Skills"],"points":[{"start":468,"end":476,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":453,"end":465,"text":"Microservices"}]},{"label":["Email_Address"],"points":[{"start":49,"end":72,"text":"neharastogi077@gmail.com"}]},{"label":["Mobile_No"],"points":[{"start":28,"end":41,"text":"+91 9990352441"}]},{"label":["Name"],"points":[{"start":7,"end":18,"text":"Neha Rastogi"}]}],"extras":null,"metadata":{"first_done_at":1564048210000,"last_updated_at":1564048210000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "Nilesh Kumar\nMail : nilesh.it447@gmail.com                                                         Mobile No :+91-9008630725\n\n\t\n\t\n\n\tTechnology Professional with over 8+ Years of experience in software development comprising:\n· Expert in JAVA, REST API, Kafka, Spark and related Technologies\n\n· Having good experience in Micro service Architecture.\n\n· Java, Scala, REST API, Hibernate, Oracle, Spring Core, Spring MVC\n· Big Data Technologies (Spark, Kafka, Hadoop, HIVE)\n\n· CI and Code quality tools (Hudson, Sonar, Jira)\n\n· Version control- GIT , BitBucket\n· Excellent track-record of implementing technology solutions to increase operational efficiency.\n\n· Have extensive experience in the SDLC process of analysis, design, development\n· Good in problem solving using Data Structures and Algorithms.\n· Using tools to achieve AGILE goals.\n\n· Mentoring juniors, code reviews\n\n· Excellent and fast at self-learning of any Technologies.\n· Good hands on experience in deployment tools like docker and Kubernates\n\n\tWork Experience:\n\t\n\n\tORACLE India Private Ltd.                                    October 2015–till Date                \n\nSenior Member Technical Staff\n\nPROJECT: DATACATALOG\nDuration: 1.5 year\n\nData Catalog is cloud service for discovering, finding, organizing, enriching and tracing data assets on Oracle cloud and beyond, in order to provide efficient and governed data exploitation.\n\nRoles and Responsibilities:\n\n· Worked on design and development of metadata Harvest module to connect to different data sources system like databases, object storage, Kafka, cloud storage and read the metadata from different files systems.\n· Develop wrapper scheduling framework on top of cron job. To trigger schedule job for metadata harvest.\nTechnologies: Kafka, spark, Hadoop, REST web services, JAVA, Maven, Git, Docker, Kubernetes , IntelliJ\nPROJECT: DataflowML\nDuration: 2 year\n\nDFML is a cloud platform that provides easy to use services to define manage and execute smart pipelines to process and transform data. It's a 'Lambda Application Platform with Real-time Analytics' as a Service based on Apache Spark, Machine Learning, Mesos, Kafka, YARN, DockerContainers, and written in Scala                 .\n\nRoles and Responsibilities:\n\n· Worked on runtime and scheduling framework and actively involved in coding/development of ingestion service, execution of runtime pipeline and managing the lifecycle of the pipelines using spark and Kafka.\n· Worked on Development of scheduling framework REST wrapper on top of Chronos/Mesos scheduling framework which will handle the error/exception reporting.\n\nTechnologies: Kafka, spark, REST web services, Scala , JAVA, Chronos, mesos Maven, Git, Docker                             \nEMC Data Storage Systems, India               September 2010 – 2015 (5 yrs. exp.)\nAssociate consultant II\n\nPROJECT: CeTi LifeCare\nClient: Apollo , Manipal\n\nDuration: 8 Month\n\n· LifeCare Project is the Cloud based enterprise application, which is for checking of disease of patient and generate the alert if he/she require consulting the doctor based on measurement. A Medhub device (Bluetooth enabled) which takes the measurement from different medical devices and process the measurement and give the alert if it’s required.\n\nRoles and Responsibilities:\n· Worked on processing module to process the different type of vital, and call the CDSS web services for generating the alert.\n· Design the architecture of processing module and mediation flow based on spring integration.\nTechnologies used: Spring integration, Spring data JPA, Spring web Services, Maven, Git.\n\nPROJECT: SGX\nClient: Singapore Stock Exchange \nDuration: 8 Month\n\n· SGX Project is the Cloud based enterprise application, which manages the stock related activity in fast and more reliable by uses of best architecture and best practice which helps and supports the clients to make use of Java and spring related application development.\n\nRoles and Responsibilities:\n\n· Worked on Audit trail framework, User, Customer and Spring Security module.\n\n· Utilizing and applying best practices of all spring related technology.\nTechnologies used: vFabric SQLFire, Spring, JIRA, Maven, JavaScript, Github,\n\nSonar.\nAchievement: Got Silver Prize from company for good contribution in the project.\nPROJECT: EMC Cloud\nClient: EMC Initiative/ Product\n\nDuration: 6 Month\n\n· EMC Cloud competency makes the technology Cloud enabled by using EMC cloud products like Cloud foundry and applying best practices of Platform as a Service using Cloud foundry. It helps and supports EMC internal clients to make use of Cloud platform and Java application development.\n\nRoles and Responsibilities:\n\n· Ramp up on vFabric SQLFire.\n\n· POCs on Spring Data, SQLFire, Spring MVC, Spring Security\n\n· JavaScript frameworks including Bootstrap.js\n\n· Utilizing, and applying best practices of Platform as a Service using Cloud foundry.\n\nTechnologies used: vFabric SQLFire, Spring, JIRA, Maven, JavaScript, Github.\nPROJECT: Data As a Service (DAAS)\n\nRole: Developer / Analysis, Design & Implementation.\nClient: EMC Initiative/ Product\nDuration: 6 Month\n\nSKILLS: Activiti BPM, Spring MVC, GreenPlum, Web Services , Attivio , GlobalIds\n\nDAAS is providing end user the application that has capability to provide on time data with minimal time and maximum accuracy. The data is for business people that do not understand the database technologies but want to access data for their analysis and reporting work.\n\nThis program includes:\n\nThis project includes workflow management that is done by Activiti BPM an Open source Business Process Management engine. Activiti is integrated with data governance tool GlobalIds through web services to get the Metadata. Once got the metadata Activiti flow goes further to Attivio through their API and creates a universal index for the particular table that exists in any other database. At the end an external web table is created in GreenPlum i.e. Analytical database with the URL provided by Attivio for universal data access.\n\nResponsibilities:\n\nAnalyzing the core functionality of the project and requirements during the requirements analysis phase of projects. Set up Activiti BPM process and create the process diagram to decide the flow of entire application. Integrate Activiti with GlobalIds and Attivio for access data at application level, sending request to create index for Attivio and finally create external table in GP i.e. GreenPlum.\n\nFull flow testing and Activiti BPM work flow management.\n\nPROJECT: Paetec Network – EMC, Bangalore\n\nClient: Paetec (wind stream), USA.\n\nDuration:  6 Month\nObjective of this project is to migrate the Existing legacy database ‘AS400’ to Oracle database.\n\nPAETEC is telecom domain project which will maintain wired network. Each module will provide new scope for maintaining Paetec wired network and its components like Site, Shelf, Slot, and Port. And migrating the network database ‘AS400’ to oracle database.\n\n· Developed and analyze the design.\n\n· Management of telecom inventory helps Paetec wired network to be more flexible.\n\n· Maintenance of such large wired networks manually is very difficult. We develop codes to automate all the manual process for high performance.\n\n· Removal of unwanted and free slots and ports greatly increase network performance.\n\nResponsibilities:\n\n· Analyzing the design document and requirements during the requirements analysis phase of projects.\n\n· Develop the assigned requirement and design the job according to requirement in talend tool.\n\n· Develop the modules and working as per project need.\n\n· Writing, Reviewing and Executing codes.\n\n· Working with SQL, PL/SQL code to fetch the proper data from Paetec Databases i.e. used by Paetec to manage network data.\n\nPROJECT: Lightspeed (Wired Network Management)\n\nRole: Developer / Analysis, Design & Implementation.\nClient: ATnT LABS/ EMC Consulting Integrated Product\n\nDuration: 1.8 years\nSKILLS:  Core JAVA, SOAP, XML, SQL, Junit\n\nLIGHTSPEED is telecom domain project which will maintain ATnT wired network. Each module will provide new scope for maintaining AT&T wired network and its components like Site, Shelf, Slot, and Port. Also manages Inventory system that contains the data i.e. Granite Inventory.\n\nResponsibilities:\n\nAnalyzing the design document and requirements during the requirements analysis phase of projects. Develop the assigned requirement and unit test the same. Develop and unit test modules are working as per project need and those services are giving desired output. Writing, Reviewing and Executing codes. Working with SQL PLUS to fetch the proper data from Granite telcodia inventory i.e. used by AT&T to manage network data. Resolving the issues about project requirements (Software, Hardware, Resources).\n\nProduct Training: Information Storage Management(ISM), Spring Source.\nAwards and Recognitions: Got Silver award as individual contributor in Java based Project. Appreciated For the good knowledge of Core JAVA, Recognized as fast learner of core Spring and J2EE.\n\n\t\n\t\n\n\tEDUCATION:\n\tInstitute   : Gandhi Institute of Engineering and Technology Orissa.\n\nUniversity : Biju Patnaik University of Technology (BPUT) Raurkela\n\nGrade        : B.TECH (IT) in 2006-2010 with CGPA 7.48\n\n\t\n\t   \n\n\tCERTIFICATIONS:\n\n\t· Oracle Certified java professional (java-1.6).\n· Oracle Certified Associates (Oracle9i) (SQL, PL/SQL).\n\n\tPERSONAL:\n\tDate of Birth:  13th  April 1983\nPlace of Birth:  Motihari , India\n\nCitizenship:  Indian\n\nPassport No: J5027309,Valid till 2020\nHobbies: Social Working, Listening to music, Enjoy with friends & Team\nStrength: Enthusiasm, Devotion to work, Quick learner, Sincere & Responsible\n\n\n\nDECLARATIONS: I vouch for the authenticity of the above information.\n                                                                                                                                                                                                                                                                                Place: Bangalore                                                                                                      Nilesh Kumar","annotation":[{"label":["Name"],"points":[{"start":10130,"end":10141,"text":"Nilesh Kumar"}]},{"label":["Certifications"],"points":[{"start":9325,"end":9376,"text":"Oracle Certified Associates (Oracle9i) (SQL, PL/SQL)"}]},{"label":["Certifications"],"points":[{"start":9276,"end":9320,"text":"Oracle Certified java professional (java-1.6)"}]},{"label":["Degree"],"points":[{"start":9206,"end":9216,"text":"B.TECH (IT)"}]},{"label":["Skills"],"points":[{"start":8976,"end":8979,"text":"JAVA"}]},{"label":["Skills"],"points":[{"start":7939,"end":7942,"text":"JAVA"}]},{"label":["Tools"],"points":[{"start":4968,"end":4973,"text":"Github"}]},{"label":["Tools"],"points":[{"start":4187,"end":4192,"text":"Github"}]},{"label":["Tools"],"points":[{"start":3591,"end":3593,"text":"Git"}]},{"label":["Tools"],"points":[{"start":2694,"end":2699,"text":"Docker"}]},{"label":["Tools"],"points":[{"start":2689,"end":2691,"text":"Git"}]},{"label":["Skills"],"points":[{"start":2676,"end":2686,"text":"mesos Maven"}]},{"label":["Skills"],"points":[{"start":2667,"end":2673,"text":"Chronos"}]},{"label":["Skills"],"points":[{"start":2661,"end":2664,"text":"JAVA"}]},{"label":["Skills"],"points":[{"start":2653,"end":2657,"text":"Scala"}]},{"label":["Skills"],"points":[{"start":2634,"end":2650,"text":"REST web services"}]},{"label":["Skills"],"points":[{"start":2627,"end":2631,"text":"spark"}]},{"label":["Skills"],"points":[{"start":2620,"end":2624,"text":"Kafka"}]},{"label":["Skills"],"points":[{"start":2521,"end":2527,"text":"Chronos"}]},{"label":["Skills"],"points":[{"start":2443,"end":2447,"text":"Kafka"}]},{"label":["Skills"],"points":[{"start":2433,"end":2437,"text":"spark"}]},{"label":["Skills"],"points":[{"start":2188,"end":2192,"text":"Scala"}]},{"label":["Tools"],"points":[{"start":2155,"end":2160,"text":"Docker"}]},{"label":["Skills"],"points":[{"start":2142,"end":2146,"text":"Kafka"}]},{"label":["Tools"],"points":[{"start":1815,"end":1820,"text":"Docker"}]},{"label":["Tools"],"points":[{"start":1810,"end":1812,"text":"Git"}]},{"label":["Skills"],"points":[{"start":1797,"end":1800,"text":"JAVA"}]},{"label":["Skills"],"points":[{"start":1778,"end":1794,"text":"REST web services"}]},{"label":["Skills"],"points":[{"start":1763,"end":1767,"text":"spark"}]},{"label":["Skills"],"points":[{"start":1756,"end":1760,"text":"Kafka"}]},{"label":["Skills"],"points":[{"start":1564,"end":1568,"text":"Kafka"}]},{"label":["Skills"],"points":[{"start":449,"end":453,"text":"Kafka"}]},{"label":["Skills"],"points":[{"start":357,"end":361,"text":"Scala"}]},{"label":["Skills"],"points":[{"start":253,"end":257,"text":"Kafka"}]},{"label":["Skills"],"points":[{"start":237,"end":240,"text":"JAVA"}]},{"label":["Years_of_Experience"],"points":[{"start":166,"end":173,"text":"8+ Years"}]},{"label":["Mobile_No"],"points":[{"start":110,"end":123,"text":"+91-9008630725"}]},{"label":["Email_Address"],"points":[{"start":20,"end":41,"text":"nilesh.it447@gmail.com"}]},{"label":["Name"],"points":[{"start":0,"end":11,"text":"Nilesh Kumar"}]}],"extras":null,"metadata":{"first_done_at":1564047657000,"last_updated_at":1564047657000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "Pankaj Kumar\n\t\n\tHSR Layout, Sector 2\n\t\n\n\t\n\t\n\t\n\tBengaluru/Bangalore, KA 560102\n\t\n\n\t\n\tMu-Sigma,\n\t\n\t(+91) 94808 62869\n\t\n\n\t\n\t\n\t\n\tPK179288@gmail.com\n\t\n\n\t\n\tData Engineer\n\t\n\t\n\t\n\n\t\n\t(Oct, 2018 – Present)\n\t\n\t\n\t\n\t\n\n\t\n\t\n\t\n\t\n\t\n\t\n\n\n\n\n\nEXECUTIVE SUMMARY\n\n\nI am a results-oriented accomplished professional serving the industry with 6.7 years of extensive experience in Data Engineering (D3 Stack), Devops/CICD using GITLAB, Docker & Kubernetes, Cloud Computing, EDGE Computing, IoT Analytics, Hadoop Ecosystem, Data Integration & Business Intelligence Tools.\n· I am a fundamental, strong and a promising professional.\n\n· Market/self-driven consistent performer, working towards scaling/expanding the business.\n\n· Can organize constructive meetings with clients to understand the business trends and requirements.\n\n· Strengths: quick learner, adaptive, highly motivated, growth & development oriented.\n\n· Demonstrated experience with excellent problem-solving skills, solutioning, high analytical and interpersonal skills.\n· I am an IT geek with a never-ending zeal and hunger to learn and grow.\n\n\nTECHNICAL SUMMARY\n\n\n· End-to-end design, solutioning and deployment experience of data engineering pipelines on AWS and Open Stack or on premise.\n\n· Research & implementation experience on IoT Analytics using edge devices such as Raspberry PI, Beacons, Cameras, Sensors, etc.\n· Experience in implementing, fully managed high-end Hadoop, Spark and\n\nKafka clusters on bare metal and cloud using YARN and Kubernetes/Docker for research and development purpose.\n\n· Aware, engaged & experienced in designing and developing end to end\n\nData Engineering pipelines on Lambda and Kappa architectures on cloud and on-premise landscape: Data Modelling, Data Ingestion, Data Scrubbing, Data Curation, Data Wrangling/Crunching/Munching, Data Governance & Security, Data Lineage, Data Sciences, Decision Sciences (D3 Stack), Data Consumption, DevOps (CICD), AI/ML and IoT.\n\n· 2+ years of relevant hands-on experience on Hadoop Technologies - HDFS, Map Reduce, Pig, Hive, Sqoop, Flume, Oozie/Zookeeper, HBase, Cassandra.\n\n· 5+ years of relevant hands-on experience in Teradata – architecture, utilities, performance tuning, stored procs, etc.\n\n· 4+ years of relevant hands-on experience in Informatica PC, architecture, performance tuning, etc.\n· 1+ Year of relevant hands-on experience in Tableau & SAP BO.\n\n· 3+ Years of relevant experience on Unix/Linux/Python Scripting.\n\n· Strong conceptual knowledge on OOP’s & Structured programming languages– Core Java, C++, C, Python, etc.\n\n\nEDUCATIONAL QUALIFICATIONS\n\n\n\tDegree\n\tUniversity/College\n\tYear of Passing\n\tCGPA\n\n\tM.Tech\n\tBits Pilani [WILP/WASE] *\n\t2016\n\t6.85\n\n\tBCA\n\tGraphic Era University, Dehradun\n\t2012\n\t68.80%\n\n\t\n\t\n\t\n\t\n\n\t12th\n\tSGRR Public School, Dehradun\n\t2008\n\t60%\n\n\t10th\n\tSGRR Public School, Dehradun\n\t2006\n\t66%\n\n\n\nAll courses are Full Time\n\n*WILP stands for Work Integrated Learning Program/WASE – Wipro Academy of Software Excellence\n\n\n\n\nPROFESSIONAL COMPETENCIES\n\n\nSpark, PySpark\n\nKafka\n\nTeradata\n\nInformatica\n\nHadoop – Hive, Pig, Sqoop, etc\n\nGitlab/Github\n\nData Engineering\n\nCassandra, HBase, MongoDB\n\nShell scripting\n\nTableau\n\nHyperion Essbase\n\nDocker on Kubernetes\n\nJira\n\nJupyter/Zeppelin/Atom\n\nPython\n\nApplied Stats using R & SAS\n\nScala/Java/C++/C\n\n\nAcquired Skills\n\n\nIoT and Sensor Analytics\n\nMachine Learning\n\nDecision Sciences\n\nKubernetes and Docker\n\nCICD using DevOps\n\nWeb App Development\n\n\n\nNOTEWORTHY CREDITS\n\n\n· ‘Early Prodigy & Feather in Cap Award’.\n\n· Felicitated with ‘Peoples Champ Award winner’ for 4 years in a row.\n\n· Got rewarded with ‘Project Execution Excellence Award’ and ‘Long Service Award’ in 2017.\n\n· Major achievements include Client Satisfaction, multiple appreciations and accolades from clients.\n\n· Major contributor in Business development & Growth initiatives.\n\n· 3 things which best describes me – A Traveler, A reader (Technical and Non-Fiction) & A big foodie.\n\n\nPERSONAL DETAILS\n\n\nDate of Birth: 13-Nov-1990\tGender: Male\n\nNationality: Indian\nLanguages Known: English and Hindi\n\nCAREER PROGRESSION\n\n\n\nCompany - Mu-Sigma, Bengaluru Team - Mu-Sigma LABS (R&D wing of Mu-Sigma) Data Sciences Team (DEFT)\n\n\n\n\nRole: Data Engineer\n\nOctober 2018 – Till Date\n\nTeam Size: 14\n\n\n\nSpark    Kafka    NoSQL    Kubernetes    Docker   GCP   AWS\n\n\n\n\nAlteryx\n\n\n\nSAS    Python & R    Teradata\n\n\n\n\nOpenStack\n\n\n\n\nData Engineering\n\n\n\n\nInformatica BDM\n\n\n\n\nAzure\n\n\n\nEDGE Computing   AWS Kinesis\n\n\n\n\nIOT Analytics\n\n\n\n\nResearch and Development wing of Mu-Sigma (DEFT):\n\nBeing part of LABS team in Mu-Sigma, I have helped the organization in growing and building endless capabilities around major and niche-end technologies of the current and future era. I have worked round the clock and helped multiple teams accelerate in Data engineering space. With an aim of business and revenue growth, I have been closely working with presales and sales team to help engage themselves more and more with clients in the space of Data Engineering or D3 Stack.\n\n· Worked on creating capabilities around real-time and batch-based data pipelines using Lambda and Kappa architectures on cloud and on-premise infrastructure. Heavily engaged in handling multiple clients on data engineering portfolios from different domains such as Telecom, Retail, Manufacturing, Banking, E-Commerce, Healthcare and from different geographies across the globe as well.\n\n· Worked with teams on operationalizing, curating and creating meaningful insights from data of up to 40+ TB’s by suggesting architectures, landscapes and deploying pipelines in client environments;\n\n· Implemented and created reusable artefacts from the research and experiments around multiple cloud vendors (AWS, GCP, Azure);\n\n· Benchmarked different open-source and proprietary tools on different API’s, platforms, versions using different resource managers;\n· Benchmarked Apache products like Spark, Flink, Kafka, etc. using YARN/Kubernetes on cloud and on-premise;\n\n· Created and shared a dozen of artefacts within the org and with the clients for showcasing capabilities in space of Data Engineering and cloud;\n\n· Created real-time and batch-based data pipelines on Lambda and Kappa architecture using Kafka, Spark, AWS Kinesis, Cassandra, Redshift and Python, keeping compliance, InfoSec and security protocols as top concern;\n\n\n\nClient: World’s Largest Networking Company [B2B] Project: Retail/Finance ASBI Domain: Manufacturing [NEPC] Metrics – Finance and Retail\n\n\n\n\nRole: Senior Data Analyst\n\nJune 2017 – Till Date\nTeam Size: 12\n\nWipro Limited, Bengaluru\n\n\n\n Hive\n\n\n\n Sqoop\n\n\n\n Teradata\n\n\n\n Cassandra\n\n\n\n Impala\n\n\n\n Oracle\n\n\n\n Informatica\n\n\n\n Spark\n\n\n\n Scala\n\n\n\n Python\n\n\n\n\nProject Description: Setting up batch and real time data pipeline for data analysis\n\nThis is a Teradata to Hadoop and SAP HANA migration project. The objective of this project is to induct the data into Hadoop environments from existing legacy systems [Teradata, Oracle, etc.]. The project deals with Finance and Revenue metrics. With source of truth as Teradata for business users, providing solutions to the customers where in optimal data ingestions techniques have to be laid down. Major challenges involved in this project are: first is to integrate various metrics from Teradata in a single Hadoop platform without hampering the data; secondly to keep up the current reporting system with incremental and parallel releases in Hadoop environment; implementing an automated synchronous data load mechanism to reduce the manual intervention while loading data to multiple platforms including Hadoop.\n\n\n\tClient: World’s leading electronics manufacturer [B2C/B2B]\n\t\n\tRole: Data Analyst\n\t\n\t\n\t\n\n\tProject: GBI TURBO & GBI SPA\n\t\n\t\n\t\n\t\n\tOct 2012 – May 2017\n\t\n\t\n\n\tDomain: Manufacturing\n\t\n\t\n\t\n\t\n\tTeam Size: 16\n\t\n\t\n\t\n\n\tMetrics - Finance/Retail/Operations/Sales\n\t\n\t\n\t\n\tWipro Limited, Bengaluru\n\t\n\t\n\n\tTeradata\n\tInformatica\n\tSAP BO\n\tEssbase\n\tTableau\n\tShell Scripting\n\tPython\n\tHive\n\tOracle\n\tCassandra\n\tSqoop\n\n\n\n\nProject Description: Global Business Intelligence [GBI] is an ecosystem of multiple Analytical tools and technologies which helps Business users to make decisions, forecasting, perform analysis, etc.\n\nThe objective of this project was to fill gaps in the reporting and analysis tools available to Finance analysts. The project will integrate units, revenue, and margin data into Essbase and automate reporting and analysis. This engagement started with providing world-class Essbase finance solutions using Informatica PC and Teradata as ETL Tools to the finance and retail business users. The project continues to provide seamless reporting experience.\n\nProjects/Application Implementations (Oct,2012 – May,2017):\n\n\tSPA (Ship Plan Attainment)\n\tDeveloped Corp Units\n\tSTF Actuals and Forecast\n\n\tEssbase DR Setup\n\tEQM [Essbase Quality Management]\n\tFAST [Finance Analytics Spreadsheet Tool]\n\n\tEssbase Alias Optimization\n\tRAST [Retail Analytics Spreadsheet Tool]\n\tFINOPEX [Financial Operating Expenses]\n\n\tDeveloped URM & GM/PL\n\tHyperion Upgrades and enhancements\n\tSupported EDW migrations/up-gradations","annotation":[{"label":["Skills"],"points":[{"start":8491,"end":8498,"text":"Teradata"}]},{"label":["Skills"],"points":[{"start":8472,"end":8482,"text":"Informatica"}]},{"label":["Skills"],"points":[{"start":7955,"end":7959,"text":"Sqoop"}]},{"label":["Tools"],"points":[{"start":7944,"end":7952,"text":"Cassandra"}]},{"label":["Skills"],"points":[{"start":7930,"end":7933,"text":"Hive"}]},{"label":["Skills"],"points":[{"start":7922,"end":7927,"text":"Python"}]},{"label":["Tools"],"points":[{"start":7896,"end":7902,"text":"Tableau"}]},{"label":["Skills"],"points":[{"start":7866,"end":7876,"text":"Informatica"}]},{"label":["Skills"],"points":[{"start":7856,"end":7863,"text":"Teradata"}]},{"label":["Skills"],"points":[{"start":7559,"end":7564,"text":"Hadoop"}]},{"label":["Skills"],"points":[{"start":7396,"end":7401,"text":"Hadoop"}]},{"label":["Skills"],"points":[{"start":7261,"end":7266,"text":"Hadoop"}]},{"label":["Skills"],"points":[{"start":7240,"end":7247,"text":"Teradata"}]},{"label":["Skills"],"points":[{"start":7018,"end":7025,"text":"Teradata"}]},{"label":["Skills"],"points":[{"start":6917,"end":6924,"text":"Teradata"}]},{"label":["Skills"],"points":[{"start":6867,"end":6872,"text":"Hadoop"}]},{"label":["Skills"],"points":[{"start":6771,"end":6776,"text":"Hadoop"}]},{"label":["Skills"],"points":[{"start":6759,"end":6766,"text":"Teradata"}]},{"label":["Skills"],"points":[{"start":6653,"end":6658,"text":"Python"}]},{"label":["Skills"],"points":[{"start":6643,"end":6647,"text":"Scala"}]},{"label":["Skills"],"points":[{"start":6633,"end":6637,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":6617,"end":6627,"text":"Informatica"}]},{"label":["Tools"],"points":[{"start":6581,"end":6589,"text":"Cassandra"}]},{"label":["Skills"],"points":[{"start":6568,"end":6575,"text":"Teradata"}]},{"label":["Skills"],"points":[{"start":6558,"end":6562,"text":"Sqoop"}]},{"label":["Skills"],"points":[{"start":6549,"end":6552,"text":"Hive"}]},{"label":["Skills"],"points":[{"start":6238,"end":6243,"text":"Python"}]},{"label":["Skills"],"points":[{"start":6214,"end":6222,"text":"Cassandra"}]},{"label":["Skills"],"points":[{"start":6194,"end":6198,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":6187,"end":6191,"text":"Kafka"}]},{"label":["Tools"],"points":[{"start":5913,"end":5922,"text":"Kubernetes"}]},{"label":["Skills"],"points":[{"start":5890,"end":5894,"text":"Kafka"}]},{"label":["Skills"],"points":[{"start":5876,"end":5880,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":4381,"end":4391,"text":"Informatica"}]},{"label":["Skills"],"points":[{"start":4333,"end":4340,"text":"Teradata"}]},{"label":["Skills"],"points":[{"start":4328,"end":4328,"text":"R"}]},{"label":["Skills"],"points":[{"start":4319,"end":4324,"text":"Python"}]},{"label":["Skills"],"points":[{"start":4312,"end":4314,"text":"SAS"}]},{"label":["Tools"],"points":[{"start":4278,"end":4283,"text":"Docker"}]},{"label":["Tools"],"points":[{"start":4264,"end":4273,"text":"Kubernetes"}]},{"label":["Skills"],"points":[{"start":4246,"end":4250,"text":"Kafka"}]},{"label":["Skills"],"points":[{"start":4237,"end":4241,"text":"Spark"}]},{"label":["Tools"],"points":[{"start":3381,"end":3386,"text":"Docker"}]},{"label":["Tools"],"points":[{"start":3366,"end":3375,"text":"Kubernetes"}]},{"label":["Skills"],"points":[{"start":3347,"end":3363,"text":"Decision Sciences"}]},{"label":["Skills"],"points":[{"start":3329,"end":3344,"text":"Machine Learning"}]},{"label":["Skills"],"points":[{"start":3311,"end":3326,"text":"Sensor Analytics"}]},{"label":["Skills"],"points":[{"start":3303,"end":3305,"text":"IoT"}]},{"label":["Skills"],"points":[{"start":3281,"end":3281,"text":"C"}]},{"label":["Skills"],"points":[{"start":3277,"end":3279,"text":"C++"}]},{"label":["Skills"],"points":[{"start":3272,"end":3275,"text":"Java"}]},{"label":["Skills"],"points":[{"start":3266,"end":3270,"text":"Scala"}]},{"label":["Skills"],"points":[{"start":3261,"end":3263,"text":"SAS"}]},{"label":["Skills"],"points":[{"start":3257,"end":3257,"text":"R"}]},{"label":["Skills"],"points":[{"start":3229,"end":3234,"text":"Python"}]},{"label":["Tools"],"points":[{"start":3223,"end":3226,"text":"Atom"}]},{"label":["Tools"],"points":[{"start":3214,"end":3221,"text":"Zeppelin"}]},{"label":["Tools"],"points":[{"start":3206,"end":3212,"text":"Jupyter"}]},{"label":["Tools"],"points":[{"start":3200,"end":3203,"text":"Jira"}]},{"label":["Tools"],"points":[{"start":3188,"end":3197,"text":"Kubernetes"}]},{"label":["Tools"],"points":[{"start":3178,"end":3183,"text":"Docker"}]},{"label":["Tools"],"points":[{"start":3151,"end":3157,"text":"Tableau"}]},{"label":["Skills"],"points":[{"start":3134,"end":3148,"text":"Shell scripting"}]},{"label":["Tools"],"points":[{"start":3125,"end":3131,"text":"MongoDB"}]},{"label":["Tools"],"points":[{"start":3118,"end":3122,"text":"HBase"}]},{"label":["Tools"],"points":[{"start":3107,"end":3115,"text":"Cassandra"}]},{"label":["Tools"],"points":[{"start":3081,"end":3086,"text":"Github"}]},{"label":["Tools"],"points":[{"start":3074,"end":3079,"text":"Gitlab"}]},{"label":["Skills"],"points":[{"start":3062,"end":3066,"text":"Sqoop"}]},{"label":["Skills"],"points":[{"start":3057,"end":3059,"text":"Pig"}]},{"label":["Skills"],"points":[{"start":3051,"end":3054,"text":"Hive"}]},{"label":["Skills"],"points":[{"start":3042,"end":3047,"text":"Hadoop"}]},{"label":["Skills"],"points":[{"start":3029,"end":3039,"text":"Informatica"}]},{"label":["Skills"],"points":[{"start":3019,"end":3026,"text":"Teradata"}]},{"label":["Skills"],"points":[{"start":3012,"end":3016,"text":"Kafka"}]},{"label":["Skills"],"points":[{"start":3003,"end":3009,"text":"PySpark"}]},{"label":["Skills"],"points":[{"start":2996,"end":3000,"text":"Spark"}]},{"label":["Degree"],"points":[{"start":2635,"end":2640,"text":"M.Tech"}]},{"label":["Skills"],"points":[{"start":2538,"end":2543,"text":"Python"}]},{"label":["Skills"],"points":[{"start":2535,"end":2535,"text":"C"}]},{"label":["Skills"],"points":[{"start":2530,"end":2532,"text":"C++"}]},{"label":["Skills"],"points":[{"start":2524,"end":2527,"text":"Java"}]},{"label":["Skills"],"points":[{"start":2425,"end":2430,"text":"Python"}]},{"label":["Tools"],"points":[{"start":2358,"end":2364,"text":"Tableau"}]},{"label":["Skills"],"points":[{"start":2258,"end":2268,"text":"Informatica"}]},{"label":["Skills"],"points":[{"start":2136,"end":2143,"text":"Teradata"}]},{"label":["Tools"],"points":[{"start":2078,"end":2086,"text":"Cassandra"}]},{"label":["Skills"],"points":[{"start":2078,"end":2078,"text":"C"}]},{"label":["Tools"],"points":[{"start":2071,"end":2075,"text":"HBase"}]},{"label":["Skills"],"points":[{"start":2040,"end":2044,"text":"Sqoop"}]},{"label":["Skills"],"points":[{"start":2034,"end":2037,"text":"Hive"}]},{"label":["Skills"],"points":[{"start":2029,"end":2031,"text":"Pig"}]},{"label":["Skills"],"points":[{"start":1989,"end":1994,"text":"Hadoop"}]},{"label":["Skills"],"points":[{"start":1937,"end":1939,"text":"IoT"}]},{"label":["Skills"],"points":[{"start":1864,"end":1880,"text":"Decision Sciences"}]},{"label":["Tools"],"points":[{"start":1496,"end":1501,"text":"Docker"}]},{"label":["Tools"],"points":[{"start":1485,"end":1494,"text":"Kubernetes"}]},{"label":["Skills"],"points":[{"start":1431,"end":1435,"text":"Kafka"}]},{"label":["Skills"],"points":[{"start":1420,"end":1424,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":1412,"end":1417,"text":"Hadoop"}]},{"label":["Skills"],"points":[{"start":1272,"end":1274,"text":"IoT"}]},{"label":["Skills"],"points":[{"start":479,"end":484,"text":"Hadoop"}]},{"label":["Skills"],"points":[{"start":464,"end":466,"text":"IoT"}]},{"label":["Tools"],"points":[{"start":419,"end":428,"text":"Kubernetes"}]},{"label":["Tools"],"points":[{"start":410,"end":415,"text":"Docker"}]},{"label":["Years_of_Experience"],"points":[{"start":318,"end":326,"text":"6.7 years"}]},{"label":["Email_Address"],"points":[{"start":125,"end":142,"text":"PK179288@gmail.com"}]},{"label":["Mobile_No"],"points":[{"start":97,"end":113,"text":"(+91) 94808 62869"}]},{"label":["Name"],"points":[{"start":0,"end":11,"text":"Pankaj Kumar"}]}],"extras":null,"metadata":{"first_done_at":1564124980000,"last_updated_at":1564124984000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "PARUL UPADHYAY\n+91-8005195220 | parulupadhyay82@gmail.com\nhttps://www.linkedin.com/in/parul-upadhyay-a34297b6\nSUMMARY\n\nAstute software professional working in banking domain with almost 2 years of experience in Java, Spring, Hibernate, Kafka and REST APIs.\nEXPERIENCE\n\n\tMagic Software Pvt Ltd.\tNoida, India\n\tSr. Associate Engineer\tApril 2017 – Present\n· Working on authoring tool under project Pearson for authors and writers to edit books.\n· Developing Rest APIs for modifying, deleting and updating data.\n·Recently started understanding basic of Dockers and Jenkins with Webflux.\t\nPuresoftware Pvt Ltd.\tNoida, India\n\tSoftware Engineer\tJuly 2017 – March 2019\n· Working on multi-tenant database wallet banking system i.e. Arttha banking system for Nepal micro-finance institutions and cooperative banks using Hibernate and Spring.\n· Involved in complete development of modules like bulk upload via CSV, user risk and fraud \n· Involved in developing REST AP along with bug fixing of various modules including transaction validation and payment methods.\nINTERNSHIP\n\n\tPie Infocomm Pvt. Ltd.\tLucknow, India\n\tSoftware Development Intern\tJune 2016 - July 2016\n· Worked on web service using REST APIs. for web development projects.\nPROJECT\n\nArttha Banking System:\n\tPuresoftware Pvt. Ltd.\tJuly 2017 - Present\n· Working on banking application for Nepal micro-finance institutions and cooperative banks for maintaining all banking operations.\n· Integrated Apache Kafka with the application for logging and analysis.\n· Performed end to end migration of the application from MySql to MSSql.\n· Developed REST APIs for the banking application.\n· Developed customer profile using JSP for the application to manage various users accordingly.\nFriends Book Application:\n\tDr.AKTU\tJune 2016 - March, 2017\n· Worked with the team in college to develop application for friend’s interaction within the college using Java, Javascript and JSP.\n· Completely developed the REST APIs for the application to enhance extra features for the college project\nPRIMARY SKILLS\n\nJava, Spring, Hibernate and Apache Kafka.\nTECHNICAL SKILLS\n\n\tProgramming Languages:\n\tJava , C/C++\n\n\tWeb Technologies:\n\tSpring, Hibernate, Apache Kafka, REST APIs\n\n\tTechnical Tools:\n\tEclipse, Jira, Glowroot\n\n\tDatabase:\n\tMS Sql, MySQL\n\n\tServer:\n\tApache Tomcat, Jetty\n\n\tOperating Systems:\n\tWindows, Linux\n\n\n\nEDUCATION\n\nDr. Abdul Kalam Technical University \t\t\t\t\t\t     Aug. 2013 - Jun. 2017 \nBachelor of Technology in Information & Technology                                   \t\t\t             74.06 %\n      Kendriya Vidyalaya No.2, AFS Jodhpur\tApr. 2012 - Mar. 2013\n\tSenior Secondary, Mathematics\t73 %\n       Kendriya Vidyalaya No.2, AFS Jodhpur\tApr. 2010 - Mar. 2011\n\tHigh School, Science\t9.4 CGPA\nACHIEVEMENTS\n\n· Served as treasurer for official technical community of CS/IT department.\n· Served as assistant coordinator in technical team of college.\n· Attended workshop on internet security conducted by Cyber Crime Cell, Lucknow.\n· National level Gold medalist in handball.","annotation":[{"label":["Degree"],"points":[{"start":2428,"end":2477,"text":"Bachelor of Technology in Information & Technology"}]},{"label":["Operating_Systems"],"points":[{"start":2337,"end":2341,"text":"Linux"}]},{"label":["Operating_Systems"],"points":[{"start":2328,"end":2334,"text":"Windows"}]},{"label":["Skills"],"points":[{"start":2300,"end":2304,"text":"Jetty"}]},{"label":["Skills"],"points":[{"start":2285,"end":2297,"text":"Apache Tomcat"}]},{"label":["Skills"],"points":[{"start":2268,"end":2272,"text":"MySQL"}]},{"label":["Skills"],"points":[{"start":2260,"end":2265,"text":"MS Sql"}]},{"label":["Tools"],"points":[{"start":2238,"end":2245,"text":"Glowroot"}]},{"label":["Tools"],"points":[{"start":2232,"end":2235,"text":"Jira"}]},{"label":["Tools"],"points":[{"start":2223,"end":2229,"text":"Eclipse"}]},{"label":["Skills"],"points":[{"start":2193,"end":2201,"text":"REST APIs"}]},{"label":["Skills"],"points":[{"start":2179,"end":2190,"text":"Apache Kafka"}]},{"label":["Skills"],"points":[{"start":2168,"end":2176,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":2160,"end":2165,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":2135,"end":2137,"text":"C++"}]},{"label":["Skills"],"points":[{"start":2132,"end":2133,"text":" C"}]},{"label":["Skills"],"points":[{"start":2126,"end":2129,"text":"Java"}]},{"label":["Skills"],"points":[{"start":2069,"end":2080,"text":"Apache Kafka"}]},{"label":["Skills"],"points":[{"start":2055,"end":2063,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":2047,"end":2052,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":2041,"end":2044,"text":"Java"}]},{"label":["Skills"],"points":[{"start":1945,"end":1953,"text":"REST APIs"}]},{"label":["Skills"],"points":[{"start":1898,"end":1901,"text":"Java"}]},{"label":["Skills"],"points":[{"start":1892,"end":1895,"text":"Java"}]},{"label":["Skills"],"points":[{"start":1591,"end":1599,"text":"REST APIs"}]},{"label":["Skills"],"points":[{"start":1446,"end":1457,"text":"Apache Kafka"}]},{"label":["Skills"],"points":[{"start":1184,"end":1192,"text":"REST APIs"}]},{"label":["Skills"],"points":[{"start":823,"end":828,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":809,"end":817,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":246,"end":254,"text":"REST APIs"}]},{"label":["Skills"],"points":[{"start":225,"end":233,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":217,"end":222,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":211,"end":214,"text":"Java"}]},{"label":["Years_of_Experience"],"points":[{"start":186,"end":192,"text":"2 years"}]},{"label":["Email_Address"],"points":[{"start":32,"end":56,"text":"parulupadhyay82@gmail.com"}]},{"label":["Mobile_No"],"points":[{"start":15,"end":28,"text":"+91-8005195220"}]},{"label":["Name"],"points":[{"start":0,"end":13,"text":"PARUL UPADHYAY"}]}],"extras":null,"metadata":{"first_done_at":1564208098000,"last_updated_at":1564208098000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "PAVAN G BELGAUMKAR\nMobile: +919916445169                                                  Email: pavan.belgaumkar@gmail.com\n\nLooking for challenging assignment in Software Development/support environment with a leading organization where I can apply and build my technical & analytical skills\n\nSummary\n\nFocused, results-oriented and passionate Software engineer with 5+ years of professional experience in Informatica Power center, Informatica cloud, MSSQL, Pl/Sql, IBM DB2, UNIX\n\n· Sound knowledge of informatica, Informatica cloud , SQL, Pl/Sql, Accent(VMS), UNIX\n· Coordinate with Onsite team and Business user to ensure smooth delivery of services and applications.\n· Worked on Project deliverables, Applications development and Production Support, L3 Problem Management and Enhancement Projects.\n· Highly motivated, energetic, hardworking and self-driven professional with good analytical, organizational, creative and communication skills.\n\nEducation\n\n· B.E. (Electronics & Communication Engineering in DSI College) from Visvesvaraya Technological University, in 2012. Secured 62.2% with First Class.\n· Diploma(Electronics & Communication Engineering) from KHK Institute Dharwadin 2009.Secured 71.2% with First Class\n· Pavan English Medium High School Dharwad in 2006. Secured 90.08 in SSLC\n\n\nTechnical Skills\n\nDomain\t\t: Microsoft, Database\nTools/Application\t: CA Workload Automation, Informatica cloud, Informatica Power center. Oracle     pl/Sql, Microsoft SQL, Control-M, IBM DB2, Unix\n\t\t\t\n\nWork Experience\n\n· June 2017 to till date with Infosys Ltd as Technology Analyst\nProject 1:\n· Title\t\t\t: Medtronic.\n· Client\t\t\t: Medtronic\n· Tools/Applications\t: Informatica, Informatica cloud, MS SQL, Oracle Pl/Sql, IBM DB2, WinSCP\n· Role\t\t\t: Developer\n\n\nDetails:\nMedtronic plc is a medical device company. Its headquarters are in Dublin, Ireland. Its operational headquarters are in Fridley, Minnesota. Medtronic is among the world's largest medical equipment development companies\n\nResponsibilities \n\n\n· Involved in resolving all the issue related to Medtronic\n· Created various Informatica Mappings, Sessions and Workflows.\n· Created various Informatica cloud Mappings, Configuration Tasks, Task flows, Schedulers, Connections, Data synchronization task.\n· Involved in fixing the issue with Mappings.\n· Responsible for Design and implementation of different Jobs which run in Informatica cloud.\n· Performed query level tuning by creating indexes & modifying SQL queries. \n· Involved in Incident Management, Change Management and Problem Management.                           \n· Involved in effort estimates for various projects/enhancements with accuracy. \n· Conducted Team meetings to maintain and enhance the ways of working. \n· Created SQL objects namely Tables, Views, Indexes, Procedures etc.\n· Performed accurate Unit Testing and System Testing.\n· Documented all the processes complying Industry standards.\n· Involved in compatibility testing and estimated the scope of changes with accuracy.\n· Performance tuning in Informatica.\n\n\n· Dec 2013 to June 2017 with HCL Technologies Ltd as Software Engineer\nProject 1:\n· Title\t\t:VS10-Megadata.\n· Client\t\t: Allianz\n· Language\t: Informatica PC, CA Workload Automation, SQL, Pl/Sql, \n· Role\t\t: Software Engineer\n\n\nDetails:\nAllianz is a European financial services company. Its core businesses are insurance and asset management. As of 2014, it is the world's largest insurance company. \nHCL Tech Manages ALLIANZ US with Day to Day activities.\n\nResponsibilities \n\n· Responsible for Daily Cycle which runs in CA Workload Automation Tool, \n· Daily cycle also involves activities in Informatica, PL/Sql, Sql, .Net and VB 6. Daily cycle also has lot of criticality as the data we provide will directly go to US trade market NYSE.\n· Involved in resolving all the issue related to Megadata.\n· Involved in Development and Enhancement of Megadata\n· Responsible for Design and implementation of different Jobs which run in CA Workload.\n· Performed query level tuning by creating indexes & modifying SQL queries. \n· Involved in Incident Management, Change Management and Problem Management.                           \n· Involved in effort estimates for various projects/enhancements with accuracy. \n· Interacted with clients to understand their requirements.\n\nProject 2:\n· Title\t\t: IRT (Intermediate Reconciliation Tool)\n· Client\t\t: Deutsche Bank\n· Language\t\t: Informatica PC, Pl/Sql, Control-Toad, Unix\n· Role\t\t: Software Engineer\n\n\nDeutsche Bank AG is a German global banking and financial services company with its headquarters in the Deutsche Bank Twin Towers in Frankfurt and has a large presence in Europe, the Americas, Asia-Pacific and the emerging markets. In 2009, Deutsche Bank was the largest foreign exchange dealer in the world with a market share of 21 percent. The company was a component of the STOXX Europe 50 stock market index until being delisted on August 8, 2016\n\n\nResponsibilities \n\n· Performed query level tuning by creating indexes & modifying SQL queries. \n· Building mappings using Flat Files and CSV files.\n· Matching incoming reports to generate Reconciliation report.\n· Created SQL objects namely Tables, Views, Indexes, Procedures etc.\n· Performed accurate Unit Testing and System Testing.\n· Documented all the processes complying Industry standards.\n· Created various Informatica Mappings, Sessions and Workflows.\n· Involved in fixing the issue with Mappings.\n· Creation of SDLC life cycle.\n· Created various Change requests.\n· Interacted with L2/L3 teams and understood the requirements for Development.\n\n\nStrengths\n\n· Quick learner and good listener.\n· Self-driven and Self-motivated.\n· Responsible and Hardworking.\n\nPersonal details\n\nPermanent Address\t:  F.No.7, Suvidha Apartments, Kabbur Road, Malmaddi, Dharwad-580007.\nDate of Birth\t\t:  27 Feb 1990\nMarital Status\t\t:  Single\nLanguages Known\t:  English(Read/Write), Kannada(Read/Write), Hindi(Read/Write),    Telugu(Understand), Tamil(Understand).","annotation":[{"label":["Skills"],"points":[{"start":5148,"end":5151,"text":" SQL"}]},{"label":["Skills"],"points":[{"start":5009,"end":5012,"text":" SQL"}]},{"label":["Skills"],"points":[{"start":4439,"end":4442,"text":"Unix"}]},{"label":["Skills"],"points":[{"start":4417,"end":4422,"text":"Pl/Sql"}]},{"label":["Skills"],"points":[{"start":4039,"end":4042,"text":" SQL"}]},{"label":["Skills"],"points":[{"start":3226,"end":3231,"text":"Pl/Sql"}]},{"label":["Skills"],"points":[{"start":3220,"end":3223,"text":" SQL"}]},{"label":["Skills"],"points":[{"start":2741,"end":2744,"text":" SQL"}]},{"label":["Skills"],"points":[{"start":2460,"end":2463,"text":" SQL"}]},{"label":["Skills"],"points":[{"start":2379,"end":2395,"text":"Informatica cloud"}]},{"label":["Skills"],"points":[{"start":2145,"end":2161,"text":"Informatica cloud"}]},{"label":["Skills"],"points":[{"start":1716,"end":1722,"text":"IBM DB2"}]},{"label":["Skills"],"points":[{"start":1708,"end":1713,"text":"Pl/Sql"}]},{"label":["Skills"],"points":[{"start":1701,"end":1706,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":1693,"end":1698,"text":"MS SQL"}]},{"label":["Skills"],"points":[{"start":1674,"end":1690,"text":"Informatica cloud"}]},{"label":["Skills"],"points":[{"start":1490,"end":1493,"text":"Unix"}]},{"label":["Skills"],"points":[{"start":1481,"end":1487,"text":"IBM DB2"}]},{"label":["Skills"],"points":[{"start":1470,"end":1478,"text":"Control-M"}]},{"label":["Skills"],"points":[{"start":1455,"end":1467,"text":"Microsoft SQL"}]},{"label":["Skills"],"points":[{"start":1447,"end":1452,"text":"pl/Sql"}]},{"label":["Skills"],"points":[{"start":1436,"end":1441,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":1391,"end":1407,"text":"Informatica cloud"}]},{"label":["Degree"],"points":[{"start":960,"end":963,"text":"B.E."}]},{"label":["Skills"],"points":[{"start":561,"end":564,"text":"UNIX"}]},{"label":["Skills"],"points":[{"start":548,"end":558,"text":"Accent(VMS)"}]},{"label":["Skills"],"points":[{"start":540,"end":545,"text":"Pl/Sql"}]},{"label":["Skills"],"points":[{"start":534,"end":537,"text":" SQL"}]},{"label":["Skills"],"points":[{"start":515,"end":531,"text":"Informatica cloud"}]},{"label":["Skills"],"points":[{"start":502,"end":512,"text":"informatica"}]},{"label":["Skills"],"points":[{"start":475,"end":478,"text":"UNIX"}]},{"label":["Skills"],"points":[{"start":466,"end":472,"text":"IBM DB2"}]},{"label":["Skills"],"points":[{"start":458,"end":463,"text":"Pl/Sql"}]},{"label":["Skills"],"points":[{"start":432,"end":448,"text":"Informatica cloud"}]},{"label":["Years_of_Experience"],"points":[{"start":367,"end":374,"text":"5+ years"}]},{"label":["Email_Address"],"points":[{"start":97,"end":122,"text":"pavan.belgaumkar@gmail.com"}]},{"label":["Mobile_No"],"points":[{"start":27,"end":39,"text":"+919916445169"}]},{"label":["Name"],"points":[{"start":0,"end":17,"text":"PAVAN G BELGAUMKAR"}]}],"extras":null,"metadata":{"first_done_at":1564048530000,"last_updated_at":1564048530000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "Prabhanshu Chaubey\nSenior Developer\n\nProfessional Summary:\n\n· Software Engineer with 4 years of experience in designing and developing robust code for high-volume businesses using CoreJava (OOPS ,Collection,Exception) and different frameworks like Spring MVC, Hibernate/JPA, Mybatis with REST based Web Services.\n· Hands on experience in developing MVC (Model View Controller) architecture based Web applications using Spring MVC, Spring Boot.\n· Experience in developing REST (JAX-RS) based Resource and Client RESTful Services by using Jersey\n· Hands on experience in using Application/Web servers like Tomcat, IBM WAS,Weblogic\n· Proficient Experience on working with various tools like Maven, JUnit, Log4j, Git&GitHub, CVS\n· Worked efficiently under Scrum Model of Agile Methodology for Project Development.\n· Hands on experience in application development using Spring boot and JPA frameworks.\n· Proficient experience to integrate Spring with RESTful Services based application.\n· Good amount of application development exposure of working on third party libraries;\nApache POI, PDFBox.\n· Good amount of exposure and experience to databases like Oracle & MySQL,Postgres.\n· Handled to work on Payment and Insurance domains.\n· Having excellent analytical, communication and monitoring skills and flexible to work across different APIs and technologies.\n· Having experience working on Authentication mechanisms like BASIC, OAUTH.\n\n\n\nProfessional Experience:\n\n\n· Worked as a Senior Software Engineer at ITC Infotech for MASTERCARD in the Payment domain.\n· Worked as a Product Developer in DXC Technologies Private Limited Indore from May- 2017\nto July-2018\n· Worked as a Software Engineer in TATA Consultancy Services Pvt Limited From May- 2015\nto May-2017.\nAcademic Qualification:\n\n· B.Tech from RGPV with 7.95 CGPA.\n\n\tTechnical Skills:\nProgramming Languages\n\t\n:\n\t\nJava,J2EE\n\n\tMarkup Languages\n\t:\n\tHTML, XML\n\n\n\n\tDistributed (Web Services) Technologies\n\t:\n\tREST,JAX-RS (Jersey), JAX-WS.\n\n\tApplication Frameworks\n\t:\n\tSpring MVC, Spring Boot\n\n\tORM Framework\n\t:\n\tMybatis, Hibernate (JPA)\n\n\tRESTful API Documentation Framework\n\t:\n\tSwagger UI\n\n\tOther Frameworks\n\t:\n\tPOI, PDFBox\n\n\tProtocols\n\t:\n\tHTTP\n\n\tServers\n\t:\n\tTomcat, IBM WAS\n\n\tDatabases\n\t:\n\tOracle, MySQL\n\n\tTools\n\t:\n\tMaven, JUnit, log4j, CVS, Git, GitHub, JENKINS, JIRA,SONAR,Rally,mRemoteNG\n\n\tIDEs\n\t:\n\tEclipse,IntelliJ, SQLDeveloper.\n\n\tOperating Systems\n\t:\n\tWindows and Linux.\n\n\n\n\n\n\n\nProject #2:\n\tTitle\n\t:\n\tNew Business Accelerator\n\n\tClient\n\t:\n\tAXA, USA\n\n\tDuration\n\t:\n\tMay-2017 to till date\n\n\tTeam Size\n\t:\n\t9\n\n\tRole\n\t:\n\tJava Developer\n\n\tDomain\n\t:\n\tInsurance.\n\n\tEnvironment\n\t:\n\tSpring Boot, Spring MVC, JAX-RS-2.0, JDK-1.7\n\n\tTools\n\t:\n\tMaven, JUnit, Log4j, Git\n\n\tProcess\n\t:\n\tAgile\n\n\nDescription :\nNew Business Accelerator (NBA) is a web application which is used by AXA Insurance Group agents. Agent gets all the information related to customer like name, address, prior insurance history, Underwriting information and accordingly certain Requirements and Impairments are generated. Once these are fulfilled by the customer a suitable Rate class is assigned to the Insurance being provided to the customer based on the eligibility.\n\nResponsibilities :\n· Involved in understanding and analyzing the requirements, and development of various modules in different releases\n· Involved in the Analysis, Design, Coding, modification and implementation.\n· Implementing RESTfulWeb Services using JAX-RS API with Jersey library.\n\n· Tested RESTful Web Services using Postman.\n· Developed controller classes using Spring MVC.\n· Followed Agile process using Scrum for managing the development of the application.\n· Responsible for providing knowledge sessions to the peer members in the team\n· Responsible for the code review and monitoring the code quality.\n\n\nProject #1:\n\tTitle\n\t:\n\tGE Forms\n\n\tClient\n\t:\n\tGeneral Electric,USA\n\n\tDuration\n\t:\n\tMay-2015 to May-2017\n\n\tTeam Size\n\t:\n\t6\n\n\tRole\n\t:\n\tJava Developer\n\n\tDomain\n\t:\n\tFinancial\n\n\tEnvironment\n\t:\n\tSpring, JAX-RS, Mybatis, JDK 1.7\n\n\tTools used\n\t:\n\tMaven, JUnit, Log4j, GIT.\n\n\n\nProcess\t: Agile\nDescription :\nGE Forms is a web application which is used by the General Electric employees. This application was being used to record the payroll related information of the GE Employees which includes creating surveys for the GE Employees, creating forms for the Rewards and the Recognition internally in GE, exposing different API’s to be consumed by the GE Employees to get the user information, designation, manager’s information.\n\nResponsibilities :\n· Involved in Client interaction on regular basis to understand the Business.\n· Involved in the Analysis, Design, Coding, modification and implementation.\n· Implementing REST based web services using JAX-RS API with Jersey library.\n· Responsible for writing test cases for Unit Testing using JUnit.\n· Followed Agile process using Scrum for managing the development of the application.\n· Responsible for monitoring the code quality of the project.","annotation":null,"extras":null,"metadata":{"first_done_at":1564136774000,"last_updated_at":1564137207000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "Skills\n\tYears of Experience\n\tProject Name\n\tDuration\n\n\tData scientist/ Machine learning\n\t3 yrs \n\tQuix solution\nX-byte technolabs\n\t2016 - 2019\n\n\tText Mining/ Data\n\t1 yrs \n\tQuix solution\nX-byte technolabs\n\t2016 – 2019\n\n\tWeb scraping \n\t1 yrs \n\tX-byte technolabs\n\t2017 - 2018\n\n\tPython / SQL / NLP\n\t1 Yrs\n\tQuix solution\n\t2018 - 2019\n\n\n\nPremal Sheth                                                       A-1109, Yash Pinnacle,\nData Analyst                                                 Bhattha, Ahmedabad, Gujarat\n\nEmail-ID: shethprem@gmail.com                                                 Ph: +91-999-808-5600\n\nObjective\nSeeking a challenging assignment in field of data science where I can also utilize my\nIT skills acquired over a period\nSkills and Expertise\n  Quick learner\n  Learning Attitude\n  R\n  Big Data\n  Python\n  Data visualization\n  Good team player\nTechnical Skills\n•   Tools\no  Rstudio\no Jupyter notebook\no  CFDstudio\no  Anaconda\n•   Programming Languages\no  Python\no  R\no  C\no  Beginner MySQL\no  Django REST Framework\n\n•   Working Experience in UNIX/LINUX operating system\n\nProfessional Experience\n\nData Analyst in Quix solution pvt ltd\nBaroda,Gujart April – 2018- January-2019\n\n  Using Fuzzy matching Apply Machine Learning algorithm for Talent and\nEmployer scoring and create API using Django Framework.\n  Web crawling using Python\n  CV parsing using NLP in python\n  Using SQL Query in python and Create API in Django REST API\n  Develop recommender system for giving similar Talent title using NLP\n  Feature engineering\nData Analyst Consultant X-byte technolabs\nAhmedabad,Gujarat\n November-2017 – Feb 2018\n  web scrapping\n  Coordinate with sales and technical teams.\n  Data mining, Data visualization and data exploratory\n•   Analyze data using predictive models like linear regression, decision\n•   trees etc. and interpretation results.\n  Preparing case studies and technical documents\n\nOperation Executive\nACM sales & Marketing - Ahmedabad, Gujarat\nJune-2017 - November-2017\n  Reliable supply of products\n  Coordinate distribution of goods\n  Manage relation with clients\n  Make records of purchase and sale\n\nData Analyst\nACRi infotech pvt Ltd - Bangalore, Karnataka\nOctober-2016 - April-2017\n  Data mining,\n  Data visualization\n  data exploratory\n  Hypothesis testing using t-test, Z-test, ANNOVA etc.\n\tAnalyze data using predictive models like linear regression, decision trees etc. and interpretation results.\n  Preparing case studies and technical documents\n\nSenior Research fellow\nCFDvri pvt Ltd - Bangalore, Karnataka\nDecember-2013 - September-2016\n\nWorking on a project \"Study of Water Level Rise and Inundation in\nTsunami Condition generated by Earthquake\"\n  NOAA Benchmark problems for testing TIDAL software\n  Identify the Hypothetical Scenarios and simulate TIDAL software.\n  Preparing technical report. .\nAnother Responsibility\n  Data visualization and hypothesis testing\n  Apply predictive models on data and analyzed result\n  Preparing Report for client\n\nCFD engineer\nP M Dimensions Pvt Ltd - Gandhinagar, Gujarat\nApril-2012 - November-2013\n\tResponsible for gathering the requirements and compilation of the same for the analytics team.\n  Track progress and follow up with the analytics team.\n\tTechnical documentation of the results and presentations for the clients. Preparing CFD case studies for Clients.\n\n\nProduction Engineer\nB T corporation Pvt Ltd - Ahmedabad, Gujarat\nAugust-2009 - February-2011\n\nProjects:\nHere I attached my all project links:\n1)   https://drive.google.com/open?id=1edQ341KSSoqEmO8OcjR_QY-nI60eozy6\n2)   https://drive.google.com/open?id=1UV_Ya4jpJ90GHOvQn89eOQNgkFV0rq8e\n3)   https://drive.google.com/open?id=1aNLz6DxU8nOLngN343_n0M028jDLgpzo\n4)   https://drive.google.com/open?id=11lBlm3tQ6C2iad-xP2Eut1xSRpqaSRRN\n5)   https://drive.google.com/open?id=1yESlPhANBf0AQFwYmR7t_O5fb3XOOFTd\n\nAcademic Credentials\n  Bachelor of engineering in Aeronautical from Gujarat University in 2009\n  Standard 12th Science from N. M high school , Ahmedabad in 2005\n\n\nTechnical Certifications\n  CFD professional Course from IERA in May 2011 to March 2012\n  Certified “Introduction of Data Analytics” MOOC course from NPTEL – IITM\nhttps://drive.google.com/open?id=0B-Xia8EoWkGcMGVvdy1JZFFidWs\n\nExtra-curricular Activities\n  Participated in Robotics during college\n  Was a SCOUT member in school\n  Volunteer in Bangalore Trekking Club\n\nPersonal Details\n  Date of Birth: September 23, 1987\n  Languages: English, Hindi, Gujarati\n  Marital Status: Unmarried","annotation":[{"label":["Certifications"],"points":[{"start":4147,"end":4176,"text":"Introduction of Data Analytics"}]},{"label":["Certifications"],"points":[{"start":4073,"end":4105,"text":"CFD professional Course from IERA"}]},{"label":["Degree"],"points":[{"start":3904,"end":3942,"text":"Bachelor of engineering in Aeronautical"}]},{"label":["Skills"],"points":[{"start":2890,"end":2907,"text":"Data visualization"}]},{"label":["Skills"],"points":[{"start":2250,"end":2267,"text":"Data visualization"}]},{"label":["Skills"],"points":[{"start":2234,"end":2237,"text":"Data"}]},{"label":["Skills"],"points":[{"start":2147,"end":2150,"text":"Data"}]},{"label":["Skills"],"points":[{"start":1713,"end":1730,"text":"Data visualization"}]},{"label":["Skills"],"points":[{"start":1700,"end":1703,"text":"Data"}]},{"label":["Skills"],"points":[{"start":1548,"end":1551,"text":"Data"}]},{"label":["Skills"],"points":[{"start":1521,"end":1523,"text":"NLP"}]},{"label":["Skills"],"points":[{"start":1399,"end":1401,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":1376,"end":1378,"text":"NLP"}]},{"label":["Skills"],"points":[{"start":1349,"end":1354,"text":"Python"}]},{"label":["Skills"],"points":[{"start":1119,"end":1122,"text":"Data"}]},{"label":["Operating_Systems"],"points":[{"start":1070,"end":1074,"text":"LINUX"}]},{"label":["Operating_Systems"],"points":[{"start":1065,"end":1068,"text":"UNIX"}]},{"label":["Skills"],"points":[{"start":1007,"end":1011,"text":"MySQL"}]},{"label":["Skills"],"points":[{"start":993,"end":993,"text":"C"}]},{"label":["Skills"],"points":[{"start":988,"end":988,"text":"R"}]},{"label":["Skills"],"points":[{"start":978,"end":983,"text":"Python"}]},{"label":["Skills"],"points":[{"start":828,"end":845,"text":"Data visualization"}]},{"label":["Skills"],"points":[{"start":818,"end":823,"text":"Python"}]},{"label":["Skills"],"points":[{"start":806,"end":813,"text":"Big Data"}]},{"label":["Skills"],"points":[{"start":801,"end":801,"text":"R"}]},{"label":["Mobile_No"],"points":[{"start":592,"end":607,"text":"+91-999-808-5600"}]},{"label":["Email_Address"],"points":[{"start":520,"end":539,"text":"shethprem@gmail.com "}]},{"label":["Skills"],"points":[{"start":420,"end":423,"text":"Data"}]},{"label":["Name"],"points":[{"start":330,"end":342,"text":"Premal Sheth "}]},{"label":["Skills"],"points":[{"start":288,"end":290,"text":"NLP"}]},{"label":["Skills"],"points":[{"start":282,"end":284,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":273,"end":278,"text":"Python"}]},{"label":["Skills"],"points":[{"start":217,"end":228,"text":"Web scraping"}]},{"label":["Skills"],"points":[{"start":156,"end":159,"text":"Data"}]},{"label":["Skills"],"points":[{"start":143,"end":153,"text":"Text Mining"}]},{"label":["Skills"],"points":[{"start":70,"end":85,"text":"Machine learning"}]},{"label":["Skills"],"points":[{"start":54,"end":67,"text":"Data scientist"}]}],"extras":null,"metadata":{"first_done_at":1564206435000,"last_updated_at":1564206435000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "PUSHPENDU DAS\nPH04, Keerthana King Bliss, \n\n7th cross, Kundanhalli Gate, \n\nBangalore, Karnataka - 560037\nContact No: +91 9916412192\nE-mail Id: pushpendu.das1112@gmail.com\nPROFESSIONAL SUMMARY: \n· Junior data scientist with 3+ years of experience in live time projects.\n· 2.3 years’ experience in Machine learning, Natural language processing, Data modeling, Statistical analysis and prediction using Machine learning, Quantitative analysis \n\n· 1 years of experience on cloud and implementations using Azure Infrastructure service, curious to explore and learn more in cloud.\n· Excellent organizational and analytical skills and Strong communication, presentation, business and technical workflow representation.\n· Strong background in Python, Machine Learning frameworks, Natural Language Processing framework, SQL.\n· Worked with the team in buliding a ML model, preprocessing the data, feature selection, system implementation, and evaluation the algorithm.\n\n· Relevant knowledge of machine learning and data mining concepts with an understanding of supervised and unsupervised algorithms (such as Regression, classification, Decision tree, PCA, Clustering, NLTK,  Association Rule Mining Techniques.)\n· Work with global clients, locally and internationally and received appreciation letter from our client for outstanding performance and excellent work.\n· Experience in Microsoft Azure IAAS.\nTechnical Skills:\nData Science Skill\n: Machine Learning, Natural Language Processing, Data analysis, Statistics analysis.\n\nCloud Technology \n: Azure IaaS, Azure ML Studio\n\nScripting Language\n: Python\n\nData Science framework\n: Numpy, Pandas, Matplotlib, Scikit-learn, Seaborn, NLTK\n\nDatabase\n\n: MySQL,\n\nIDE\n\n\n: Anaconda, Azure Portal\n\nData Visualization tool\n: Tableau\nCourse and Certification:\n· Learning Python for Data Analysis and Visualization\n\n· Machine Learning A-Z™: Hands-On Python & R In Data Science\nWork Experience:\nTata Consultancy Service\nDuration: Nov, 16 to till date\n\nProfile: Data science Engineer\nClient: Office Depot\nProject: Office Dep Opt ADM  \nProcess overview:  Optumera is a Digital Merchandising Solution that harnesses the power of Big Data analytics, and ML prescriptive algorithms to respond to evolving consumer and market trends. Leverages over 500 data sources with advanced algorithms to localize and right-size store space categories and simulate store space changes to de-risk expensive investments. Analyses shopper behavior, missions, trips, decision hierarchies, demand transfer, preferences. Leverages computer vision and artificial intelligence to align with customer expectations and enhance shelf performance and in store shelf execution. Helps fashion retailers to ideate, collaborate and negotiate with vendors, merchants, designers, fashionistas, and shoppers.\nRoles and Responsibilities:\n· Fulfilled all data science duties for a high-end capital management firm.\n\n· Design and modeling for statistical analysis and prediction on huge unstructured and structured data.\n\n· Built fuzzy matching algorithm using k-nearest neighbors to identify non-exact matching duplicates.\n\n· Applying model selection on prepared data based on highest accuracy.\n\n· Designed and developed real time recommendation engine to rank sales leads for upsell opportunities\n\n· Used Python and Pig to scrape, hive to clean, and analyze large datasets\n\n· Created machine learning models with Python and scikit-learn to predict marketing strategy, buildings with 98% accuracy\n\n· Constructed a review model to accomplish the expectation and trend of customer\n\n· Model consist of multilayer neural network to predict the sales of organization based on product and location.\nTools and technology: ML, NLP, Python, Numpy, Pandas, Sci-Kit Learn, NLTK, Seaborn, Statistical Analysis\nTata Consultancy Service\nDuration: Dec, 15 to Nov, 16\n\nProfile: Cloud Engineer\nClient: Allianz Life Insurance Company\nProject: Data Solution and preparation\nProcess overview: Allianz Life leading provider of retirement solutions, including fixed and variable annuities and life insurance. These products are offered through a network of more than 100,000 agents nationwide.\nRoles and Responsibilities:\n· Debugging real time issue in production and providing solution to meet the requirement of nightly cycle.\n\n· Analyze and research the data to come out with permanent solution as per business requirement.\n\n· Build algorithms to automate and generate the daily report and experienced on change request (CR).\nTools and technology: Azure IaaS\nHands on Experience:\n\nProject: Determining Quality of hotels and market trend:\nAn organization was planning to increase their business by investing money in OYO rooms in Hyderabad, before investing the huge money, they wanted to get more information about OYO rooms and their hotels.\n\nAsked to get the review of all the hotels also to predict the review of new hotels, which will be built in future. Performed data analysis technique to get information about hotels where it have good reviews and bad reviews.\n\nHow OYO rooms can make profit in future.\nRoles and Responsibilities: \n\n· Constructed a model which can read and clean the text to perform the ML algorithms\n\n· Model showed excellent performance to extract information from given context\n\n· Two models were built with python, numpy, panda, NLTK, Sci-kit learn\n\n· Sparse vector and count vectorization technique added to build the model\n\n· Algorithm was implemented using Python in Machine learning platform\n\n· Dropout technique was used to prevent the model from overfitting.\nTools and technology: ML, NLP, Python, Sci-Kit Learn\n\nEducation: \n2015\n\nBachelor of Technology: Electornics and Communication Engineering \n\nHeritage Institiute iof technology, Maulana Abul Kalam Azad University of Technology, West Bengal\n\n2011\n\nHigher Secondary: Physics-Chemistry-Math\n\nMidnapore Town School, West Bengal Council of Higher Secondary Education, West Bengal\n\n2009\n\nSecondary\n\nBasantia High School, West Bengal Board of Secondary Education, West Bengal, 2003-2009\n\nExtra-curricular:\n· Member of University Sports Council\n\n· Won medals in athletics games at college level competition\n\n· Played Cricket, Football at college level\nPersonal Details:\n· Date of Birth: 4th Aug, 1992\n\n· Gender & Status: Male & Single\n\n· Languages Known: English, Hindi, Bengali\n\n· Hobbies: Drawing and swimming \n\nPlace:\n\n\n\n\n\n\n\n\n\nPushpendu Das\n\nDate:","annotation":[{"label":["Skills"],"points":[{"start":5594,"end":5599,"text":"Python"}]},{"label":["Skills"],"points":[{"start":5458,"end":5463,"text":"Python"}]},{"label":["Skills"],"points":[{"start":5327,"end":5330,"text":"NLTK"}]},{"label":["Skills"],"points":[{"start":4517,"end":4526,"text":"Azure IaaS"}]},{"label":["Skills"],"points":[{"start":3756,"end":3762,"text":"Seaborn"}]},{"label":["Skills"],"points":[{"start":3750,"end":3753,"text":"NLTK"}]},{"label":["Skills"],"points":[{"start":3727,"end":3732,"text":"Pandas"}]},{"label":["Skills"],"points":[{"start":3720,"end":3724,"text":"Numpy"}]},{"label":["Skills"],"points":[{"start":3712,"end":3717,"text":"Python"}]},{"label":["Skills"],"points":[{"start":3402,"end":3407,"text":"Python"}]},{"label":["Skills"],"points":[{"start":3294,"end":3299,"text":"Python"}]},{"label":["Certifications"],"points":[{"start":1868,"end":1902,"text":"Hands-On Python & R In Data Science"}]},{"label":["Certifications"],"points":[{"start":1845,"end":1865,"text":"Machine Learning A-Z™"}]},{"label":["Certifications"],"points":[{"start":1790,"end":1841,"text":"Learning Python for Data Analysis and Visualization\n"}]},{"label":["Tools"],"points":[{"start":1754,"end":1760,"text":"Tableau"}]},{"label":["Skills"],"points":[{"start":1688,"end":1692,"text":"MySQL"}]},{"label":["Skills"],"points":[{"start":1670,"end":1673,"text":"NLTK"}]},{"label":["Skills"],"points":[{"start":1661,"end":1667,"text":"Seaborn"}]},{"label":["Skills"],"points":[{"start":1647,"end":1658,"text":"Scikit-learn"}]},{"label":["Skills"],"points":[{"start":1635,"end":1644,"text":"Matplotlib"}]},{"label":["Skills"],"points":[{"start":1627,"end":1632,"text":"Pandas"}]},{"label":["Skills"],"points":[{"start":1620,"end":1624,"text":"Numpy"}]},{"label":["Skills"],"points":[{"start":1595,"end":1606,"text":"Data Science"}]},{"label":["Skills"],"points":[{"start":1587,"end":1592,"text":"Python"}]},{"label":["Skills"],"points":[{"start":1549,"end":1563,"text":"Azure ML Studio"}]},{"label":["Skills"],"points":[{"start":1537,"end":1546,"text":"Azure IaaS"}]},{"label":["Skills"],"points":[{"start":1495,"end":1513,"text":"Statistics analysis"}]},{"label":["Skills"],"points":[{"start":1480,"end":1492,"text":"Data analysis"}]},{"label":["Skills"],"points":[{"start":1451,"end":1477,"text":"Natural Language Processing"}]},{"label":["Skills"],"points":[{"start":1433,"end":1448,"text":"Machine Learning"}]},{"label":["Skills"],"points":[{"start":1412,"end":1423,"text":"Data Science"}]},{"label":["Skills"],"points":[{"start":1159,"end":1162,"text":"NLTK"}]},{"label":["Skills"],"points":[{"start":772,"end":798,"text":"Natural Language Processing"}]},{"label":["Skills"],"points":[{"start":743,"end":758,"text":"Machine Learning"}]},{"label":["Skills"],"points":[{"start":735,"end":740,"text":"Python"}]},{"label":["Years_of_Experience"],"points":[{"start":223,"end":230,"text":"3+ years"}]},{"label":["Email_Address"],"points":[{"start":143,"end":169,"text":"pushpendu.das1112@gmail.com"}]},{"label":["Mobile_No"],"points":[{"start":117,"end":130,"text":"+91 9916412192"}]},{"label":["Name"],"points":[{"start":0,"end":12,"text":"PUSHPENDU DAS"}]}],"extras":null,"metadata":{"first_done_at":1564137511000,"last_updated_at":1564137511000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "Skills\n\tYears of Experience\n\tProject Name\n\tDuration\n\n\tPython\n\t2 Years\n\tModern Guesthouse and Apartment,Simplilearn\n\t2015-2018\n\n\tR\n\t4 Months\n\tSimplilearn\n\t2016 - 2017\n\n\tPandas\n\t6 Months\n\tSimplilearn\n\t2016 - 2017\n\n\tSql\n\t5 Years\n\tModern Guesthouse and Apartment,Simplilearn\n\t2011-2015\n\n\n\tRADHIKA SRINIVASAN\n\t+91 9880654877 |\n\tBengaluru, India\n\n\tBUSINESS ANALYST | DATA SCIENTIST\n\tradsrinivasan@gmail.com | Skype Id:\n\t\n\t\n\n\t\n\t\n\tradhika.srinivasan19\n\n\nhttps://www.linkedin.com/in/radhika-srinivasan/\n\nI am a Business Analyst with 2.7 years of experience in Air Ambulance service and Open ERP industry. My area of expertise is in Data Management, Business Process Diagrams and Business Application Development. Improved efficiency in database design, customization, development and implementation of software applications. Seeking to leverage technical skills and responsibilities while at the same time, contributing my best to the overall development of the organization. I graduated in Data Scientist Master’s Program with a distinction.\n\nCOMPETENCIES AND SKILLS\n\n· Planning and Implementation\n· Designing and Development\n· Research, Analysis and Documentation\n· Analytical mindset with an eye for detail\n· Action oriented high initiative level\n· Detailed Oriented\n\nEMPLOYMENT CHRONICLE\n\nAviators Air Rescue, Bengaluru, INDIA - BUSINESS ANALYST | BUSINESS MANAGEMENT OCTOBER 2015 - OCTOBER 2017\n\n· Developed Computer Aided Dispatch system, Pre-Hospital Report Application, Medical related forms using MS Access and VBA\n· Skilled in validating and testing complex scenarios for quality standards\n· Experienced in writing Test plans and executing Test cases for Software Applications and Mobile Applications\n· Experienced in analysing and documenting business requirements and system functional specifications including Use Cases\n· Gather data and make a thorough data analysis to extract the relevant information\n· Designed Prototype for Mobile App using Marvel\n· Used Microsoft Visio 365 to create Wireframes and Mock up screens for Computer Aided Dispatch system\n· User Specification documents, Process Flow Chart(s) and workflow\nPrime Minds Consultancy Services, Bengaluru, INDIA - EXECUTIVE BUSINESS ANALYST | TECHNICAL CONSULTANT APRIL 2015 - SEPTEMBER 2015\n\n· Gathered, technical and user requirements for Odoo v 8.0 projects\n· Data analysis and documented business requirements\n· Developed project presentation using Prezi and Odoo v 8.0 (OpenERP) for project demonstration\n· Odoo configuration and customization\n· Trained on TCS iON on Cloud\nReverse Garbage, Brisbane, AUSTRALIA - VOLUNTEER DATABASE SYSTEM\n\nDEVELOPER AUGUST 2014 - NOVEMBER 2014\n\n· Developed modules for workshops with the help of Odoo v8.0 Technology: PostgreSQL, Ubuntu 14.04 using VirtualBox\n\n\nTECHNICAL SKILLS AND CORE COMPETENCIES\n\nData Science with Python\n\n· * * * * Data Analytics, Data Visualization\n· * * * * Machine Learning, Web\nScraping and Natural Language Processing\n\nData Analytics using Python\n\n· * * * * Libraries: NumPy, Pandas, SciPy, Scikit-learn, and Matplotlib\nProgramming Tools\n\n· * * * * SAS, R, Scala\n· * * * * Hadoop MapReduce\nBusiness Analytics with Excel\n\n· * * * * Analysing Data with Pivot Tables, Dashboarding, Power BI\nAdvanced Analytics Tools\n\n· * * * * Flume, Apache Spark, Apache Kafka\nETL Tools\n\n* * * * * Hive, Pig, Sqoop\n\nFile System\n\n* * * * * HDFS\n\nData collection & Storage Tools\n\n* * * * * HBase\n\nDatabases\n\n· * * * * Oracle 10g, Oracle 11g\n· * * * * APEX 4.2, PostgreSQL\n· * * * * Forms / Reports\n· * * * * Normalization\n· * * * * Stored Procedure, Trigger\n· * * * * MS Access, PL/SQL, MySQL\nSoftware Development and Web Programming\n\n· * * * * HTML/HTML5, CSS/CSS3\n· * * * * WEB 2.0\n· * * * * PHP, Java, VBA\n· * * * * django, JavaScript, Lynx\nModern Guesthouse and Apartment Living, Tokyo based, AUSTRALIA INTERN - PYTHON AND DJANGO DEVELOPER JANUARY 2014 - APRIL 2014\n\n· Employed for Pacific Business KK, that trades as Modern Guesthouse and Apartment Living\n· Developed an App for Property Management\n· Received support from stackoverflow.com forum and Heroku support\n· Technology: Python, django, SQLite3, PostgreSQL, Git and Heroku\n\n\nSoftware\n\n· * * * * Microsoft Visio 365\n· * * * * MS Office Suite\nOperating System\n\n· * * * * Windows, Mac OS X\n· * * * * Ubuntu\n\tRADHIKA SRINIVASAN\n\t+91 9880654877 |\n\tBengaluru, India\n\n\tBUSINESS ANALYST | DATA SCIENTIST\n\tradsrinivasan@gmail.com | Skype Id:\n\t\n\t\n\n\t\n\t\n\tradhika.srinivasan19\n\n\nhttps://www.linkedin.com/in/radhika-srinivasan/\n\nAmerica Online, Bengaluru, INDIA - TECHNICAL SUPPORT CONSULTANT\n\nOCTOBER 2006 - JUNE 2007\n\n· Provided chat support for AOL Internet connection and AOL website\n· Handled and resolved technical issues\nSCHOLASTICS\n\n\n\nContent Management\n\n* * * * * Joomla!, WordPress\n\nCloud Application Platform\n\n* * * * * Salesforce, Heroku\n\nVersion Control\n\n* * * * * Git\n\nSimplilearn, Online\n\nFEBRUARY 2018 - OCTOBER 2018\n\nDATA SCIENTIST MASTER’S PROGRAM\n\n· Python\no Data Analytics, Machine Learning, Data Visualization, Web Scraping and Natural Language Processing\n\no Aim of the project is to Analyse complaint types (NYC 311) and city together and to find out top 10 complaint types and their occurrence.\n\n· Big Data, Hadoop and Spark Developer and Apache Kafka o Hadoop, Spark, MapReduce, Pig, Hive, Sqoop, Flume\no  Kafka architecture, installation, configuration and Apache Zookeeper\n\no Aim of the project was to help a Portuguese bank’s contact-center to do direct marketing campaigns which were based on phone calls using SCALA\n\n· Data Science with R certified with project implementation over 3 months o R studio, DPLY functions, Data Visualization, Regression Model\no Aim of the project was to analyse the healthcare cost and utilisation in hospitals using R-Studio.\n\n· Data Science with SAS certified with project implementation over 3 months o Statistics, Hypothesis Testing, Clustering, Decision tree\no Aim of the project was to find employee attrition rate in telecommunication domain\n\n· Business Analytical with Excel\no  Analysing Data with Pivot Tables, Dashboarding, Power BI\n\no Aim of the project was an e-commerce company wanted to design a Sales dashboard to analyze the sales based on various product categories, trends month-wise and product wise, and histogram to analyze number of shipping days.\n\n\n\nAWARDS\n\n· Dean's Excellence Student's Award, from Queensland University of Technology, Australia\nEXTERNAL CERTIFICATIONS\n\n· Data Scientist Master’s Program –\nDistinction 2018\n\n· Business Analytics with Excel 2018\n· Training program on Apache Kafka\n2018\n\n· Data Science with Python 2018\n· Big Data, Hadoop & Spark Developer 2018\n· Data Science with R 2018\n· Data Science with SAS 2018\n· Core Java Certification 2010\n· Java Programmer 2009\nMEMBERSHIPS\n\n· Women In Technology (WIT) 2012 –\n2014\n\n· Australian Computer Society (ACS)\n2013\n\nLINGUISTIC PROFICIENCY\n\nEnglish, Telugu, Kannada, Hindi, Tamil\n\nQueensland University of Technology, Brisbane, AUSTRALIA\n\nFEBRUARY 2012 - NOVEMBER 2013\n\nMASTER OF INFORMATION TECHNOLOGY (ADVANCED) SOFTWARE ARCHITECTURE\n\nKNS Institute of Technology, Visvesvaraya Technological University, Bengaluru, INDIA\n\nBACHELOR’S OF ENGINEERING - INFORMATION SCIENCE AND ENGINEERING\n\nEXTRAMURAL ACCOLADES\n\n· Presentation 2005 PowerPoint, Scientific Applications of Bionics, VTU, India\n· Painting competition on National Integration, Bangalore city police and Rainbow Forum, India\n· Painting competition on Road Safety, Bangalore City Police Traffic Warden Organization, India","annotation":[{"label":["Degree"],"points":[{"start":7134,"end":7158,"text":"BACHELOR’S OF ENGINEERING"}]},{"label":["Skills"],"points":[{"start":6711,"end":6714,"text":"Java"}]},{"label":["Skills"],"points":[{"start":6685,"end":6688,"text":"Java"}]},{"label":["Skills"],"points":[{"start":6669,"end":6671,"text":"SAS"}]},{"label":["Skills"],"points":[{"start":6651,"end":6662,"text":"Data Science"}]},{"label":["Skills"],"points":[{"start":6642,"end":6642,"text":"R"}]},{"label":["Skills"],"points":[{"start":6624,"end":6635,"text":"Data Science"}]},{"label":["Skills"],"points":[{"start":6568,"end":6573,"text":"Python"}]},{"label":["Skills"],"points":[{"start":6550,"end":6561,"text":"Data Science"}]},{"label":["Skills"],"points":[{"start":6529,"end":6540,"text":"Apache Kafka"}]},{"label":["Certifications"],"points":[{"start":6418,"end":6448,"text":"Data Scientist Master’s Program"}]},{"label":["Skills"],"points":[{"start":5769,"end":5771,"text":"SAS"}]},{"label":["Skills"],"points":[{"start":5751,"end":5762,"text":"Data Science"}]},{"label":["Skills"],"points":[{"start":5610,"end":5627,"text":"Data Visualization"}]},{"label":["Skills"],"points":[{"start":5528,"end":5528,"text":"R"}]},{"label":["Skills"],"points":[{"start":5510,"end":5521,"text":"Data Science"}]},{"label":["Skills"],"points":[{"start":5283,"end":5287,"text":"Flume"}]},{"label":["Tools"],"points":[{"start":5276,"end":5280,"text":"Sqoop"}]},{"label":["Tools"],"points":[{"start":5270,"end":5273,"text":"Hive"}]},{"label":["Tools"],"points":[{"start":5265,"end":5267,"text":"Pig"}]},{"label":["Skills"],"points":[{"start":5224,"end":5235,"text":"Apache Kafka"}]},{"label":["Skills"],"points":[{"start":5011,"end":5037,"text":"Natural Language Processing"}]},{"label":["Skills"],"points":[{"start":4974,"end":4991,"text":"Data Visualization"}]},{"label":["Skills"],"points":[{"start":4956,"end":4971,"text":"Machine Learning"}]},{"label":["Skills"],"points":[{"start":4940,"end":4953,"text":"Data Analytics"}]},{"label":["Skills"],"points":[{"start":4931,"end":4936,"text":"Python"}]},{"label":["Email_Address"],"points":[{"start":4373,"end":4395,"text":"radsrinivasan@gmail.com"}]},{"label":["Mobile_No"],"points":[{"start":4301,"end":4314,"text":"+91 9880654877"}]},{"label":["Name"],"points":[{"start":4281,"end":4298,"text":"RADHIKA SRINIVASAN"}]},{"label":["Operating_Systems"],"points":[{"start":4273,"end":4278,"text":"Ubuntu"}]},{"label":["Operating_Systems"],"points":[{"start":4254,"end":4259,"text":"Mac OS"}]},{"label":["Operating_Systems"],"points":[{"start":4245,"end":4251,"text":"Windows"}]},{"label":["Skills"],"points":[{"start":4122,"end":4131,"text":"PostgreSQL"}]},{"label":["Skills"],"points":[{"start":4105,"end":4110,"text":"django"}]},{"label":["Skills"],"points":[{"start":4097,"end":4102,"text":"Python"}]},{"label":["Skills"],"points":[{"start":3751,"end":3754,"text":"Lynx"}]},{"label":["Skills"],"points":[{"start":3739,"end":3748,"text":"JavaScript"}]},{"label":["Skills"],"points":[{"start":3731,"end":3736,"text":"django"}]},{"label":["Skills"],"points":[{"start":3717,"end":3719,"text":"VBA"}]},{"label":["Skills"],"points":[{"start":3711,"end":3714,"text":"Java"}]},{"label":["Skills"],"points":[{"start":3706,"end":3708,"text":"PHP"}]},{"label":["Skills"],"points":[{"start":3688,"end":3690,"text":"WEB"}]},{"label":["Skills"],"points":[{"start":3673,"end":3675,"text":"CSS"}]},{"label":["Skills"],"points":[{"start":3669,"end":3671,"text":"CSS"}]},{"label":["Skills"],"points":[{"start":3662,"end":3665,"text":"HTML"}]},{"label":["Skills"],"points":[{"start":3657,"end":3660,"text":"HTML"}]},{"label":["Skills"],"points":[{"start":3599,"end":3603,"text":"MySQL"}]},{"label":["Skills"],"points":[{"start":3591,"end":3596,"text":"PL/SQL"}]},{"label":["Skills"],"points":[{"start":3580,"end":3588,"text":"MS Access"}]},{"label":["Skills"],"points":[{"start":3562,"end":3568,"text":"Trigger"}]},{"label":["Skills"],"points":[{"start":3544,"end":3559,"text":"Stored Procedure"}]},{"label":["Skills"],"points":[{"start":3520,"end":3532,"text":"Normalization"}]},{"label":["Skills"],"points":[{"start":3502,"end":3508,"text":"Reports"}]},{"label":["Skills"],"points":[{"start":3494,"end":3498,"text":"Forms"}]},{"label":["Skills"],"points":[{"start":3473,"end":3482,"text":"PostgreSQL"}]},{"label":["Skills"],"points":[{"start":3463,"end":3466,"text":"APEX"}]},{"label":["Skills"],"points":[{"start":3442,"end":3447,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":3430,"end":3439,"text":"Oracle 10g"}]},{"label":["Tools"],"points":[{"start":3402,"end":3406,"text":"HBase"}]},{"label":["Skills"],"points":[{"start":3353,"end":3356,"text":"HDFS"}]},{"label":["Tools"],"points":[{"start":3323,"end":3327,"text":"Sqoop"}]},{"label":["Tools"],"points":[{"start":3318,"end":3320,"text":"Pig"}]},{"label":["Tools"],"points":[{"start":3312,"end":3315,"text":"Hive"}]},{"label":["Skills"],"points":[{"start":3278,"end":3289,"text":"Apache Kafka"}]},{"label":["Skills"],"points":[{"start":3264,"end":3275,"text":"Apache Spark"}]},{"label":["Skills"],"points":[{"start":3257,"end":3261,"text":"Flume"}]},{"label":["Skills"],"points":[{"start":3106,"end":3121,"text":"Hadoop MapReduce"}]},{"label":["Skills"],"points":[{"start":3090,"end":3094,"text":"Scala"}]},{"label":["Skills"],"points":[{"start":3087,"end":3087,"text":"R"}]},{"label":["Skills"],"points":[{"start":3082,"end":3084,"text":"SAS"}]},{"label":["Skills"],"points":[{"start":3042,"end":3051,"text":"Matplotlib"}]},{"label":["Skills"],"points":[{"start":3024,"end":3035,"text":"Scikit-learn"}]},{"label":["Skills"],"points":[{"start":3017,"end":3021,"text":"SciPy"}]},{"label":["Skills"],"points":[{"start":3009,"end":3014,"text":"Pandas"}]},{"label":["Skills"],"points":[{"start":3002,"end":3006,"text":"NumPy"}]},{"label":["Skills"],"points":[{"start":2973,"end":2978,"text":"Python"}]},{"label":["Skills"],"points":[{"start":2952,"end":2965,"text":"Data Analytics"}]},{"label":["Skills"],"points":[{"start":2923,"end":2949,"text":"Natural Language Processing"}]},{"label":["Skills"],"points":[{"start":2906,"end":2917,"text":"Web\nScraping"}]},{"label":["Skills"],"points":[{"start":2888,"end":2903,"text":"Machine Learning"}]},{"label":["Skills"],"points":[{"start":2859,"end":2876,"text":"Data Visualization"}]},{"label":["Skills"],"points":[{"start":2843,"end":2856,"text":"Data Analytics"}]},{"label":["Skills"],"points":[{"start":2825,"end":2830,"text":"Python"}]},{"label":["Skills"],"points":[{"start":2807,"end":2818,"text":"Data Science"}]},{"label":["Operating_Systems"],"points":[{"start":2735,"end":2740,"text":"Ubuntu"}]},{"label":["Skills"],"points":[{"start":2723,"end":2732,"text":"PostgreSQL"}]},{"label":["Skills"],"points":[{"start":1511,"end":1513,"text":"VBA"}]},{"label":["Skills"],"points":[{"start":1497,"end":1505,"text":"MS Access"}]},{"label":["Skills"],"points":[{"start":1143,"end":1155,"text":"Documentation"}]},{"label":["Skills"],"points":[{"start":1130,"end":1137,"text":"Analysis"}]},{"label":["Skills"],"points":[{"start":1120,"end":1127,"text":"Research"}]},{"label":["Skills"],"points":[{"start":1092,"end":1116,"text":"Designing and Development"}]},{"label":["Skills"],"points":[{"start":1062,"end":1088,"text":"Planning and Implementation"}]},{"label":["Certifications"],"points":[{"start":982,"end":1012,"text":"Data Scientist Master’s Program"}]},{"label":["Years_of_Experience"],"points":[{"start":524,"end":532,"text":"2.7 years"}]},{"label":["Email_Address"],"points":[{"start":377,"end":399,"text":"radsrinivasan@gmail.com"}]},{"label":["Mobile_No"],"points":[{"start":305,"end":318,"text":"+91 9880654877"}]},{"label":["Name"],"points":[{"start":285,"end":302,"text":"RADHIKA SRINIVASAN"}]},{"label":["Skills"],"points":[{"start":213,"end":215,"text":"Sql"}]},{"label":["Skills"],"points":[{"start":168,"end":173,"text":"Pandas"}]},{"label":["Skills"],"points":[{"start":128,"end":128,"text":"R"}]},{"label":["Skills"],"points":[{"start":54,"end":59,"text":"Python"}]}],"extras":null,"metadata":{"first_done_at":1564231274000,"last_updated_at":1564231274000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "Rahul Pratap Singh \nE-Mail: rathaur.rps@gmail.com\n\n\n\nContact: +91 9953579739(M)\nObjective\nTo seek the challenging position in Software industry that needs innovation, creativity, dedication and enable me to continue to work in a challenging and fast paced environment, leveraging my current knowledge and fostering creativity with many learning opportunities. \n\nProfessional Synopsis\n· A growth oriented professional total experience of 8 years in IT industry including 3 years in the BigData Technologies.\n·  Having exposure in the complete Software development life cycle involving development, documentation, and testing.  Currently working with the Wipro Technologies Solutions, Bangalore, India as project Lead\n· Experience of end-to-end implementation Of BIG data & Financial Solutions\n· Hands on Experience in working with Hadoop ecosystem like Hive, PIG, Sqoop, Map Reduce, Oozie\n\n· Hands on Experience in Scala ,Spark Core, Spark Sql, Spark Streaming, Kafka, Cassandra\n· Proficient in team handling and people management. \n\n· Exposure in handling client queries, providing them feasible solutions & building healthy relationships thereby achieving high customer satisfaction.\n\n· Ability to perform and deliver under pressure and deadlines, and to work with a team\n· Capable to delve into the new leading Technologies\nExperience Summary\n· Working with Wipro Technologies since  FEB 2016 to till date. Currently Working as Big data Analytics project lead for last 3 years\n· Worked with NSEL Noida as a software Engineer before the Wipro engagement. \nWorked on CAS product enhancement, implementation and support project for 4.7 years & delivered \nQuality services to the reputed Banks and NBFC’s \n\n\tTechnical Skills\n\t\n\n\tBigData Skills\n\tHadoop, HDFS, , Hive, Pig, Sqoop, MapReduce, Oozie, Hbase,  Spark, Scala, Kafka, Cassndra, \n\n\tJava Technologies       \n\tJava/J2ee, Struts1.2, Rest Web Services , java Scripts, Spring\n\n\tRDBMS\n\tOracle SQL/PL-SQL\n\n\tFramework\n\tHadoop (HDFS), Struts 1.2\n\n\tVersion control Tool\n\t     SVN, Git\n\n\tApplication Servers\n\tOracle -10g ,  Weblogic , WAS\n\n\tHadoop Distributions\n\tCloudera \n\n\tDev Tools\n\tWindows, Linux, Eclipse\n\n\nProject Exposure in Wipro\n\tPROJECT-1 EDP Yes Bank\n\t\n\n\tClient\n\nSkills \n\tTD Bank()\nHDFS, Hive, Pig , Shell Script, Tableau, HBASE, Rest API, OOZIE, IMPALA, Spark, Scala\n\n\tRole \n\tProject Lead\n\n\tTeam Size\n\t07                                 \n\n\tDuration\n\nProcess\n\tDEC 2017 to till date.\nAgile\n\n\n\nDescription: \n\n· Working with leading Bank to build Enterprise Data provisioning platform. \n· Data ingestion is taking place from various Upstream systems, FCR, API data & SFDC.\n· Data is ingestion is done into HDFS.\n· The incoming data is specifically related to banking customers, Retail & corporate Lending, Cards and account details which is filtered against data quality checks and stored for further processing.\n· The input data is dierectly loaded from oracle tables to HDFS using Sqoop. Some data is in  XML, CSV and JSON File formats and same is loaded using HDFS Scripts.\n·  Job workflows are scheduled using Oozie.\n· Data is transformed using Pig scripts and loaded in the Hive tables in ORC format.\n· Based on the requirement data is loaded in HBASE ,Cassandra & RDBMS tables, and also directly used by downstream systems from Hive \n\n· HBASE is used as input for mReport system which is Data as a service implementation.\nResponsibilities:  \n\nInvolved in the business oriented process like requirement gathering, design the solution on Big Data Platform, implementation/development of the requirement using Big Data capabilities, ramping up new team members, client’s status call, Unit Testing.\n· Leading a Team of 7 members involved in the EDPP analytics project\n· Requirement gathering & Co-ordination with Client.\n\n· Big data Project Implementation based on the requirement\n· Loading Data from source Systems to HDFS\n· Performing ETL operations using pig.\n\n· Implementing pig operations using Cascading APIs.\n\n· Loading data into hive tables and Creating hive views.\n· Loading data into Hbase.\n\n· Writing UDF’s & map reduce code\n· Involved in writing Junit test cases.\n..\n\n\tPROJECT 2- EDPP– BOQ\n\t\n\n\tClient\n\nSkills \n\tBOQ\n\nJava, HDFS, Hive, Pig , Shell Script, Tableau, HBASE, Rest API, OOZIE, IMPALA\n\n\tRole \n\tSenior Developer\n\n\tTeam Size\n\t05                                \n\n\tDuration\n\nProcess\n\tFEB 2016- DEC 2017 \nAgile\n\n\n\n\nDescription: \n\n· Working with leading Bank to build Enterprise Data provisioning platform. \n· Data ingestion is taking place from various Upstream systems like  Lending , FCR, API data & SFDC.\n\n· Data ingestion is done by Sqoop and HDFS commands.\n\n· The incoming data is specifically related to banking customers, Retail & corporate Lending, Cards and account details which is filtered against data quality checks and stored for further processing.\n\n· The input data is directly loaded from oracle tables to HDFS using Sqoop. Some data is in  XML, CSV and JSON File formats and same is loaded into HDFS using HDFS commands. \n· Job workflows are scheduled using Oozie.\n\n· Data is transformed using Pig scripts and loaded in the Hive tables in ORC format.\n\n· Based on the requirement data is loaded in HBASE , RDBMS tables, and also directly used by downstream systems from Hive \n\nResponsibilities:  \n\nInvolved in the business oriented process like requirement gathering, design the solution on Big Data Platform, implementation/development of the requirement using Big Data capabilities, ramping up new team members, client’s status call, Unit Testing.\n· Requirement gathering & Co-ordination with Client.\n\n· Big data Project Implementation based on the requirement\n\n· Loading Data from source Systems to HDFS\n\n· Involved for writing sqoop scripts for data import in HDFS\n· Performing data cleansing operations using pig.\n\n· Implementing pig operations using Cascading APIs.\n\n· Loading data into hive tables and Creating hive views.\n\n· Loading data into Hbase.\n\n· Writing UDF’s & map reduce code\n\n· Involved in writing Junit test cases.\n\n.\nProject # 3: Aircel Campaign Project \n\nCampaign Analytics component retrieves the required data from the source system - Nextra Unified.\n\nCampaign Analytics Unified generates Events and Customer Data. These are exported at the pre-set frequency (Configurable - say 15 min, 30 minutes etc., for Event Data and once in a day for Customer Data, which is configurable). These are exported using shell script, which SFTPs and coverts the files from TCI to CSV format.\n\nETL Tool PIG Latin processes the flat files, cleanses the data and applies the business rules, loading the data into HDFS (Hive output tables).\n\nETL Tool PIG Latin processes the flat files, cleanses the data and applies the business rules, loading the data into HDFS (Hive output tables). \n\n.\nResponsibilities:  \n\nInvolved in the business oriented process like requirement gathering, design the solution on Big Data Platform, implementation/development of the requirement using Big Data capabilities, ramping up new team members, client’s status call, Unit Testing.\n· Requirement gathering & Co-ordination with Client.\n\n· Big data Project Implementation based on the requirement\n\n· Loading Data from source Systems to HDFS\n\n· Performing data cleansing operations using pig.\n· Loading data into hive tables and Creating hive views.\n\nProject # 3: HDFC CAS on mobile implementation. Handling and leading Implementation \n\n· Interacting with the client for requirements gathering & scoping and providing solution.\n\n· Handling architecture and designing the solution.\n\n· Developing application and undertaking code review.\n\n· Managing process setup & development with the team\n\n· Production Implementation of the product\n\n· Coordinating for Client Testing, Deployment and Training.\n\n· Debugging and troubleshooting the application. \n\tTechnical Skills\n\t\n\n\tTechnologies Used       \n\t Java, Servlets, JSP, JDBC, Struts 1.2 Frame Work, XML and Java Script and,    android, Webservices , Oracle PL/SQL\n\n\tTools Used       \n\tJDeveloper , Eclipse\n\n\nProject#4 BOM Philippines CAS on mobile implementation. Handling and leading implementation\n\n· Interacting with the client for requirements gathering & scoping and providing solution.\n\n· Handling architecture and designing the solution.\n\n· Developing application and undertaking code review.\n\n· Managing process setup & development with the team\n\n· Production Implementation of the product\n\n· Coordinating for Client Testing, Deployment and Training.\n\n· Debugging and troubleshooting the application. \n\n\tTechnical Skills\n\t\n\n\tTechnologies Used       \n\tJava, Servlets, JSP, JDBC, Struts 1.2 Frame Work, XML and Java Script and,    android, Webservices , Oracle PL/SQL\n\n\tTools Used       \n\tJDeveloper , Eclipse\n\n\nProject # 5: CAS SBI\nProject Description:  Customer Acquisition System (CAS) is a Web-based system that fulfils the requirements of banking and non-banking financial organizations in automating the loan origination and processing operations This system serve as an open standard, scalable, and maintainable solution for organizations in the lending industry .the loan processing mechanism takes into account various categories and products for which loan is sought, such as \n\n1. AUTO LOAN \n\n2. PERSION LOAN \n\n3. EDUCATION LOANS\n\n4. HOME LOANS\n\n\tTechnical Skills\n\t\n\n\tTechnologies Used       \n\t  Java, Servlets, JSP, JDBC, Struts 1.2 Frame Work, XML and Java Script and,  Webservices , Oracle PL/SQL\n\n\tTools Used       \n\tJDeveloper , Eclipse\n\n\n\t\n\n\n\t\n\n\nProclamation                                                                                                           Date: Monday, April 22, 2019 \n\nHereby I declared the above reveal data is true and correct the supportive information and document can be provided on demand.\nYours Sincerely\nRahul Pratap Singh\nAcademia Dossier\n\n\n\nB.TECH-(Computer Science)\n\n (UPTU) – 2010  -  63%\n\n\n\n10+2- (UP Board) - 67%\n\n\n\n10th - (UP Board) - 77%\n\n\n\n\n\nPersonal Dossier\n\n\n\nAddress:  27 - B Ritty Villa, Kodathi Gate, Sarjapura  Road ,  Bangalore\n\n\n\n\n\nLanguages Known: English, Hindi and Sanskrit\n\n\n\nMarital Status: Married","annotation":[{"label":["Degree"],"points":[{"start":9815,"end":9820,"text":"B.TECH"}]},{"label":["Name"],"points":[{"start":9776,"end":9793,"text":"Rahul Pratap Singh"}]},{"label":["Skills"],"points":[{"start":9424,"end":9429,"text":"PL/SQL"}]},{"label":["Skills"],"points":[{"start":9417,"end":9422,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":9385,"end":9388,"text":"Java"}]},{"label":["Skills"],"points":[{"start":9354,"end":9359,"text":"Struts"}]},{"label":["Skills"],"points":[{"start":9327,"end":9330,"text":"Java"}]},{"label":["Skills"],"points":[{"start":8682,"end":8687,"text":"PL/SQL"}]},{"label":["Skills"],"points":[{"start":8675,"end":8680,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":8632,"end":8635,"text":"Java"}]},{"label":["Skills"],"points":[{"start":8601,"end":8606,"text":"Struts"}]},{"label":["Skills"],"points":[{"start":8574,"end":8577,"text":"Java"}]},{"label":["Skills"],"points":[{"start":7972,"end":7977,"text":"PL/SQL"}]},{"label":["Skills"],"points":[{"start":7965,"end":7970,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":7922,"end":7925,"text":"Java"}]},{"label":["Skills"],"points":[{"start":7891,"end":7896,"text":"Struts"}]},{"label":["Skills"],"points":[{"start":7864,"end":7867,"text":"Java"}]},{"label":["Skills"],"points":[{"start":7206,"end":7209,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":6756,"end":6759,"text":"Hive"}]},{"label":["Skills"],"points":[{"start":6750,"end":6753,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":6611,"end":6614,"text":"Hive"}]},{"label":["Skills"],"points":[{"start":6605,"end":6608,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":5938,"end":5942,"text":"Hbase"}]},{"label":["Skills"],"points":[{"start":5751,"end":5754,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":5689,"end":5692,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":5257,"end":5260,"text":"Hive"}]},{"label":["Skills"],"points":[{"start":5112,"end":5115,"text":"Hive"}]},{"label":["Skills"],"points":[{"start":5082,"end":5084,"text":"Pig"}]},{"label":["Skills"],"points":[{"start":5046,"end":5050,"text":"Oozie"}]},{"label":["Skills"],"points":[{"start":4994,"end":4997,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":4983,"end":4986,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":4904,"end":4908,"text":"Sqoop"}]},{"label":["Skills"],"points":[{"start":4893,"end":4896,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":4617,"end":4620,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":4607,"end":4611,"text":"Sqoop"}]},{"label":["Skills"],"points":[{"start":4200,"end":4202,"text":"Pig"}]},{"label":["Skills"],"points":[{"start":4194,"end":4197,"text":"Hive"}]},{"label":["Skills"],"points":[{"start":4188,"end":4191,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":4182,"end":4185,"text":"Java"}]},{"label":["Skills"],"points":[{"start":4048,"end":4052,"text":"Hbase"}]},{"label":["Skills"],"points":[{"start":3873,"end":3876,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":3286,"end":3289,"text":"Hive"}]},{"label":["Skills"],"points":[{"start":3131,"end":3134,"text":"Hive"}]},{"label":["Skills"],"points":[{"start":3101,"end":3103,"text":"Pig"}]},{"label":["Skills"],"points":[{"start":3066,"end":3070,"text":"Oozie"}]},{"label":["Skills"],"points":[{"start":3015,"end":3018,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":2935,"end":2939,"text":"Sqoop"}]},{"label":["Skills"],"points":[{"start":2924,"end":2927,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":2658,"end":2661,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":2317,"end":2321,"text":"Scala"}]},{"label":["Skills"],"points":[{"start":2310,"end":2314,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":2249,"end":2251,"text":"Pig"}]},{"label":["Skills"],"points":[{"start":2243,"end":2246,"text":"Hive"}]},{"label":["Skills"],"points":[{"start":2237,"end":2240,"text":"HDFS"}]},{"label":["Operating_Systems"],"points":[{"start":2139,"end":2143,"text":"Linux"}]},{"label":["Operating_Systems"],"points":[{"start":2130,"end":2136,"text":"Windows"}]},{"label":["Skills"],"points":[{"start":2085,"end":2090,"text":"Hadoop"}]},{"label":["Skills"],"points":[{"start":2079,"end":2081,"text":"WAS"}]},{"label":["Skills"],"points":[{"start":2068,"end":2075,"text":"Weblogic"}]},{"label":["Skills"],"points":[{"start":2053,"end":2058,"text":"Oracle"}]},{"label":["Tools"],"points":[{"start":2026,"end":2028,"text":"Git"}]},{"label":["Tools"],"points":[{"start":2021,"end":2023,"text":"SVN"}]},{"label":["Skills"],"points":[{"start":1981,"end":1986,"text":"Struts"}]},{"label":["Skills"],"points":[{"start":1974,"end":1977,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":1966,"end":1971,"text":"Hadoop"}]},{"label":["Skills"],"points":[{"start":1946,"end":1951,"text":"PL-SQL"}]},{"label":["Skills"],"points":[{"start":1942,"end":1944,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":1935,"end":1940,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":1919,"end":1924,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":1905,"end":1916,"text":"java Scripts"}]},{"label":["Skills"],"points":[{"start":1885,"end":1901,"text":"Rest Web Services"}]},{"label":["Skills"],"points":[{"start":1874,"end":1882,"text":"Struts1.2"}]},{"label":["Skills"],"points":[{"start":1868,"end":1871,"text":"J2ee"}]},{"label":["Skills"],"points":[{"start":1863,"end":1866,"text":"Java"}]},{"label":["Skills"],"points":[{"start":1837,"end":1840,"text":"Java"}]},{"label":["Skills"],"points":[{"start":1824,"end":1831,"text":"Cassndra"}]},{"label":["Skills"],"points":[{"start":1817,"end":1821,"text":"Kafka"}]},{"label":["Skills"],"points":[{"start":1810,"end":1814,"text":"Scala"}]},{"label":["Skills"],"points":[{"start":1803,"end":1807,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":1795,"end":1799,"text":"Hbase"}]},{"label":["Skills"],"points":[{"start":1788,"end":1792,"text":"Oozie"}]},{"label":["Skills"],"points":[{"start":1777,"end":1785,"text":"MapReduce"}]},{"label":["Skills"],"points":[{"start":1770,"end":1774,"text":"Sqoop"}]},{"label":["Skills"],"points":[{"start":1765,"end":1767,"text":"Pig"}]},{"label":["Skills"],"points":[{"start":1759,"end":1762,"text":"Hive"}]},{"label":["Skills"],"points":[{"start":1751,"end":1754,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":1743,"end":1748,"text":"Hadoop"}]},{"label":["Skills"],"points":[{"start":1727,"end":1733,"text":"BigData"}]},{"label":["Skills"],"points":[{"start":961,"end":965,"text":"Kafka"}]},{"label":["Skills"],"points":[{"start":944,"end":948,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":933,"end":937,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":921,"end":925,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":914,"end":918,"text":"Scala"}]},{"label":["Skills"],"points":[{"start":882,"end":886,"text":"Oozie"}]},{"label":["Skills"],"points":[{"start":863,"end":867,"text":"Sqoop"}]},{"label":["Skills"],"points":[{"start":852,"end":855,"text":"Hive"}]},{"label":["Skills"],"points":[{"start":830,"end":835,"text":"Hadoop"}]},{"label":["Skills"],"points":[{"start":485,"end":491,"text":"BigData"}]},{"label":["Years_of_Experience"],"points":[{"start":437,"end":443,"text":"8 years"}]},{"label":["Mobile_No"],"points":[{"start":62,"end":78,"text":"+91 9953579739(M)"}]},{"label":["Email_Address"],"points":[{"start":28,"end":48,"text":"rathaur.rps@gmail.com"}]},{"label":["Name"],"points":[{"start":0,"end":17,"text":"Rahul Pratap Singh"}]}],"extras":null,"metadata":{"first_done_at":1564211120000,"last_updated_at":1564211120000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "Raj Gupta \n\n                                                   Noida, UP | 8800804404 | rg2592@gmail.com \n\nProfessional Summary \n\n• Company training in JAVA with frameworks. \n\n• Forward-thinking development professional bringing expertise in Java, SQL, HTML, CSS and other \nprogramming languages. \n\n• Experience of working in three projects in Insurance and Retail. \n\n• Excellent written and verbal communication skills, self-motivator and a good team player. \n\n• Capable of quickly grasping and applying new concepts / technologies and the ability to impart \nknowledge to others on technical topics. \n\n• Capable of problem solving and delivering realistic and practical solutions. Enthusiastic, innovative and \nchallenge oriented \n\nSkills \n\n• Operating Systems: Windows 7-10 , DOS \n\n \n\n• Languages: JAVA ,UNIX(Shell) , \n\nC(Basic) \n\n \n\n• Databases: Oracle SQL developer , \n\nSQuirreL \n\n \n\n• Tools: Service now, Control-M, JIRA, \n\nEclipse \n\n \n\n• Domain: Retail, Insurance \n\n \n\n \n\n• Project management \n\n• Maintaining applications \n\n• Process improvements \n\n• Agile methodology \n\n• Team leadership \n\n• Project documentation \n\n• Testing plans \n\n• Excellent communication skills \n\n• Workflow analysis \n\n• Written and oral communication \n\nWork History \n\nTotal Work Exp:  \n\n1. JAVA DEVELOPER, 04/2018-01/2019 \n\nTata Consultancy Services-Noida, UP \n\nClient: Walgreens pharmacy Ltd, US \n\n• Worked on JIRA tool for development in  JAVA language and SQL \n\n• Contributed to team success by completing jobs quickly and accurately. \n\n• Resolved issues in a timely manner to maintain productivity goals. \n\n• Participated in requirements gathering and design development meetings. \n\n• Reviewed code and debugged errors to improve performance. \n\n \n\n2. Application support and development, 03/2017-03/2018 \n\nTata Consultancy Services-Noida, UP \n\nClient: Sears Holding corp. –US                                                                                                                Contd... \n\n                                                                                                                                                                             \n\n\n\nPage 2 \n\n \n\n• Project aimed at providing a complete solution to the client or technicians issues which raise through \n\nincident management Service Now. \n\n• Recognized as subject matter expert for LIS, AIMS. \n\n• Reviewed code and debugged errors to improve performance. \n\n \n\n3. Developer and support executive, 05/2015-12/2018 \n\nTata Consultancy Services-Mumbai, Maharashtra \n\nClient-HDFC Standard Life Insurance, Mumbai (HQ) \n\n• Technical Enabled Business Transformation, Project aimed at providing a complete solution to the \n\nclient from new business(Lead generation) to Conversion of insurance policies. \n\n• System uses the data base and other tools for providing the Fixes to the client whether it's temporary or \n\npermanent. \n\n• Key role for handling the live issues and provide the resolution to the client in given SLA based on \n\nticket raised by client or sales agent. \n\n• Developed new process for employee evaluation which resulted in marked performance improvements. \n\n \n\nEducation \n\n \n\nB.TECH: ELECTRICAL AND ELECTRONICS, 2014 \n\nABES Engineering College- Ghaziabad, UP \n\n• Graduated with 71.1% \n\n10+2: PCM (CBSE), 2009 \n\nSBM, Hapur \n\n• Passed with 85.6% \n\n10TH: CORE SUBJECTS (CBSE), 2007 \n\nSBM, Hapur \n\n• Passed with 80.66% \n\n \n\nLocation Preferences \n\nOffshore: Noida, Gurgaon, Pune and Bangalore \n\nOnshore: US, Canada, UK and Europe \n\nDeclaration \n\nI do here by confirm that the information given in this form is true to do the best of my knowledge and belief. \n\nRaj Gupta \n\nDate: \n\n\n\nPage 3","annotation":[{"label":["Name"],"points":[{"start":3636,"end":3644,"text":"Raj Gupta"}]},{"label":["Degree"],"points":[{"start":3157,"end":3162,"text":"B.TECH"}]},{"label":["Skills"],"points":[{"start":1439,"end":1441,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":1421,"end":1424,"text":"JAVA"}]},{"label":["Tools"],"points":[{"start":1391,"end":1394,"text":"JIRA"}]},{"label":["Skills"],"points":[{"start":1270,"end":1273,"text":"JAVA"}]},{"label":["Tools"],"points":[{"start":929,"end":935,"text":"Eclipse"}]},{"label":["Tools"],"points":[{"start":921,"end":924,"text":"JIRA"}]},{"label":["Tools"],"points":[{"start":910,"end":918,"text":"Control-M"}]},{"label":["Tools"],"points":[{"start":897,"end":907,"text":"Service now"}]},{"label":["Skills"],"points":[{"start":874,"end":881,"text":"SQuirreL"}]},{"label":["Skills"],"points":[{"start":856,"end":858,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":848,"end":854,"text":" Oracle"}]},{"label":["Skills"],"points":[{"start":822,"end":829,"text":"C(Basic)"}]},{"label":["Skills"],"points":[{"start":806,"end":816,"text":"UNIX(Shell)"}]},{"label":["Skills"],"points":[{"start":800,"end":803,"text":"JAVA"}]},{"label":["Operating_Systems"],"points":[{"start":763,"end":769,"text":"Windows"}]},{"label":["Skills"],"points":[{"start":248,"end":250,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":152,"end":155,"text":"JAVA"}]},{"label":["Email_Address"],"points":[{"start":88,"end":103,"text":"rg2592@gmail.com"}]},{"label":["Mobile_No"],"points":[{"start":75,"end":84,"text":"8800804404"}]},{"label":["Name"],"points":[{"start":0,"end":8,"text":"Raj Gupta"}]}],"extras":null,"metadata":{"first_done_at":1564212277000,"last_updated_at":1564212277000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "RAJA SEKHAR REDDY DODDA\n\nContact: +91-9160100636.\n\nE-Mail:rajasekhardodda.ds@gmail.com\n\nOBJECTIVE\n\nTo seek a challenging position in an organization that provides me an opportunity to pursue career in field of\nData Science & Big data Analytics and provide me with global exposure to excel in my work to make\ncontribution towards growth of the organization.\n\nCORE COMPETENCIES \n\nSummary:consultant with 3 years of experience in core data science,\nAnalytics, Experience in Banking, credit cards, Telecom, Retail, Web\nanalytics and Debt Collection Strategy models.\n\nI have been working on different Algorithms include Machine Learning\ntechniques, NLP (Natural language processing), Linear & Logistic\nRegression analysis, Segmentation, Decisions trees, Cluster analysis\nand factor analysis, Natural language processing, Time Series Analysis,\nK-Means algorithm, Random Forests Algorithm, Sentimental analysis.\n\nMajor Contribution Towards: MACHINE LEARNING Algorithms\n(Predictive analytics).s\n\nTECHNICAL SKILLSET\n\n Data analysis Using various machine learning algorithms and deep learning algorithms.\n Proficient in tools for analysis using Python (NumPy, Pandas, Matplotlib, seaborn, scikit learn, stat models\n\nand NLTK).\n R Tool (Caret, Random Forest, Dplyr and ggplot2).\n Data visualization using R, Python and Tableau.\n Predictive analytics and NLP.\n Platforms like Linux/UNIX and Virtual Machine Environments.\n Experience with SQL and databases like My SQL server and Teradata.\n Well versed in big data tools are pig, Hive and Impala.\n\nBitstat Technologies Pvt Ltd                       Auguest 2015 to till date                        Senior Consultant\n\nProjects:\n\n Voice connectivity Project :\nIt was a US telecom company project in which we have to find out the major reasons behind the call\ndrop. While making any call  the company stores all the information of the customers such as Cell ID,\nLocation Area Code, Type Allocation Code, Physical Cell ID, Latitude and Longitude, Network type ,\nName of network operator, data state on device, is device on roaming, mobile country code, mobile\n\nSystems Analysis\n\nMachine Learning/Deep\nLearning/Tableau/Analytical\n\nSpecialist\n\n/NLP\n\nPredictive Analytics\n\n\n\nnetwork code, frequency band, etc.. Data is prepared in such a way that these change in attributes at\nthe time of call drop and call end are captured.\nSuccessful call have events such as \n New outgoing call\n Placing call\n Off hook\n Connected \n Call disconnected\n Call end\n\nCall drop call have events such as\n New outgoing call\n Placing call\n Off hook\n Connected \n Call drop\n\n Responsibilities:                  \n\n Worked on data pre-processing stage which involved cleaning of data using Python\n Created various dummy variables from the data using Python \n Used data imputation techniques for missing data using Python\n Involved in coming up with various visualizations using ggplot\n Analyzed and used advanced analytical techniques such as Regression, ARM and\n\nRandom forest algorithm.\n Used ROC and AUC to check optimal cutoff value and performance of model.\n\nPROJECT II: Web site views forecasting.\nTools: Python (NumPy, Pandas, Stat models, Matplotlib, scikitlearn and seaborn).\nDuration:Nov 2016 - Sep2017.\n\nRoles & Responsibility:\n\n Identifying the business problem and understanding the requirements of the clients.\n Weblog data sanitization by extracting useful information from the web URL and outlier treatment.\n Identifying the KPIs and representing them in visual form using graphs and charts.\n Identifying and neutralizing volatility and seasonality in the data.\n Identifying the model (AR, MA, ARIMA) and the order of the model.\n Building the model and cross validation of model using white noise and other metrics (RMSE, AIC,\n\nMAPE).\n Forecasting the values using ARIMA model.\n\nPROJECT III: Digital Marketing Monthly BI Reports and analysis and forecasting for website views.\nTools: R, Python,Tableau,Excel and SQL server.\nDuration:Dec 2015 - Oct 2016.\n\n Phase1: Weblog data sanitization, missing value treatment, outlier treatment finally data audit.\n\n\n\n Phase2: Preparation of BI reports that include creating tables, descriptive analysis & charts. Visualization\nof Key Performance Indicators. Control charts and alert system. Benchmark analysis & RAG status\ncharts.\n\n Phase3: Automation of tables & reports generation using R& Excel macros.\n\nSocial Media Reporting and analysis:\n\n Phase1: Data preparation including data sanitization, missing value treatment and outlier treatment. \n Phase2: Reporting of social media metrics like number of comments, mentions, posts, blog posts, tweets,\n\nlikes, fans, followers, replies, reviews, shares, time interval between posts across demographics\nparameters like location, age and gender.\n\nPROJECT IV: Library data weblog Analysis\nTools: R, Excel and SQL server.\nDuration: Aug 2015 - Nov 2015.\n\n Data preparation including data sanitization, missing value treatment and outlier treatment.\n\nAcademia:\n\nQUALIFICATION YEAR UNIVERSITY SPECILIZATION\n\nB.TECH 2015 VIGNAN E.C.E\n\nH.S.C 2011 AP Board MATHS\n\nS.S.C 2009 AP Board COMMON\nTextControl1","annotation":[{"label":["Degree"],"points":[{"start":5048,"end":5053,"text":"B.TECH"}]},{"label":["Skills"],"points":[{"start":4852,"end":4854,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":4839,"end":4839,"text":"R"}]},{"label":["Skills"],"points":[{"start":4383,"end":4383,"text":"R"}]},{"label":["Skills"],"points":[{"start":3964,"end":3966,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":3946,"end":3952,"text":"Tableau"}]},{"label":["Skills"],"points":[{"start":3939,"end":3944,"text":"Python"}]},{"label":["Skills"],"points":[{"start":3936,"end":3936,"text":"R"}]},{"label":["Skills"],"points":[{"start":3204,"end":3210,"text":"seaborn"}]},{"label":["Skills"],"points":[{"start":3176,"end":3185,"text":"Matplotlib"}]},{"label":["Skills"],"points":[{"start":3155,"end":3160,"text":"Pandas"}]},{"label":["Skills"],"points":[{"start":3148,"end":3152,"text":"NumPy"}]},{"label":["Skills"],"points":[{"start":3140,"end":3145,"text":"Python"}]},{"label":["Skills"],"points":[{"start":2840,"end":2845,"text":"Python"}]},{"label":["Skills"],"points":[{"start":2775,"end":2780,"text":"Python"}]},{"label":["Skills"],"points":[{"start":2714,"end":2719,"text":"Python"}]},{"label":["Skills"],"points":[{"start":2185,"end":2187,"text":"NLP"}]},{"label":["Skills"],"points":[{"start":2152,"end":2158,"text":"Tableau"}]},{"label":["Skills"],"points":[{"start":1534,"end":1539,"text":"Impala"}]},{"label":["Skills"],"points":[{"start":1525,"end":1528,"text":"Hive"}]},{"label":["Skills"],"points":[{"start":1520,"end":1522,"text":"pig"}]},{"label":["Skills"],"points":[{"start":1501,"end":1508,"text":"big data"}]},{"label":["Skills"],"points":[{"start":1474,"end":1481,"text":"Teradata"}]},{"label":["Skills"],"points":[{"start":1456,"end":1461,"text":"My SQL"}]},{"label":["Skills"],"points":[{"start":1433,"end":1435,"text":"SQL"}]},{"label":["Operating_Systems"],"points":[{"start":1376,"end":1379,"text":"UNIX"}]},{"label":["Operating_Systems"],"points":[{"start":1370,"end":1374,"text":"Linux"}]},{"label":["Skills"],"points":[{"start":1348,"end":1350,"text":"NLP"}]},{"label":["Skills"],"points":[{"start":1323,"end":1342,"text":"Predictive analytics"}]},{"label":["Skills"],"points":[{"start":1312,"end":1318,"text":"Tableau"}]},{"label":["Skills"],"points":[{"start":1301,"end":1306,"text":"Python"}]},{"label":["Skills"],"points":[{"start":1298,"end":1298,"text":"R"}]},{"label":["Skills"],"points":[{"start":1273,"end":1290,"text":"Data visualization"}]},{"label":["Skills"],"points":[{"start":1261,"end":1267,"text":"ggplot2"}]},{"label":["Skills"],"points":[{"start":1251,"end":1255,"text":"Dplyr"}]},{"label":["Skills"],"points":[{"start":1236,"end":1248,"text":"Random Forest"}]},{"label":["Skills"],"points":[{"start":1229,"end":1233,"text":"Caret"}]},{"label":["Skills"],"points":[{"start":1221,"end":1221,"text":"R"}]},{"label":["Skills"],"points":[{"start":1195,"end":1205,"text":"stat models"}]},{"label":["Skills"],"points":[{"start":1181,"end":1192,"text":"scikit learn"}]},{"label":["Skills"],"points":[{"start":1172,"end":1178,"text":"seaborn"}]},{"label":["Skills"],"points":[{"start":1160,"end":1169,"text":"Matplotlib"}]},{"label":["Skills"],"points":[{"start":1152,"end":1157,"text":"Pandas"}]},{"label":["Skills"],"points":[{"start":1145,"end":1149,"text":"NumPy"}]},{"label":["Skills"],"points":[{"start":1137,"end":1142,"text":"Python"}]},{"label":["Skills"],"points":[{"start":1070,"end":1093,"text":"deep learning algorithms"}]},{"label":["Skills"],"points":[{"start":1038,"end":1064,"text":"machine learning algorithms"}]},{"label":["Skills"],"points":[{"start":963,"end":982,"text":"Predictive analytics"}]},{"label":["Skills"],"points":[{"start":857,"end":869,"text":"Random Forest"}]},{"label":["Skills"],"points":[{"start":644,"end":646,"text":"NLP"}]},{"label":["Years_of_Experience"],"points":[{"start":402,"end":408,"text":"3 years"}]},{"label":["Email_Address"],"points":[{"start":58,"end":85,"text":"rajasekhardodda.ds@gmail.com"}]},{"label":["Mobile_No"],"points":[{"start":34,"end":47,"text":"+91-9160100636"}]},{"label":["Name"],"points":[{"start":0,"end":22,"text":"RAJA SEKHAR REDDY DODDA"}]}],"extras":null,"metadata":{"first_done_at":1564138961000,"last_updated_at":1564139499000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "Name DODDA RAJASEKHAR REDDY\t\n\t Senior Consultant\nPh :  9160100636\t\n\t\t\n\t\t\n\tEmail:\t rajasekhardodda.ds@gmail.com\n\nBackground\n3 + years of experience of Working as a senior consultant with Bit stat Technologies Pvt Ltd practice with strong domain knowledge in Banking and Telecom domain with the skills of machine learning, deep learning and predictive modelling.      \nHaving extensive working knowledge in customer life cycle and customer segmentation analysis\nGood working knowledge in NLP with python, performed sentimental analysis, text cleaning data with the help of NLTK, Text blob packages and draw the useful insights from text data.\nWorked on end to end business problem for telecom domain and provided recommendation to client , proactively suggested solutions for business trivial problems\nDid Engineering from VIGNAN University\n\nProfessional experience\n Senior consultant with 3 + years of experience in core data science, Analytics, Experience in Banking, credit cards, Telecom, Retail, Web analytics and Debt Collection Strategy models.\nI have been working on different Algorithms include Machine Learning techniques, NLP (Natural language processing), Linear & Logistic Regression analysis, Segmentation, Decisions trees, Cluster analysis and factor analysis, Natural language processing, Time Series Analysis, K-Means algorithm, Random Forests Algorithm, Sentimental analysis\nMajor Contribution Towards: MACHINE LEARNING Algorithms (Predictive analytics).\n\n\nProjects:\nVoice connectivity analysis for telecom industry Project:\nIt was a European telecom company project in which we must find out the major reasons behind the call drops.\nCustomer life cycle analysis by using the python with various machine learning algorithms such as Logistic regression, Decision trees and Random forest.\nDebt collection strategy analysis for never pay customers\nSkills\nData analysis Using various machine learning algorithms and deep learning algorithms\nProficient in tools for analysis using Python (NumPy, Pandas, Matplotlib, seaborn, scikit learn, stat models and NLTK).\nR Tool (Caret, Random Forest, Dplyr and ggplot2).\nData visualization using R, Python and Tableau.\nPredictive analytics and NLP.\nPlatforms like Linux/UNIX and Virtual Machine Environments.\nExperience with SQL and databases like My SQL server and Teradata.\nWell versed in big data tools are pig, Hive and Impala.\n\nWeb site views forecasting and website review analysis Project:\nForecasting the number of views to understand the website traffic.\nCleaning the semi structed and unstructured data by using the beautiful soap package in NLTK\nLoading the data from web log server files.\nWorked on sentimental analysis for website reviews for new launched products\nCalculating the KPI’s for social media data\nExtensively using python with NLTK packages.\n\n\nDigital Marketing Monthly BI Reports and analysis and forecasting for website views :\nWeblog data sanitization, missing value treatment, outlier treatment finally data audit.\nPreparation of BI reports that include creating tables, descriptive analysis & charts. Visualization of Key Performance Indicators. Control charts and alert system. Benchmark analysis & RAG status charts.\nAutomation of tables & reports generation using R& Excel macros\nData preparation including data sanitization, missing value treatment and outlier treatment. \nReporting of social media metrics like number of comments, mentions, posts, blog posts, tweets, likes, fans, followers, replies, reviews, shares, time interval between posts across demographics parameters like location, age and gender.\n\n\n\n\n\n\n\n\n‹#›","annotation":[{"label":["Skills"],"points":[{"start":2804,"end":2807,"text":"NLTK"}]},{"label":["Skills"],"points":[{"start":2604,"end":2607,"text":"NLTK"}]},{"label":["Tools"],"points":[{"start":2376,"end":2381,"text":"Impala"}]},{"label":["Tools"],"points":[{"start":2367,"end":2370,"text":"Hive"}]},{"label":["Tools"],"points":[{"start":2362,"end":2364,"text":"pig"}]},{"label":["Skills"],"points":[{"start":2343,"end":2350,"text":"big data"}]},{"label":["Skills"],"points":[{"start":2318,"end":2326,"text":"Teradata."}]},{"label":["Skills"],"points":[{"start":2300,"end":2305,"text":"My SQL"}]},{"label":["Skills"],"points":[{"start":2277,"end":2279,"text":"SQL"}]},{"label":["Operating_Systems"],"points":[{"start":2216,"end":2225,"text":"Linux/UNIX"}]},{"label":["Skills"],"points":[{"start":2196,"end":2198,"text":"NLP"}]},{"label":["Skills"],"points":[{"start":2171,"end":2190,"text":"Predictive analytics"}]},{"label":["Skills"],"points":[{"start":2162,"end":2168,"text":"Tableau"}]},{"label":["Skills"],"points":[{"start":2151,"end":2156,"text":"Python"}]},{"label":["Skills"],"points":[{"start":2148,"end":2148,"text":"R"}]},{"label":["Skills"],"points":[{"start":2080,"end":2120,"text":"(Caret, Random Forest, Dplyr and ggplot2)"}]},{"label":["Skills"],"points":[{"start":2066,"end":2069,"text":"NLTK"}]},{"label":["Skills"],"points":[{"start":2050,"end":2060,"text":"stat models"}]},{"label":["Skills"],"points":[{"start":2036,"end":2047,"text":"scikit learn"}]},{"label":["Skills"],"points":[{"start":2027,"end":2033,"text":"seaborn"}]},{"label":["Skills"],"points":[{"start":2015,"end":2024,"text":"Matplotlib"}]},{"label":["Skills"],"points":[{"start":2007,"end":2012,"text":"Pandas"}]},{"label":["Skills"],"points":[{"start":2000,"end":2004,"text":"NumPy"}]},{"label":["Skills"],"points":[{"start":1992,"end":1997,"text":"Python"}]},{"label":["Skills"],"points":[{"start":1928,"end":1951,"text":"deep learning algorithms"}]},{"label":["Skills"],"points":[{"start":1896,"end":1922,"text":"machine learning algorithms"}]},{"label":["Skills"],"points":[{"start":1712,"end":1738,"text":"machine learning algorithms"}]},{"label":["Skills"],"points":[{"start":1448,"end":1467,"text":"Predictive analytics"}]},{"label":["Skills"],"points":[{"start":1131,"end":1133,"text":"NLP"}]},{"label":["Years_of_Experience"],"points":[{"start":888,"end":896,"text":"3 + years"}]},{"label":["Skills"],"points":[{"start":571,"end":574,"text":"NLTK"}]},{"label":["Skills"],"points":[{"start":486,"end":488,"text":"NLP"}]},{"label":["Years_of_Experience"],"points":[{"start":123,"end":131,"text":"3 + years"}]},{"label":["Email_Address"],"points":[{"start":82,"end":109,"text":"rajasekhardodda.ds@gmail.com"}]},{"label":["Mobile_No"],"points":[{"start":55,"end":64,"text":"9160100636"}]},{"label":["Name"],"points":[{"start":5,"end":26,"text":"DODDA RAJASEKHAR REDDY"}]}],"extras":null,"metadata":{"first_done_at":1564130954000,"last_updated_at":1564130954000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "Rashmi Priya \t\t\t\t          \t\tTechnology: ETL.Informatica, Tableau, OBIEEE\nIBM India Pvt. Ltd.\t\t\t\t\t\t               Unix/ Linux, Oracle, SQl/Db2,\nBangalore, India.\t\t\t\t\t\t\t   Shell Scripting,Salesforce CRM\nMobile: +919036383755                        \t\t\t            Experience: 5 Years\nE-Mail: \trashmi.rashi1428@gmail.com\t\t\t           \tNotice Period: Immediate\n\n\nCAREER PRECIS\n\n· 5 Years of experience in S/W Development with ETL.Informatica 10.x,9.x, Unix, Linux, Oracle,SQl,DB2,Shell Scripting,Tableau, Informatica Cloud,OBIEE in Finance,Mortgage,Banking and Telecom Domains.\n· Worked on Incident Management Tools(Service Now,Remedy).\n· Proven Experience in  offshore models.\n· Worked in different sector have a Good Experience in Client interaction.\n· Currently working as Data Specialist at IBM India Pvt Ltd.\n· Dynamic, dedicated and accomplished S/W Developer in large-scale Application development and Project management skills.\n· Team player with a strong work ethic, committed to hard work.\n· Exposure in analyzing information system needs, evaluating end-user requirements, custom designing solutions, troubleshooting for complex information systems management and Application life cycle management. \n· Proficient in complete SDLC including software requirement analysis, designing, code generation, testing and support. \n· An effective communicator with exceptional relationship management and analytical skills. \n\n\nSCHOLASTICS\n\n\nBachelor in Techmolgy (Btech) 2009-2013 from GITA , Bhubaneshwar\nUniversity: BPUT,Rourkela \n\nSKILL SET\n\n\nFUNCTIONAL\n\n· Designing, developing, troubleshooting and debugging of the software.\n· Ensuring smooth implementation and testing.\n· Providing post-implementation, application maintenance and enhancement support with regard to the product / software application.\n· Handling the failure analysis, preventive, predictive and breakdown reports for minimizing downtimes by providing quick turnaround responses and efficient solutions.\n· Worked for Unix to Linux Domain Migration U2L,UTF8 Special Characters adjustment in the domain/server.\nTECHNICAL\n\n· Languages\t\t             : C++,C,SQL\n· Database\t\t\t: Oracle 9i/11G,Pl/SQL,DB2\n· Server\t\t\t: Windows 2003/2008, IIS 6, IIS 7.0\n· Tools\t\t\t: Informatica PowerCenter 9.x,10.x, Informatica Cloud\n· OS                                       : Unix,Linux,AIX\n· BI Tools                               : Tableau\n· Methodologies\t\t: Object Oriented Programming (OOP)\nUML/Design pattern,Waterfall Model, Agile/Scrum\nEMPLOYMENT CHRONICLE\n\n\n\t\n\tCompany\n\tDesignation \n\tDuration\n\n\t1.\n\tIBM India Pvt Ltd.\n\tData Specialist\n\tJan 2014 to till date\n\n\n\nPROJECT HANDLED\nIBM  India Pvt. Ltd. as Data Specialist\n\nDWP ,UK\t             \t\t\t\tMar 2018 to Currently working     \n(Department of Works&Pension)\n\nTeam size:\t\t2\nRole:\t\t\tData Specialist\nApplication: \t\tWeb Application, Middleware Interfaces, Windows Server\nTechnologies:\t\tInformatica Powercenter ,Unix, Linux, Cloud Informatica,DB2,Data Transformers\nDatabase:\t\tOBIEEE Tools, Oracle 11G ,SQL Server 2008\nMethodology\t\tDevelopment ,Application Maintainence & Support\nResponsibilities:\t\tWorked as Developer for development of masked data reports which will be used for analytics later by the analytics applications.Reports Generation, Workflow/Data monitoring, Impact Analysis and Technical        document,Designing, Developing, Bug fixing,Unit Test, System Test Support.\nWork based on SDLC methodologies.\n\nPHH Corp. Mortgage, USA\t             \t\t\t\tMar 2016 to Mar 2018     \nTeam size:\t\t5\nRole:\t\t\tData Specialist\nApplication: \t\tWeb Application, Middleware Interfaces, Windows Server\nTechnologies:\t\tC++,Oracle, Informatica Powercenter ,Unix, Linux, Cloud Informatica\nDatabase:\t\tOracle 11G ,SQL Server 2008\nMethodology\t\tApplication Maintaince Support ,Enhancement\nResponsibilities:\t\tReports Generation, Workflow/Data monitoring, Impact Analysis and Technical        document,Designing, Developing, Bug fixing,Unit Test, System Test Support\nDuetsche Bank ,Germany\t             \t\t\t\tJuly 2015 to Feb 2016    \nTeam size:\t\t4\nRole:\t\t\tSoftware Developer\nApplication: \t\tWeb Application, Middleware Interfaces, Windows Server\nTechnologies:\t\tC++,Oracle, Unix, Linux\t\t\t\nDatabase:\t\tOracle 11G,SQL Server 2008\nMethodology\t\tApplication Maintaince,Enhancement\nResponsibilities:\t\tImpact Analysis and Technical Design document,Server Maintainence\n\t\t\tDesigning, Developing, Bug fixing, Unit Test, System Test Support\nAT&T Texas, USA      \t\t\t\tMarch 2014 to Jun 2015   \nTeam size:\t\t8\nRole:\t\t\tSoftware Developer\nApplication: \t\tWeb Application, Middleware Interfaces, Windows Server\nTechnologies:\t\tC++,Oracle, Unix, Linux\t\t\t\nDatabase:\t\tOracle 11G, SQL Server 2008,PL/SQl\nMethodology\t\tAgile/Scrum\nResponsibilities:\t\tImpact Analysis and Technical Design document,Designing, Developing, Bug fixing,Unit Test\nDECLARATION\n\nI hereby declare that the information given above is true and correct to the best of my knowledge. I fully understand that in the event of any information being found false or incorrect, it will disqualify my candidature and I will myself be responsible.\nDate:  …………………………………….\t\t\t\t\t\t…………………………………………………………\nPlace: …………………………………….\t\t\t\t\t\t\t( Rashmi Priya )","annotation":[{"label":["Name"],"points":[{"start":5141,"end":5153,"text":"Rashmi Priya "}]},{"label":["Skills"],"points":[{"start":4653,"end":4655,"text":"SQl"}]},{"label":["Skills"],"points":[{"start":4634,"end":4636,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":4622,"end":4627,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":4602,"end":4606,"text":"Linux"}]},{"label":["Skills"],"points":[{"start":4596,"end":4599,"text":"Unix"}]},{"label":["Skills"],"points":[{"start":4588,"end":4593,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":4584,"end":4586,"text":"C++"}]},{"label":["Skills"],"points":[{"start":4189,"end":4191,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":4178,"end":4183,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":4158,"end":4162,"text":"Linux"}]},{"label":["Skills"],"points":[{"start":4152,"end":4155,"text":"Unix"}]},{"label":["Skills"],"points":[{"start":4144,"end":4149,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":4140,"end":4142,"text":"C++"}]},{"label":["Skills"],"points":[{"start":3699,"end":3701,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":3687,"end":3692,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":3664,"end":3674,"text":"Informatica"}]},{"label":["Skills"],"points":[{"start":3651,"end":3655,"text":"Linux"}]},{"label":["Skills"],"points":[{"start":3645,"end":3648,"text":"Unix"}]},{"label":["Skills"],"points":[{"start":3620,"end":3630,"text":"Informatica"}]},{"label":["Skills"],"points":[{"start":3612,"end":3617,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":3608,"end":3610,"text":"C++"}]},{"label":["Skills"],"points":[{"start":3001,"end":3003,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":2989,"end":2994,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":2975,"end":2980,"text":"OBIEEE"}]},{"label":["Skills"],"points":[{"start":2942,"end":2944,"text":"DB2"}]},{"label":["Skills"],"points":[{"start":2930,"end":2940,"text":"Informatica"}]},{"label":["Skills"],"points":[{"start":2917,"end":2921,"text":"Linux"}]},{"label":["Skills"],"points":[{"start":2911,"end":2914,"text":"Unix"}]},{"label":["Skills"],"points":[{"start":2886,"end":2896,"text":"Informatica"}]},{"label":["Skills"],"points":[{"start":2380,"end":2386,"text":"Tableau"}]},{"label":["Operating_Systems"],"points":[{"start":2333,"end":2335,"text":"AIX"}]},{"label":["Operating_Systems"],"points":[{"start":2327,"end":2331,"text":"Linux"}]},{"label":["Operating_Systems"],"points":[{"start":2322,"end":2325,"text":"Unix"}]},{"label":["Tools"],"points":[{"start":2259,"end":2275,"text":"Informatica Cloud"}]},{"label":["Tools"],"points":[{"start":2225,"end":2247,"text":"Informatica PowerCenter"}]},{"label":["Skills"],"points":[{"start":2162,"end":2164,"text":"DB2"}]},{"label":["Skills"],"points":[{"start":2155,"end":2160,"text":"Pl/SQL"}]},{"label":["Skills"],"points":[{"start":2141,"end":2146,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":2122,"end":2124,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":2120,"end":2120,"text":"C"}]},{"label":["Skills"],"points":[{"start":2116,"end":2118,"text":"C++"}]},{"label":["Skills"],"points":[{"start":1985,"end":1988,"text":"Unix"}]},{"label":["Degree"],"points":[{"start":1459,"end":1465,"text":"(Btech)"}]},{"label":["Skills"],"points":[{"start":501,"end":517,"text":"Informatica Cloud"}]},{"label":["Skills"],"points":[{"start":492,"end":498,"text":"Tableau"}]},{"label":["Skills"],"points":[{"start":476,"end":490,"text":"Shell Scripting"}]},{"label":["Skills"],"points":[{"start":472,"end":474,"text":"DB2"}]},{"label":["Skills"],"points":[{"start":468,"end":470,"text":"SQl"}]},{"label":["Skills"],"points":[{"start":461,"end":466,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":454,"end":458,"text":"Linux"}]},{"label":["Skills"],"points":[{"start":448,"end":451,"text":"Unix"}]},{"label":["Skills"],"points":[{"start":426,"end":436,"text":"Informatica"}]},{"label":["Skills"],"points":[{"start":422,"end":424,"text":"ETL"}]},{"label":["Years_of_Experience"],"points":[{"start":376,"end":382,"text":"5 Years"}]},{"label":["Email_Address"],"points":[{"start":291,"end":316,"text":"rashmi.rashi1428@gmail.com"}]},{"label":["Years_of_Experience"],"points":[{"start":274,"end":280,"text":"5 Years"}]},{"label":["Mobile_No"],"points":[{"start":210,"end":222,"text":"+919036383755"}]},{"label":["Skills"],"points":[{"start":187,"end":200,"text":"Salesforce CRM"}]},{"label":["Skills"],"points":[{"start":171,"end":185,"text":"Shell Scripting"}]},{"label":["Skills"],"points":[{"start":139,"end":141,"text":"Db2"}]},{"label":["Skills"],"points":[{"start":134,"end":137,"text":" SQl"}]},{"label":["Skills"],"points":[{"start":127,"end":132,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":120,"end":124,"text":"Linux"}]},{"label":["Skills"],"points":[{"start":114,"end":117,"text":"Unix"}]},{"label":["Skills"],"points":[{"start":67,"end":72,"text":"OBIEEE"}]},{"label":["Skills"],"points":[{"start":58,"end":64,"text":"Tableau"}]},{"label":["Skills"],"points":[{"start":45,"end":55,"text":"Informatica"}]},{"label":["Skills"],"points":[{"start":41,"end":43,"text":"ETL"}]},{"label":["Name"],"points":[{"start":0,"end":12,"text":"Rashmi Priya "}]}],"extras":null,"metadata":{"first_done_at":1564131424000,"last_updated_at":1564131424000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "Skills\n\tYears of Experience\n\tProject Name\n\tDuration\n\n\tJava , core java \n\t5 years\n\tUNIFIED PREFCOMM\nNEW TEST ASSIGNMENT\nCoRDSPATIENT ENROLLMENT SYSTEM\neConsenting\nUNISOFT TECHNOLOGIES WEB SITE\n\t2014 - 2019\n\n\tSpring boot \n\t1.5 years\n\tUNIFIED PREFCOMM\n\t2018 - 2019\n\n\tMicro service \n\t1.5 years \n\tUNIFIED PREFCOMM\n\t2018 - 2019\n\n\tGIT,SVN \n\t1 years\n\tUNIFIED PREFCOMM\n\t2018 - 2019\n\n\tUniti testing : JUnit / Mokito \n\t1.5 Years\n\tUNIFIED PREFCOMM\nNEW TEST ASSIGNMENT\n\t2017 - 2019\n\n\tRestfull Web services\n\t1 Years\n\tUNIFIED PREFCOMM\n\t2018 - 2019\n\n\tMultithreading\n\t1 yrs\n\tUNIFIED PREFCOMM\n\t2018 - 2019\n\n\t\n\t\n\t\n\t\n\n\n\n\n\tCURRICULAM-VITAE\n\n\n\t\n\nRishi Dhar\n15/2Sewak Ashram Road,\nDehradun,\nUttarakhand.\nrishidhar975@gmail.com\nContact:9149128796\n\n\n\t\tCareer Objective\n\t\n\n\n\n\nTo secure a responsible and a remarkable position in the area of my profession by utilizing hard work determination and experience.\n\n\n\n\tExamination\n\tArea\n\tInstitution\n\tBoard / University\n\tYear Of Passing\n\n\tMCA \n\n\tComputer Application\n\tU.I.M. DEHRADUN\n\tUK TECHNICAL UNIVERSITY\n\t2013\n\n\tBSC(I.T.)\n\tComputer Application and Information Technology\n\tD.A.V. P.G COLLEGE\nDEHRADUN\n\tH.N.B. GARHWAL UNIVERSITY\n\t2010\n\n\t12th\n\tPCM\n\tTOUCH WOOD SCHOOL DEHRADUN\n\tI.S.E.\n\t2006\n\n\t10th\n\tPCM\n\tTOUCH WOOD SCHOOL DEHRADUN\n\tI.C.S.E\n\t2004\n\n\n\n\n\t\tComputing Skill Details\n\t\n\n\n\n\n\tArea\n\tSkills Competent\n\n\tProgramming Languages:\n\t\nCORE JAVA,SERVLET , JSP,JQUERY , JAVASRIPT , HTML,AJAX,\n\nSTRUTS 2X,HIBERNATES 3.0,RESTFUL WEB SERVICES,JFREECHART,\n\nJASPERREPORTS, MAVEN , SPRING CORE , SPRING MVC , \n\nSPRING SECURITY , SPRING BOOT , JUNIT , SWAGGER , KAFKA \n\nMESSAGING, MICROSERVICES\n\n\n\tDatabase:               \n\t\nMYSQL 5 , SQLSERVER 2012 , ORACLE 11g\n\n\n\tOperating Platforms:\n\t\nWINDOWS8,7,Xp\n\n\n\tWeb Servers:\n\t\nJBOSS 7 , JBOSS 6, APACHETOMCAT , GLASSFISH EE\n\n\n\tSoftware Development Methodology \n\tAgile Scrum\n\n\n\n\n\t\tCertifications\n\t\n\n\n\n\n\tOracle Certified Professional Java Programmer\n\tOracle Certified Professional for Java Standard Edition 6\n\n\n\n\tOracle Certified Web Component Developer \n\tOracle Certified Web Component Developer For Java Enterprise Edition 6\n\n\n\n\n\n\n\n\n\t\tExperience\n\t\n\n\n\n\n\tTotal Experience\n\t4.11 Years\n\n\tBayaTreeInfocom Pvt. Ltd. Mohali Punjab\n2.7 Years\n\t2.7years of working experience in Struts 2 and Hibernates 3.0 framework in BAYATREE INFOCOM PVT LTD located in industrial area MOHALI ,PUNJAB.\n\n\n\tBureau Veritas Consumer Product Services Ltd.\n1 Year\n\t1 year of working experience in Spring CORE, Spring MVC and JDBC templates in BUREAU VERITAS located in NOIDA Sector -6 ,UP.\n\n\tXavient Information Systems\nFeb 2018 – Present\n1 Year\n\tCurrently working in Xavient Information Systems located in NSEZ, Noida Sector - 82\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\t\tProject worked on:\n\t\n\n\n\n\n\tProject Name:\n\tUNIFIED PREFCOMM\n\n\tProject Description:\n\tUnified Prefcomm is a telecommunication based project which is basically used for setting the preferences of user(EMAIL,SMS,IVR) into the database for a particular user contact, and also performs different  DML operations on contacts and preferences.\nThis project is basically built in spring boot and restful web services with kafka messaging channel.\n\n\tProject Duration:\n\tFeb 2018 till current\n\n\tBuild Environment:\n\tSpring Boot, Restful Web Services, Spring Security, Swagger-UI, Kafka Messaging, Microservices, Oracle Database\n\n\tProject Name:\n\tNEW TEST ASSIGNMENT (NEWTA)\n\n\tProject Description:\n\tEstablished in 1820, Bureau Veritas is an organization which basically works on testing and certification of numerous products and its branches worldwide.Some of the reputed clients of BV are WALMART,HITACHI,SONY etc.\n\n‘Test’ in context of NEW TEST ASSIGNMENT is a procedure to test a product in labs of BV.\nNew Test assignment is a project which is basically used for the assignment of tests in the submission which acts as a unique identification number for any product which is to be tested in the labs of Bureau Veritas.\n\nApart from assigning test lines project has several other functionalities like assigning protocols,internal comments,multi applicabilities and many other essential details which are to be associated with testlines in general and are created by the TPD system before handling it to NEWTA.\n\nAfter the assignment of tests the sample has to go through several other stages and finally the resulting of the submission by the lab as weather the tests assigned in the submission has passed or not, and based on the overall rating of the submission, a pdf or word report for the client is generated by the ARG(Automatic Report Generation) system.\n\n\tProject Duration:\n\t1 Year\n\n\tBuild Environment:\n\n\t\nMaven project with Spring Web and Spring JDBC framework version 3.1\n\nDatabases – Oracle 11g\n\nOther Technologies Used (Client Side) – Promise API ,Local Forage,\nIndexed DB,HTM5 Session Storage,JQWidgets,JavaScript,Jquery\n\n\tProject Name:\n\tCoRDSPATIENT ENROLLMENT SYSTEM\n\n\tProject Description:\n\tThis project is about Cords acronym for Coordination Of Rare Disease At Sanford is a web application that collects information about rare diseases from patients for further research on that particular disease at Sanford Health which is a clinical health research organization located at New York and its branches worldwide. \n\nIncluding registration of patients there are many other features on the project like the security module which consists of complete configurable functionality like assigning roles and partnerships to users, changing settings of users and customizing application settings,themes, dynamic form designing for patients through drag and drop feature, automated email notifications for enrollment status, auditing of data, multiple database support , Adhocquery reporting tools and much more.\n\nThe web app consists of three individual portals as per authority :\n\nURL for Screening Form –https://cordsconnect.sanfordresearch.org/BayaPES/sf/\n\nURL for Participant Portal – \nhttps://cordsconnect.sanfordresearch.org/BayaPES/pp/\n\nURL for Admin Portal – \nhttps://cordsconnect.sanfordresearch.org/BayaPES/login/\n\n\n\tProject Duration:\n\t2.7Years\n\n\tBuild Environment:\n\n\n\t\nMaven Project in Struts2 And Hibernates 3.0 framework.\n\nDatabases- MySql 5.7,SqlServer 2012\n\nOther Technologies Used – Jasper reports , JFreeChart , Tiles , Bootstrap, Quartz Scheduler\n\n\n\tProject Name:\n\teConsenting\n\n\tProject Description:\n\tThis project is an electronic consenting application which basically collects consenting data in electronic format from the patients before any surgery or medical procedure.\nThis application helps the hospitals management from mundane task ofcollecting information from patients in easy and more user interactive way. \n\nThe application UI is build in Android , Objective C and Struts2 Web Appand Web Appversion is only used for remote consenting.\nThis application displays dynamic work flow data entered through its corresponding web portal to its users through videos , pdf, text , sound.\n\nThe use of various multimedia techniques allows doctors to effectively and efficiently collect data from patients who are physically disabled or handicapped.\nThe forms for different category of patients can be configured and displayed throughAndroid / iPhone/ Web application.\n\n\tProject Duration:\n\t1.0 Years\n\n\tBuilt Environment:\n\t\nAndroid , IOS , Struts2 \n\nDatabase –Sqllite for android ,SqlServer 2012\n\nRestful WEB Services for data transfer between different platforms.\n\n\n\tProject Name:\n\tUNISOFT TECHNOLOGIES WEB SITE\n\n\tProject Description:\n\tAn Information Portal for students who are interested in learning the emerging computer languages such as JAVA, .NET,ANDROID,PHP and the site provides much more services to viewers.\n\nDomain:http://www.unisoftdehradun.com/\n\n\tProject Duration\n\t\n6 Months Job Cum Training in Unisoft Technologies,Dehradun\n\n\tBuilt Environment:\n\t\nServer Programming  -J2EEServlet, Jsp\n\nFrontend  - Ajax, Dhtml, Jquery, Css\n\nDatabase – MySql\n\nBased upon MVC design pattern.\n\n\n\tProject Name:\n\tCHAT MESSENGER\n\n\tProject Description:\n\tA basic client server chat messenger allows a person to communicate his message to a different party in textual format.\nWorks basically on Lan where computers are connected through cross cables. Have features of both Unicasting and Broadcasting of message.\n\n\tBuilt Environment:\n\t\nJAVA- Network Programming/Socket Programming. \n\nDesktop Based Application.\n\n\n\n\n\n\n\n\n\n\t\tPersonal Details \n\t\n\n\n\n\n\tName :\n\tRishi Dhar\n\n\tDate of Birth:\n\tOctober 30, 1988\n\n\tAge:\n\t27\n\n\tGender :\n\tMale\n\n\tNationality:\n\tIndian\n\n\tCountry :\n\tIndia\n\n\tPermanent Address:\n\t15/2,Sewak Ashram Road,Dehradun\n\n\tState :\n\tUttarakhand\n\n\tCity :\n\tDehradun\n\n\tContact No:\n\t9149128796\n\n\tLanguages Known :\n\tHindi,English\n\n\tE-Mail Id :\n\trishidhar975@gmail.com\n\n\tLinked In Profile : \n\thttps://in.linkedin.com/in/rishi-dhar-9b855890\n\n\t\n\n\t\n\n\n\n\t\tOther Details\n\t\n\n\n\n\n\tHobbies :\n\n\t\nListening to the music,  playing PC Games\n\n\n\n\tAchievement, Awards received (Academic & Community) :\n\n\t· Took part in C.S.I. quiz and won award.\n· Take part and won many inter school competition.\n\n\n\n\t\n\n\t\n\n\t\t\n\n\n\n\n\t\tDeclaration\n\t\n\n\n\n\nI hereby declare that the information given above is true and to the best of my knowledge.\n\nSignature: Rishi Dhar\nPlace: Delhi","annotation":null,"extras":null,"metadata":{"first_done_at":1564243385000,"last_updated_at":1564293417000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "R.Krishnaveni\nMobile :9092116379\nE-Mail: krishnaraju03jan@gmail.com\nLinkedIn: linkedin.com/in/krishna-raju-124bbb167\n\n\nProfessional Summary\n\nKrishnaveni  Raju  has 8+ years of professional experience in  the IT industry  which includes 3+ yrs of Bigdata experience as a senior data analyst at UST Global.\n\n· Worked as independent Bigdata  engineer\n· Good experience in Pyspark\n· Experienced on Datamining, Data analystics, Testing\n· Worked in Yarn application\n· Experienced on Aws with PySpark.\n· Working experience on S3,Crawler,Data catalog\n· Experienced in AWS Glue PySpark Transforms for use in PySpark ETL operations\n· Working experience on DynamicFrameCollection Class,DynamicFrame Class\n· Writing the Schemas in the Data Catalog\n· Crawl the Data in the Amazon S3 Bucket\n· Worked on , Map the Data and Use Apache Spark Lambda Functions\n· Experienced in using Job Bookmarks for generating AWS scripts\n· experienced in job monitoring and debugging\n· moving the data to and from  Amazon Redshift.\n· Experience in unit testing for testing the  script\n· working with the tools in Visual Studio, Atom, Eclipse, Kafka Tool.\n· Used Hortonworks, cloudera distribution management systems\n· Developed the script for Data generation File.\n· Developed the programming in Spark streaming for Transforming the data.\n· Using REST API Services\n· Worked on data analytics tool like Elastic search\n· Experience on Storm with Kafka.\n· Executing Analytics, Dashboard and Reporting solutions like Kibana.\n· Hands on experience with Hadoop, Hive,Sql.\n· Having good hands on experience in Data Warehousing such as ETL for Data Ingestion\n· Experience with techniques of performance optimization for both data loading and data retrieval\n· Good knowledge in SQL queries.\n· Ability to deploy and maintain multi-node Hadoop cluster\n· Experience of data ingestion tools like Sqoop\n· Experience with Spark Streaming, Apache NiFi and Kafka for real-time data processing\n· Participated in Designing the Architecture of the project\n· Work with cross functional teams excellently\n· Done Enhancements to  production jobs in order to meet the customer requirements\n· Manage and Review data backups and log files\n· Developed Hadoop Migration Solution approach\n· Finished ITIL certification\n\n\n\n\n\nRoles Played in earlier/current Projects:\n· Design and Development: Have experience as a good Developer and Analyst with good interaction skills to convert the requirement into solutions.\n· Maintenance and Support Analyst:  Worked as Application technical support analyst in analyzing issues and providing permanent fixes or short term workarounds.\n\n\nEmployment summary\nOrganization\t\t\tDesignation\t\t\t\tDuration\n\nUST Global\t\t\tSenior Data analyst\t\t\t2018-Till\nHP Global Soft Pvt Ltd\t\tSenior Data Engineer\t\t\t2014-2018\nSI Systems\t\t\tSenior Software Engineer\t\t2009-2014\n\n\n\n\n\nTechnical Skills\n\tHadoop Management \n\tCloudera ,Hortonworks\n\n\tHadoopEco System\n\tHadoop, HDFS, Sqoop \n\n\tDistributed Processing,Security tools.\n\tSpark, Storm,Hive,Kerbros\n\n\tNOSQL\n\tHbase\n\n\tData Visualization Tools,Data analytics tool\n\tKibana, Zepplin, AWS, S3, Redshift\n\n\tProgramming\n\tScala, Java, PL/SQL\n\n\tSupporting Tools\n\tElasticSearch, Logstash\n\n\tRDBMS\n\tOracle, SQL Server  and Mysql, Kafka\n\n\tOperating system\n\tPython, Linux, Centos, Windows\n\n\tDevelopment Tools\n\tVisual Studio, Eclipse, Intellija, Atom, Scala, Toad, PL/SQL Developer,SQL Developer\n\n\n\n\n\n\n\n\n\n\n\n\n\nPROJECT #\nTitle\t\t:\tSAIE\nClient\t\t:\tDell\nPeriod\t\t:\tSince Sep’2018 Till\nRole\t\t:\tSenior Data Analyst\nEnvironeent\t:\tHadoop, Spark,Java,Kafka,Hive,Hbase,Zipplin,Storm,Informatica,Oracle,\n\t\t\tAws,Elasticsearch,Kibana,Tableau\n\n\nKey contribution\n\n· working with the tools in VisualStudio, Eclipse,Kafka Tool\n· Experience in Pyspark\n· Experience with Spark coding for Ingestion. \n· Developed the programming in Spark streaming for Transforming the data.\n· Experienced in Nifi,Oozie\n· Experience in Hive,Hbase.\n· Worked Kerbros for security.\n· Experienced in AWS Glue PySpark Transforms for use in PySpark ETL operations\n· Working experience on DynamicFrameCollection Class,DynamicFrame Class\n· Writing the Schemas in the Data Catalog\n· Crawl the Data in the Amazon S3 Bucket\n· Worked on , Map the Data and Use Apache Spark Lambda Functions\n· Experienced in using Job Bookmarks for generating AWS scripts\n· experienced in job monitoring and debugging\n· moving the data to and from  Amazon Redshift.\n· Experience in unit testing for testing the  script\n· Created the index, Data loading in Elastic search\n· Having good experience in Storm with Kafka.\n· Experience with Spark with AWS cluster.\n· Hands on experience with Hadoop, Hive , Sql.\n· Experience with techniques of performance optimization for both data loading and data retrieval\n· Moving data from HDFS to Oracle and vice-versa using REST API\n· Developed Storm workflow for scheduling and orchestrating the ETL process\n· Executing Analytics, Dashboard and Reporting solutions \n\n\n\n\n\nPROJECT #\nTitle\t\t:\tCanon\nClient\t\t:\tCanon\nPeriod\t\t:\t2014-2018 \nRole\t\t:\tSenior Data Engineer\nEnvironeent\t:\tHadoop, Spark,Scala,Hive,Hbase,Sqoop,Nifi,Oracle,\n\t\t\tElasticsearch,Kibana,Tableau,Kafka\n\n\n\nKey Contribution\n· Developed  Hadoop Migration Solution approach\n· working with the tools in Hadoop Ecosystem including  Hive, HDFS, Sqoop, Kafka\n· Experience in in Spark using Scala.\n· Experience with Spark Streaming, Apache NiFi and Kafka for real-time data processing\n· Developed the programming in Spark streaming for Transforming the data.\n· Develop the programming in Scala, Python.\n· Created the index, Data loading in Elastic search\n· Using Logstash to store the log files and retrieve the log files\n· Hands on experience with Hadoop, Hive, Sql.\n· Experience with techniques of performance optimization for both data loading and data retrieval\n· Moving data from Oracle into  HDFS  using Sqoop\n· Created and worked Sqoop jobs with incremental load to populate Hive External tables.\n· Very good experience in monitoring and managing the Hadoop cluster using Cloudera Manager\n· Executing Analytics, Dashboard and Reporting solutions \n· Business operations report are been generated for end users using Kibana\n· Importing and Exporting data into Hdfs using Sqoop, NiFi.\n· Participated in Designing the Architecture of the project\n\nPROJECT #\nTitle\t\t:\tSymantec\nClient\t\t:\tSymantec\nPeriod\t\t:\t2014-2015 \nRole\t\t:\tSenior Application Engineer\nEnvironeent\t:\tOracle,Java, Ebs,Icon, AppInterface,Appmigration,OpenText\n\n\nKey Contribution\n· Experience in developing extensions, upgrades and implementation Oracle E-Business Suite (11i and R12)\n· Strong expertise in PL/SQL developing - Packages, Stored Procedures and Functions, Database Triggers\n· Experience in developing, customization and extend Java, Host and PL\\SQL Concurrent programs\n· Hands-on experience with Oracle Application Framework, Oracle\n· Experience with supporting Month End Close and Year-End Close activities\n\n\nPROJECT #\nTitle\t\t:\tProcurement Systems\nClient\t\t:\tAMD Services\nPeriod\t\t:\t2012-2014 at SI Systems\nRole\t\t:\tSenior Software Engineer\nEnvironeent\t:\tOracle,Java,Sql Developer, Toad\n\n\nKey Contribution\n· Analyzing existing code for necessary changes\n· Incorporating new changes in existing packages and writing Triggers.\n· Responsible for SQL tuning and security\n· Creating external table to load data from CSV file\n· Extracting data through ETL and stored procedures from CSV file to Oracle Database\n· Extending Level 2 & 3 supports for the application in resolving day-to-day issues Working on night support for daily maintenance of the applications\n· Executing performance tuning while incorporating indexes, full table scan, temporary tables and parallel wherever required to improve performance\n· Examining and implementing change requests\n· Participating in bug fixing and peer to peer code review activities.\n· Handling implementation of security of data according to client's requirements by creating views.Analysis of the customer’s current business process\n\n\n\nPROJECT #\nTitle\t\t:\tPrice Tracking Systems\nClient\t\t:\tBanCorp, US\nPeriod\t\t:\t2011-2012 at SI Systems\nRole\t\t:\tSenior  Engineer\nEnvironeent\t:\tOracle,Java,Sql Developer, Toad\n\n\nKey Contribution\n· Serving as an Developer\n· Analyzing existing code for necessary changes\n· Writing PL/SQL package & procedures\n· Creating external table to load data from CSV file\n· Extracting data through ETL and stored procedures from CSV file to Oracle Database\n· Extending Level 2 & 3 supports for the application in resolving day-to-day issues \n· Working on night support for daily maintenance of the applications\n· Executing performance tuning while incorporating indexes, full table scan, temporary tables and parallel wherever required to improve performance\n· Examining and implementing change requests\n· Participating in bug fixing and peer to peer code review activities\n· Incorporating new changes in existing packages\n· Developing PL/SQL packages with error handling capabilities\n\nEducation\nMS (Information Technology) from Madurai kamaraj university,Madurai. \nB.Sc. (Computers) from Madurai Kamaraj University,","annotation":[{"label":["Degree"],"points":[{"start":8929,"end":8955,"text":"MS (Information Technology)"}]},{"label":["Skills"],"points":[{"start":8869,"end":8874,"text":"PL/SQL"}]},{"label":["Skills"],"points":[{"start":8374,"end":8379,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":8224,"end":8229,"text":"PL/SQL"}]},{"label":["Tools"],"points":[{"start":8116,"end":8119,"text":"Toad"}]},{"label":["Skills"],"points":[{"start":8096,"end":8099,"text":"Java"}]},{"label":["Skills"],"points":[{"start":8089,"end":8094,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":7367,"end":7372,"text":"Oracle"}]},{"label":["Tools"],"points":[{"start":7060,"end":7063,"text":"Toad"}]},{"label":["Skills"],"points":[{"start":7040,"end":7043,"text":"Java"}]},{"label":["Skills"],"points":[{"start":7033,"end":7038,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":6806,"end":6811,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":6776,"end":6781,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":6707,"end":6710,"text":"Java"}]},{"label":["Skills"],"points":[{"start":6573,"end":6578,"text":"PL/SQL"}]},{"label":["Skills"],"points":[{"start":6513,"end":6518,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":6376,"end":6379,"text":"Java"}]},{"label":["Skills"],"points":[{"start":6369,"end":6374,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":6177,"end":6181,"text":"Sqoop"}]},{"label":["Tools"],"points":[{"start":6123,"end":6128,"text":"Kibana"}]},{"label":["Skills"],"points":[{"start":5883,"end":5886,"text":"Hive"}]},{"label":["Skills"],"points":[{"start":5838,"end":5842,"text":"Sqoop"}]},{"label":["Skills"],"points":[{"start":5811,"end":5815,"text":"Sqoop"}]},{"label":["Skills"],"points":[{"start":5799,"end":5802,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":5786,"end":5791,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":5658,"end":5661,"text":"Hive"}]},{"label":["Tools"],"points":[{"start":5564,"end":5571,"text":"Logstash"}]},{"label":["Skills"],"points":[{"start":5489,"end":5493,"text":"Scala"}]},{"label":["Skills"],"points":[{"start":5417,"end":5421,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":5350,"end":5354,"text":"Kafka"}]},{"label":["Skills"],"points":[{"start":5317,"end":5321,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":5292,"end":5296,"text":"Scala"}]},{"label":["Skills"],"points":[{"start":5280,"end":5284,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":5255,"end":5259,"text":"Kafka"}]},{"label":["Skills"],"points":[{"start":5248,"end":5252,"text":"Sqoop"}]},{"label":["Skills"],"points":[{"start":5242,"end":5245,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":5236,"end":5239,"text":"Hive"}]},{"label":["Skills"],"points":[{"start":5106,"end":5110,"text":"Kafka"}]},{"label":["Tools"],"points":[{"start":5091,"end":5096,"text":"Kibana"}]},{"label":["Skills"],"points":[{"start":5066,"end":5071,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":5055,"end":5059,"text":"Sqoop"}]},{"label":["Skills"],"points":[{"start":5049,"end":5053,"text":"Hbase"}]},{"label":["Skills"],"points":[{"start":5044,"end":5047,"text":"Hive"}]},{"label":["Skills"],"points":[{"start":5038,"end":5042,"text":"Scala"}]},{"label":["Skills"],"points":[{"start":5032,"end":5036,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":4792,"end":4796,"text":"Storm"}]},{"label":["Skills"],"points":[{"start":4743,"end":4748,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":4735,"end":4738,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":4606,"end":4609,"text":"Hive"}]},{"label":["Tools"],"points":[{"start":4558,"end":4560,"text":"AWS"}]},{"label":["Skills"],"points":[{"start":4547,"end":4551,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":4522,"end":4526,"text":"Kafka"}]},{"label":["Skills"],"points":[{"start":4511,"end":4515,"text":"Storm"}]},{"label":["Tools"],"points":[{"start":4368,"end":4375,"text":"Redshift"}]},{"label":["Tools"],"points":[{"start":4272,"end":4274,"text":"AWS"}]},{"label":["Skills"],"points":[{"start":4197,"end":4201,"text":"Spark"}]},{"label":["Tools"],"points":[{"start":4144,"end":4146,"text":" S3"}]},{"label":["Skills"],"points":[{"start":3979,"end":3983,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":3949,"end":3953,"text":"Spark"}]},{"label":["Tools"],"points":[{"start":3938,"end":3940,"text":"AWS"}]},{"label":["Skills"],"points":[{"start":3899,"end":3905,"text":"Kerbros"}]},{"label":["Skills"],"points":[{"start":3883,"end":3887,"text":"Hbase"}]},{"label":["Skills"],"points":[{"start":3878,"end":3881,"text":"Hive"}]},{"label":["Skills"],"points":[{"start":3791,"end":3795,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":3731,"end":3735,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":3678,"end":3682,"text":"Kafka"}]},{"label":["Tools"],"points":[{"start":3670,"end":3676,"text":"Eclipse"}]},{"label":["Tools"],"points":[{"start":3593,"end":3598,"text":"Kibana"}]},{"label":["Skills"],"points":[{"start":3564,"end":3569,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":3546,"end":3550,"text":"Storm"}]},{"label":["Skills"],"points":[{"start":3532,"end":3536,"text":"Hbase"}]},{"label":["Skills"],"points":[{"start":3527,"end":3530,"text":"Hive"}]},{"label":["Skills"],"points":[{"start":3521,"end":3525,"text":"Kafka"}]},{"label":["Skills"],"points":[{"start":3516,"end":3519,"text":"Java"}]},{"label":["Skills"],"points":[{"start":3510,"end":3514,"text":"Spark"}]},{"label":["Tools"],"points":[{"start":3364,"end":3376,"text":"SQL Developer"}]},{"label":["Tools"],"points":[{"start":3347,"end":3362,"text":"PL/SQL Developer"}]},{"label":["Tools"],"points":[{"start":3341,"end":3344,"text":"Toad"}]},{"label":["Skills"],"points":[{"start":3334,"end":3338,"text":"Scala"}]},{"label":["Tools"],"points":[{"start":3328,"end":3331,"text":"Atom"}]},{"label":["Tools"],"points":[{"start":3317,"end":3325,"text":"Intellija"}]},{"label":["Tools"],"points":[{"start":3308,"end":3314,"text":"Eclipse"}]},{"label":["Tools"],"points":[{"start":3293,"end":3305,"text":"Visual Studio"}]},{"label":["Operating_Systems"],"points":[{"start":3264,"end":3270,"text":"Windows"}]},{"label":["Operating_Systems"],"points":[{"start":3256,"end":3261,"text":"Centos"}]},{"label":["Operating_Systems"],"points":[{"start":3249,"end":3253,"text":"Linux"}]},{"label":["Skills"],"points":[{"start":3215,"end":3219,"text":"Kafka"}]},{"label":["Skills"],"points":[{"start":3208,"end":3212,"text":"Mysql"}]},{"label":["Skills"],"points":[{"start":3192,"end":3201,"text":"SQL Server"}]},{"label":["Skills"],"points":[{"start":3184,"end":3189,"text":"Oracle"}]},{"label":["Tools"],"points":[{"start":3166,"end":3173,"text":"Logstash"}]},{"label":["Tools"],"points":[{"start":3151,"end":3163,"text":"ElasticSearch"}]},{"label":["Skills"],"points":[{"start":3124,"end":3129,"text":"PL/SQL"}]},{"label":["Skills"],"points":[{"start":3118,"end":3121,"text":"Java"}]},{"label":["Skills"],"points":[{"start":3111,"end":3115,"text":"Scala"}]},{"label":["Tools"],"points":[{"start":3087,"end":3094,"text":"Redshift"}]},{"label":["Tools"],"points":[{"start":3082,"end":3084,"text":" S3"}]},{"label":["Tools"],"points":[{"start":3078,"end":3080,"text":"AWS"}]},{"label":["Tools"],"points":[{"start":3069,"end":3075,"text":"Zepplin"}]},{"label":["Tools"],"points":[{"start":3061,"end":3066,"text":"Kibana"}]},{"label":["Skills"],"points":[{"start":3007,"end":3011,"text":"Hbase"}]},{"label":["Skills"],"points":[{"start":3000,"end":3004,"text":"NOSQL"}]},{"label":["Skills"],"points":[{"start":2990,"end":2996,"text":"Kerbros"}]},{"label":["Skills"],"points":[{"start":2985,"end":2988,"text":"Hive"}]},{"label":["Skills"],"points":[{"start":2979,"end":2983,"text":"Storm"}]},{"label":["Skills"],"points":[{"start":2972,"end":2976,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":2955,"end":2968,"text":"Security tools"}]},{"label":["Skills"],"points":[{"start":2932,"end":2953,"text":"Distributed Processing"}]},{"label":["Skills"],"points":[{"start":2923,"end":2927,"text":"Sqoop"}]},{"label":["Skills"],"points":[{"start":2917,"end":2920,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":2891,"end":2914,"text":"HadoopEco System\n\tHadoop"}]},{"label":["Skills"],"points":[{"start":2877,"end":2887,"text":"Hortonworks"}]},{"label":["Skills"],"points":[{"start":2847,"end":2874,"text":"Hadoop Management \n\tCloudera"}]},{"label":["Skills"],"points":[{"start":1908,"end":1912,"text":"Kafka"}]},{"label":["Skills"],"points":[{"start":1875,"end":1879,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":1851,"end":1855,"text":"Sqoop"}]},{"label":["Skills"],"points":[{"start":1524,"end":1527,"text":"Hive"}]},{"label":["Tools"],"points":[{"start":1481,"end":1486,"text":"Kibana"}]},{"label":["Skills"],"points":[{"start":1412,"end":1416,"text":"Kafka"}]},{"label":["Skills"],"points":[{"start":1401,"end":1405,"text":"Storm"}]},{"label":["Skills"],"points":[{"start":1264,"end":1268,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":1130,"end":1140,"text":"Hortonworks"}]},{"label":["Skills"],"points":[{"start":1111,"end":1115,"text":"Kafka"}]},{"label":["Tools"],"points":[{"start":1102,"end":1108,"text":"Eclipse"}]},{"label":["Tools"],"points":[{"start":1096,"end":1099,"text":"Atom"}]},{"label":["Tools"],"points":[{"start":1081,"end":1093,"text":"Visual Studio"}]},{"label":["Tools"],"points":[{"start":990,"end":997,"text":"Redshift"}]},{"label":["Tools"],"points":[{"start":894,"end":896,"text":"AWS"}]},{"label":["Skills"],"points":[{"start":819,"end":823,"text":"Spark"}]},{"label":["Tools"],"points":[{"start":766,"end":768,"text":" S3"}]},{"label":["Skills"],"points":[{"start":601,"end":605,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":571,"end":575,"text":"Spark"}]},{"label":["Tools"],"points":[{"start":560,"end":562,"text":"AWS"}]},{"label":["Tools"],"points":[{"start":518,"end":520,"text":" S3"}]},{"label":["Skills"],"points":[{"start":488,"end":492,"text":"Spark"}]},{"label":["Years_of_Experience"],"points":[{"start":164,"end":171,"text":"8+ years"}]},{"label":["Email_Address"],"points":[{"start":41,"end":66,"text":"krishnaraju03jan@gmail.com"}]},{"label":["Mobile_No"],"points":[{"start":22,"end":31,"text":"9092116379"}]},{"label":["Name"],"points":[{"start":0,"end":12,"text":"R.Krishnaveni"}]}],"extras":null,"metadata":{"first_done_at":1564050542000,"last_updated_at":1564050542000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "Name: SABAHAT ZARTAR JAMIL\nEmail: angelsaba.jamil588@gmail.com\nPhone: +91-9113876811\n\nExperience Summary:\n\nHaving 3.9 years of varied experience in Data science/Data analytics and TSM.\n\nEmployment History: \n\nCurrently working with Infosys ltd., Bangalore as Technology Analyst since Aug 2015 to present.\n\nProfile Overview:\n\n1. Having good knowledge on Data Science Skills such as Basic Statistics, Hypothesis Testing, Exploratory\nAnalysis, and Predictive Modeling. \n\n2. Good knowledge of Linux, UNIX, Python Programming, Numpy, Pandas, Matplotlib, Seaborn,\nScikitLearn, Networker and TSM. Experience in MS SQL Server, Microsoft technologies like HTML, and\nCSS. \n\n3. Machine Learning Techniques as Linear Regression, Classification (Logistic Regression), Segmentation\n(DBSCAN, K-Means), Decision Trees, Ensemble Learning (Random Forest), Text Mining (NLP). \n\n4. Basic knowledge in hadoop ecosystem including its core and latest component like HDFS,\n\n  Hive ,Sqoop, Flume,Impala, Hbase and Pyspark.\n\n5. Good understanding in doing exploratory analysis and statistical modeling using Python.\n\nScholastics:\n\nB-Tech with 8.01 DGPA in Information and Technology from Neotia Institute of Technology\nManagement and Science, Kolkata in year 2015\n\nTechnology:\n\nBelow is a list of important hardware, software products, tools and methods that I have worked with:- \n\nSkills:\n\nData Science Skills Basic Statistics, Hypothesis Testing, Exploratory\nAnalysis, Predictive Modelling.\n\nScripting Languages Unix Shell Scripting\n\nDatabases SQL Server\n\nPython Libraries Python Programming, Numpy, Pandas,\nMatplotlib, Seaborn, ScikitLearn etc.\n\n\n\nMachine Learning Techniques Linear Regression, Classification (Logistic\nRegression), Segmentation (DBSCAN, K-Means),\nDecision Trees, Ensemble Learning (Random\nForest), Text Mining (NLP)\n\nBig Data Ecosystem HDFS, Hive, Sqoop, Flume,Impala, Hbase and\nPyspark\n\nOS Unix and Windows\n\nDomain Expertise:\n\n1. Experience in working on Data science, Python libraries, Machine Learning Techniques and Tivoli\nStorage Manager for the EDC project of Daimler AG (Mercedes Benz) \n\n2. Completed my training successfully in Infosys Mysore on Python, My SQL, PL/SQL, Linux, UNIX and\nWindows. \n\nCertifications:\n\n1. Successful completion of industrial training from Ardent Computech Training Center, Kolkata on\nOnline Bookstore using PHP5 and My SQL.\n\n2. Successful completion of Data Science training from Analytix lab.\n\nProfessional Experience:\n\nWorking for Infosys from Aug 2015 to till date\n\nCustomer Name: Daimler AMG \n\nAnalytics Edge - Emerging Accounts (Oct 2017 – Present)\n\nSkills:  Python, SQL, Machine Learning\n\nDescription:\n\nThe objective of being a part of this team is to work across different modules in different projects, which\nrequires converting raw data into meaningful business insights. Work closely with various teams across\nthe company to identify and solve business challenges utilizing large structured, semi-structured, and\nunstructured data in a distributed processing environment.\n\nRoles & Responsibilities:\n\nWorking on end-to-end project modules of Data Ingestion, Data Cleansing, Data Preparation, Data\nSplitting, Model Preparation and validation.\n\nInvolved in working with distributed computation of large datasets, data retrieval, data preparation,\ndata cleansing, modeling, exploratory analysis, predictive modeling, model tuning, optimization etc.\n\n\n\nResponsibility (Aug 2015 – Oct 2017)\n\n Tivoli Storage Manager Configuration, Maintenance and Troubleshooting Backup and Restore\nusing TSM.\n\n Installing and Configuring a Tape Library Attached to the IBM Tivoli Storage     Manager Server\nand Working with Media.\n\n Managing Storage Pools, Storage Pool Volumes.\n Policy Management, Defining  Policies, Policy Sets, Management Classes and Copy Groups\n Customizing the Tivoli Storage Manager Database and Recovery Log, Sizing  the     Databases and\n\nRecovery Log, Adding Space, Reducing the DB and Recovery Log,     Mirroring, Space Triggers.\n Backup-Archive Client Functions, Backup Sets, Performing Client Backups,  Restoring Files,\n\nArchive and Retrieve, Backup and Restore with the command Line.\n Knowledge on performing backups & restores using web client, GUI & command     Line.\n Enhanced TSM Administration tasks such as migration and Space reclamation.\n Implementation of TSM client, ERP and storage agent\n Monitoring TSM backups (DB online, archive, offline and F/S) and resolving the backup   issues.\n\n Creating nodes and  defining schedules and associating schedules to particular node by\n\ncommand mode and GUI\n\n Check in and Check out Tapes as Scratch\n\n Triggering backups manually\n\n Analyzing and performing the trouble shooting for backup issues \n\n Implementing and Configuring the Tivoli Common Reporting Tool and Admin Console\n\n Knowledge on HANA, SQL, Oracle & DB2 database server’s backups.\n Installing Networker client software on UNIX and Windows O/S.\n Knowledge on File system, SQL DB, VADP, and EBR backup configuration on Networker.\n\nDeclaration:\n\nI do hereby declare that the information personal and professional, mentioned above are correct and\ntrue to the best of my knowledge and belief.\n\nSabahat Jamil\n\nTextControl1","annotation":[{"label":["Name"],"points":[{"start":5183,"end":5183,"text":"\n"}]},{"label":["Name"],"points":[{"start":5182,"end":5182,"text":"\n"}]},{"label":["Name"],"points":[{"start":5168,"end":5168,"text":"\n"}]},{"label":["Name"],"points":[{"start":5167,"end":5167,"text":"\n"}]},{"label":["Name"],"points":[{"start":5122,"end":5122,"text":"\n"}]},{"label":["Name"],"points":[{"start":5022,"end":5022,"text":"\n"}]},{"label":["Name"],"points":[{"start":5021,"end":5021,"text":"\n"}]},{"label":["Name"],"points":[{"start":5008,"end":5008,"text":"\n"}]},{"label":["Name"],"points":[{"start":5007,"end":5007,"text":"\n"}]},{"label":["Name"],"points":[{"start":4922,"end":4922,"text":"\n"}]},{"label":["Operating_Systems"],"points":[{"start":4910,"end":4916,"text":"Windows"}]},{"label":["Skills"],"points":[{"start":4901,"end":4904,"text":"UNIX"}]},{"label":["Name"],"points":[{"start":4858,"end":4858,"text":"\n"}]},{"label":["Name"],"points":[{"start":4792,"end":4792,"text":"\n"}]},{"label":["Name"],"points":[{"start":4791,"end":4791,"text":"\n"}]},{"label":["Name"],"points":[{"start":4709,"end":4709,"text":"\n"}]},{"label":["Name"],"points":[{"start":4708,"end":4708,"text":"\n"}]},{"label":["Name"],"points":[{"start":4641,"end":4641,"text":"\n"}]},{"label":["Name"],"points":[{"start":4640,"end":4640,"text":"\n"}]},{"label":["Name"],"points":[{"start":4610,"end":4610,"text":"\n"}]},{"label":["Name"],"points":[{"start":4609,"end":4609,"text":"\n"}]},{"label":["Name"],"points":[{"start":4567,"end":4567,"text":"\n"}]},{"label":["Name"],"points":[{"start":4566,"end":4566,"text":"\n"}]},{"label":["Name"],"points":[{"start":4545,"end":4545,"text":"\n"}]},{"label":["Name"],"points":[{"start":4544,"end":4544,"text":"\n"}]},{"label":["Name"],"points":[{"start":4455,"end":4455,"text":"\n"}]},{"label":["Name"],"points":[{"start":4454,"end":4454,"text":"\n"}]},{"label":["Name"],"points":[{"start":4356,"end":4356,"text":"\n"}]},{"label":["Name"],"points":[{"start":4302,"end":4302,"text":"\n"}]},{"label":["Name"],"points":[{"start":4225,"end":4225,"text":"\n"}]},{"label":["Name"],"points":[{"start":4138,"end":4138,"text":"\n"}]},{"label":["Name"],"points":[{"start":4074,"end":4074,"text":"\n"}]},{"label":["Name"],"points":[{"start":4073,"end":4073,"text":"\n"}]},{"label":["Name"],"points":[{"start":3980,"end":3980,"text":"\n"}]},{"label":["Name"],"points":[{"start":3887,"end":3887,"text":"\n"}]},{"label":["Name"],"points":[{"start":3886,"end":3886,"text":"\n"}]},{"label":["Name"],"points":[{"start":3788,"end":3788,"text":"\n"}]},{"label":["Name"],"points":[{"start":3699,"end":3699,"text":"\n"}]},{"label":["Name"],"points":[{"start":3651,"end":3651,"text":"\n"}]},{"label":["Name"],"points":[{"start":3650,"end":3650,"text":"\n"}]},{"label":["Name"],"points":[{"start":3626,"end":3626,"text":"\n"}]},{"label":["Name"],"points":[{"start":3528,"end":3528,"text":"\n"}]},{"label":["Name"],"points":[{"start":3527,"end":3527,"text":"\n"}]},{"label":["Name"],"points":[{"start":3516,"end":3516,"text":"\n"}]},{"label":["Name"],"points":[{"start":3425,"end":3425,"text":"\n"}]},{"label":["Name"],"points":[{"start":3424,"end":3424,"text":"\n"}]},{"label":["Name"],"points":[{"start":3387,"end":3387,"text":"\n"}]},{"label":["Name"],"points":[{"start":3386,"end":3386,"text":"\n"}]},{"label":["Name"],"points":[{"start":3385,"end":3385,"text":"\n"}]},{"label":["Name"],"points":[{"start":3384,"end":3384,"text":"\n"}]},{"label":["Name"],"points":[{"start":3283,"end":3283,"text":"\n"}]},{"label":["Name"],"points":[{"start":3181,"end":3181,"text":"\n"}]},{"label":["Name"],"points":[{"start":3180,"end":3180,"text":"\n"}]},{"label":["Name"],"points":[{"start":3135,"end":3135,"text":"\n"}]},{"label":["Name"],"points":[{"start":3039,"end":3039,"text":"\n"}]},{"label":["Name"],"points":[{"start":3038,"end":3038,"text":"\n"}]},{"label":["Name"],"points":[{"start":3012,"end":3012,"text":"\n"}]},{"label":["Name"],"points":[{"start":3011,"end":3011,"text":"\n"}]},{"label":["Name"],"points":[{"start":2952,"end":2952,"text":"\n"}]},{"label":["Name"],"points":[{"start":2849,"end":2849,"text":"\n"}]},{"label":["Name"],"points":[{"start":2746,"end":2746,"text":"\n"}]},{"label":["Name"],"points":[{"start":2638,"end":2638,"text":"\n"}]},{"label":["Name"],"points":[{"start":2637,"end":2637,"text":"\n"}]},{"label":["Name"],"points":[{"start":2624,"end":2624,"text":"\n"}]},{"label":["Name"],"points":[{"start":2623,"end":2623,"text":"\n"}]},{"label":["Skills"],"points":[{"start":2607,"end":2622,"text":"Machine Learning"}]},{"label":["Skills"],"points":[{"start":2594,"end":2599,"text":"Python"}]},{"label":["Name"],"points":[{"start":2584,"end":2584,"text":"\n"}]},{"label":["Name"],"points":[{"start":2583,"end":2583,"text":"\n"}]},{"label":["Name"],"points":[{"start":2527,"end":2527,"text":"\n"}]},{"label":["Name"],"points":[{"start":2526,"end":2526,"text":"\n"}]},{"label":["Name"],"points":[{"start":2498,"end":2498,"text":"\n"}]},{"label":["Name"],"points":[{"start":2497,"end":2497,"text":"\n"}]},{"label":["Name"],"points":[{"start":2450,"end":2450,"text":"\n"}]},{"label":["Name"],"points":[{"start":2449,"end":2449,"text":"\n"}]},{"label":["Name"],"points":[{"start":2424,"end":2424,"text":"\n"}]},{"label":["Name"],"points":[{"start":2423,"end":2423,"text":"\n"}]},{"label":["Certifications"],"points":[{"start":2383,"end":2394,"text":"Data Science"}]},{"label":["Name"],"points":[{"start":2354,"end":2354,"text":"\n"}]},{"label":["Name"],"points":[{"start":2353,"end":2353,"text":"\n"}]},{"label":["Certifications"],"points":[{"start":2337,"end":2351,"text":"PHP5 and My SQL"}]},{"label":["Name"],"points":[{"start":2313,"end":2313,"text":"\n"}]},{"label":["Name"],"points":[{"start":2215,"end":2215,"text":"\n"}]},{"label":["Name"],"points":[{"start":2214,"end":2214,"text":"\n"}]},{"label":["Name"],"points":[{"start":2198,"end":2198,"text":"\n"}]},{"label":["Name"],"points":[{"start":2197,"end":2197,"text":"\n"}]},{"label":["Operating_Systems"],"points":[{"start":2188,"end":2194,"text":"Windows"}]},{"label":["Name"],"points":[{"start":2187,"end":2187,"text":"\n"}]},{"label":["Skills"],"points":[{"start":2179,"end":2182,"text":"UNIX"}]},{"label":["Skills"],"points":[{"start":2172,"end":2176,"text":"Linux"}]},{"label":["Skills"],"points":[{"start":2164,"end":2169,"text":"PL/SQL"}]},{"label":["Skills"],"points":[{"start":2156,"end":2161,"text":"My SQL"}]},{"label":["Skills"],"points":[{"start":2148,"end":2153,"text":"Python"}]},{"label":["Name"],"points":[{"start":2088,"end":2088,"text":"\n"}]},{"label":["Name"],"points":[{"start":2087,"end":2087,"text":"\n"}]},{"label":["Name"],"points":[{"start":2020,"end":2020,"text":"\n"}]},{"label":["Skills"],"points":[{"start":1982,"end":1997,"text":"Machine Learning"}]},{"label":["Skills"],"points":[{"start":1964,"end":1969,"text":"Python"}]},{"label":["Name"],"points":[{"start":1921,"end":1921,"text":"\n"}]},{"label":["Name"],"points":[{"start":1920,"end":1920,"text":"\n"}]},{"label":["Name"],"points":[{"start":1902,"end":1902,"text":"\n"}]},{"label":["Name"],"points":[{"start":1901,"end":1901,"text":"\n"}]},{"label":["Operating_Systems"],"points":[{"start":1894,"end":1900,"text":"Windows"}]},{"label":["Operating_Systems"],"points":[{"start":1885,"end":1888,"text":"Unix"}]},{"label":["Name"],"points":[{"start":1881,"end":1881,"text":"\n"}]},{"label":["Name"],"points":[{"start":1880,"end":1880,"text":"\n"}]},{"label":["Skills"],"points":[{"start":1873,"end":1879,"text":"Pyspark"}]},{"label":["Name"],"points":[{"start":1872,"end":1872,"text":"\n"}]},{"label":["Skills"],"points":[{"start":1863,"end":1867,"text":"Hbase"}]},{"label":["Skills"],"points":[{"start":1855,"end":1860,"text":"Impala"}]},{"label":["Skills"],"points":[{"start":1849,"end":1853,"text":"Flume"}]},{"label":["Skills"],"points":[{"start":1842,"end":1846,"text":"Sqoop"}]},{"label":["Skills"],"points":[{"start":1836,"end":1839,"text":"Hive"}]},{"label":["Skills"],"points":[{"start":1830,"end":1833,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":1811,"end":1818,"text":"Big Data"}]},{"label":["Name"],"points":[{"start":1810,"end":1810,"text":"\n"}]},{"label":["Name"],"points":[{"start":1809,"end":1809,"text":"\n"}]},{"label":["Skills"],"points":[{"start":1805,"end":1807,"text":"NLP"}]},{"label":["Skills"],"points":[{"start":1792,"end":1802,"text":"Text Mining"}]},{"label":["Name"],"points":[{"start":1782,"end":1782,"text":"\n"}]},{"label":["Skills"],"points":[{"start":1776,"end":1788,"text":"Random\nForest"}]},{"label":["Skills"],"points":[{"start":1757,"end":1773,"text":"Ensemble Learning"}]},{"label":["Skills"],"points":[{"start":1741,"end":1754,"text":"Decision Trees"}]},{"label":["Name"],"points":[{"start":1740,"end":1740,"text":"\n"}]},{"label":["Skills"],"points":[{"start":1731,"end":1737,"text":"K-Means"}]},{"label":["Skills"],"points":[{"start":1723,"end":1728,"text":"DBSCAN"}]},{"label":["Skills"],"points":[{"start":1709,"end":1720,"text":"Segmentation"}]},{"label":["Name"],"points":[{"start":1695,"end":1695,"text":"\n"}]},{"label":["Skills"],"points":[{"start":1687,"end":1705,"text":"Logistic\nRegression"}]},{"label":["Skills"],"points":[{"start":1671,"end":1684,"text":"Classification"}]},{"label":["Skills"],"points":[{"start":1652,"end":1668,"text":"Linear Regression"}]},{"label":["Skills"],"points":[{"start":1624,"end":1639,"text":"Machine Learning"}]},{"label":["Name"],"points":[{"start":1623,"end":1623,"text":"\n"}]},{"label":["Name"],"points":[{"start":1622,"end":1622,"text":"\n"}]},{"label":["Name"],"points":[{"start":1621,"end":1621,"text":"\n"}]},{"label":["Name"],"points":[{"start":1620,"end":1620,"text":"\n"}]},{"label":["Skills"],"points":[{"start":1604,"end":1614,"text":"ScikitLearn"}]},{"label":["Skills"],"points":[{"start":1595,"end":1601,"text":"Seaborn"}]},{"label":["Skills"],"points":[{"start":1583,"end":1592,"text":"Matplotlib"}]},{"label":["Name"],"points":[{"start":1582,"end":1582,"text":"\n"}]},{"label":["Skills"],"points":[{"start":1575,"end":1580,"text":"Pandas"}]},{"label":["Skills"],"points":[{"start":1568,"end":1572,"text":"Numpy"}]},{"label":["Skills"],"points":[{"start":1531,"end":1536,"text":"Python"}]},{"label":["Name"],"points":[{"start":1530,"end":1530,"text":"\n"}]},{"label":["Name"],"points":[{"start":1529,"end":1529,"text":"\n"}]},{"label":["Skills"],"points":[{"start":1519,"end":1528,"text":"SQL Server"}]},{"label":["Name"],"points":[{"start":1508,"end":1508,"text":"\n"}]},{"label":["Name"],"points":[{"start":1507,"end":1507,"text":"\n"}]},{"label":["Skills"],"points":[{"start":1486,"end":1506,"text":" Unix Shell Scripting"}]},{"label":["Name"],"points":[{"start":1466,"end":1466,"text":"\n"}]},{"label":["Name"],"points":[{"start":1465,"end":1465,"text":"\n"}]},{"label":["Skills"],"points":[{"start":1444,"end":1463,"text":"Predictive Modelling"}]},{"label":["Name"],"points":[{"start":1433,"end":1433,"text":"\n"}]},{"label":["Skills"],"points":[{"start":1421,"end":1441,"text":" Exploratory\nAnalysis"}]},{"label":["Skills"],"points":[{"start":1402,"end":1419,"text":"Hypothesis Testing"}]},{"label":["Skills"],"points":[{"start":1384,"end":1399,"text":"Basic Statistics"}]},{"label":["Skills"],"points":[{"start":1364,"end":1375,"text":"Data Science"}]},{"label":["Name"],"points":[{"start":1363,"end":1363,"text":"\n"}]},{"label":["Name"],"points":[{"start":1362,"end":1362,"text":"\n"}]},{"label":["Name"],"points":[{"start":1354,"end":1354,"text":"\n"}]},{"label":["Name"],"points":[{"start":1353,"end":1353,"text":"\n"}]},{"label":["Name"],"points":[{"start":1250,"end":1250,"text":"\n"}]},{"label":["Name"],"points":[{"start":1249,"end":1249,"text":"\n"}]},{"label":["Name"],"points":[{"start":1237,"end":1237,"text":"\n"}]},{"label":["Name"],"points":[{"start":1236,"end":1236,"text":"\n"}]},{"label":["Name"],"points":[{"start":1191,"end":1191,"text":"\n"}]},{"label":["Degree"],"points":[{"start":1104,"end":1109,"text":"B-Tech"}]},{"label":["Name"],"points":[{"start":1103,"end":1103,"text":"\n"}]},{"label":["Name"],"points":[{"start":1102,"end":1102,"text":"\n"}]},{"label":["Name"],"points":[{"start":1089,"end":1089,"text":"\n"}]},{"label":["Name"],"points":[{"start":1088,"end":1088,"text":"\n"}]},{"label":["Skills"],"points":[{"start":1081,"end":1086,"text":"Python"}]},{"label":["Name"],"points":[{"start":997,"end":997,"text":"\n"}]},{"label":["Name"],"points":[{"start":996,"end":996,"text":"\n"}]},{"label":["Skills"],"points":[{"start":988,"end":994,"text":"Pyspark"}]},{"label":["Skills"],"points":[{"start":978,"end":982,"text":"Hbase"}]},{"label":["Skills"],"points":[{"start":970,"end":975,"text":"Impala"}]},{"label":["Skills"],"points":[{"start":964,"end":968,"text":"Flume"}]},{"label":["Skills"],"points":[{"start":957,"end":961,"text":"Sqoop"}]},{"label":["Skills"],"points":[{"start":951,"end":954,"text":"Hive"}]},{"label":["Name"],"points":[{"start":948,"end":948,"text":"\n"}]},{"label":["Name"],"points":[{"start":947,"end":947,"text":"\n"}]},{"label":["Skills"],"points":[{"start":942,"end":945,"text":"HDFS"}]},{"label":["Name"],"points":[{"start":857,"end":857,"text":"\n"}]},{"label":["Name"],"points":[{"start":856,"end":856,"text":"\n"}]},{"label":["Skills"],"points":[{"start":850,"end":852,"text":"NLP"}]},{"label":["Skills"],"points":[{"start":837,"end":847,"text":"Text Mining"}]},{"label":["Skills"],"points":[{"start":802,"end":818,"text":"Ensemble Learning"}]},{"label":["Skills"],"points":[{"start":786,"end":799,"text":"Decision Trees"}]},{"label":["Skills"],"points":[{"start":776,"end":782,"text":"K-Means"}]},{"label":["Skills"],"points":[{"start":768,"end":773,"text":"DBSCAN"}]},{"label":["Name"],"points":[{"start":766,"end":766,"text":"\n"}]},{"label":["Skills"],"points":[{"start":754,"end":765,"text":"Segmentation"}]},{"label":["Skills"],"points":[{"start":716,"end":729,"text":"Classification"}]},{"label":["Skills"],"points":[{"start":697,"end":713,"text":"Linear Regression"}]},{"label":["Skills"],"points":[{"start":666,"end":681,"text":"Machine Learning"}]},{"label":["Name"],"points":[{"start":662,"end":662,"text":"\n"}]},{"label":["Name"],"points":[{"start":661,"end":661,"text":"\n"}]},{"label":["Name"],"points":[{"start":655,"end":655,"text":"\n"}]},{"label":["Skills"],"points":[{"start":606,"end":615,"text":"SQL Server"}]},{"label":["Skills"],"points":[{"start":557,"end":567,"text":"ScikitLearn"}]},{"label":["Name"],"points":[{"start":556,"end":556,"text":"\n"}]},{"label":["Skills"],"points":[{"start":548,"end":554,"text":"Seaborn"}]},{"label":["Skills"],"points":[{"start":536,"end":545,"text":"Matplotlib"}]},{"label":["Skills"],"points":[{"start":528,"end":533,"text":"Pandas"}]},{"label":["Skills"],"points":[{"start":521,"end":525,"text":"Numpy"}]},{"label":["Skills"],"points":[{"start":501,"end":506,"text":"Python"}]},{"label":["Skills"],"points":[{"start":495,"end":498,"text":"UNIX"}]},{"label":["Skills"],"points":[{"start":488,"end":492,"text":"Linux"}]},{"label":["Name"],"points":[{"start":466,"end":466,"text":"\n"}]},{"label":["Name"],"points":[{"start":465,"end":465,"text":"\n"}]},{"label":["Name"],"points":[{"start":429,"end":429,"text":"\n"}]},{"label":["Skills"],"points":[{"start":417,"end":437,"text":" Exploratory\nAnalysis"}]},{"label":["Skills"],"points":[{"start":398,"end":415,"text":"Hypothesis Testing"}]},{"label":["Skills"],"points":[{"start":380,"end":395,"text":"Basic Statistics"}]},{"label":["Skills"],"points":[{"start":352,"end":363,"text":"Data Science"}]},{"label":["Name"],"points":[{"start":323,"end":323,"text":"\n"}]},{"label":["Name"],"points":[{"start":322,"end":322,"text":"\n"}]},{"label":["Name"],"points":[{"start":304,"end":304,"text":"\n"}]},{"label":["Name"],"points":[{"start":303,"end":303,"text":"\n"}]},{"label":["Name"],"points":[{"start":207,"end":207,"text":"\n"}]},{"label":["Name"],"points":[{"start":206,"end":206,"text":"\n"}]},{"label":["Name"],"points":[{"start":185,"end":185,"text":"\n"}]},{"label":["Name"],"points":[{"start":184,"end":184,"text":"\n"}]},{"label":["Years_of_Experience"],"points":[{"start":114,"end":122,"text":"3.9 years"}]},{"label":["Name"],"points":[{"start":106,"end":106,"text":"\n"}]},{"label":["Name"],"points":[{"start":105,"end":105,"text":"\n"}]},{"label":["Name"],"points":[{"start":85,"end":85,"text":"\n"}]},{"label":["Name"],"points":[{"start":84,"end":84,"text":"\n"}]},{"label":["Mobile_No"],"points":[{"start":70,"end":83,"text":"+91-9113876811"}]},{"label":["Name"],"points":[{"start":62,"end":62,"text":"\n"}]},{"label":["Email_Address"],"points":[{"start":34,"end":61,"text":"angelsaba.jamil588@gmail.com"}]},{"label":["Name"],"points":[{"start":26,"end":26,"text":"\n"}]},{"label":["Name"],"points":[{"start":6,"end":25,"text":"SABAHAT ZARTAR JAMIL"}]}],"extras":null,"metadata":{"first_done_at":1564209077000,"last_updated_at":1564209077000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "Sanjeev Dotasara\n\n\tEmail:sanjeevdotasara@gmail.com, Mobile No.: +91-8003580632\n\n\tSector-39,Gurgoan\n\n\n\n\tSUMMARY\n\n\t\n\n\n\nADM-201, PD1 Certified and 55 trailhead badges.\n·    Around 4 year of experience in Salesforce configuration, customization and Lightning.\n·    Around 2 years of experience in Android, Java, JS, Cordova and React Native framework.\n·    Experience in implementing the Security Model (Object, Field & Record level) in                                        Salesforce.com.\n·         Experience in Apex Classes, triggers, test classes and batch jobs.\n·         Experience in custom settings based development.\n·         Experience in custom labels and static resources based development.\n·         Knowledge and experience of integration API.\n·         Experience of Apex based sharing development.\n·         Experience on HTML, CSS, JavaScript, jQuery.\n·         Experience on Java, React JS, XML.\n·         Salesforce site pages.\n·         Experience in creating Visualforce pages and Visualforce components and JS remoting.\n·         Record based sharing.\n·         Experience converting to Salesforce Custom Objects, Lookup Relationships, Junction      objects, Master-detail relationships.\n·         Experience in designing Page layout, Custom Formula Fields, Field Dependencies, and Validation Rules.\n·         Flows, Workflows and Approval Process, Process Builder.\n·         Queue, Public Group. Experience in working with reports, custom report type dashboards.\n·         Experience in data migration using Data Loader and Import Wizard.\nExperience in Bit Bucket, Jira, and Agile methodology.\nExperience in deployment Via Change Set and Eclipse IDE.\n\n\n\n\tWorking Experience\n\n\t\n\n\n\n· Current Employer\nCompany Name                 \t        : Cognizant Technology Solutions, Gurgaon\nDesignation                         \t        :  Salesforce Developer\nDuration\t\t\t        : March. 2018 to Present\t\n\n\n\n\n· Previous Employer\nCompany Name                 \t        : Dotsquares. Ltd, Jaipur\nDesignation                         \t        :  Salesforce Developer\nDuration\t\t\t        : Nov. 2014 to Feb\t2018\n\n\n\n\n\n· Previous   Employer\n\nCompany Name                 \t        : Infowinder Software pvt. ltd, Jaipur\nDesignation                         \t        :  Android Developer\nDuration\t\t\t        : May.2013 to Nov.2014  \n\n \n\t\tKEY SKILLS\n \n\n\n \n\n\n \n \n \n\t Programming languages\n\tPrimary: Apex, Secondary: Java\n\n\t             \n\t             \n\n\t Operating System\n\tWindows 95/98/XP/7/8/10.\n\n\t            \n\t            \n\n\t Version Control\n\tSalesforce\n\n\t            \n\t            \n\n\t Salesforce Technology\n\tSalesforce CRM Customization: Apex Classes/ Controllers, Visual force Pages/Components, Batch Apex and Triggers, Custom Settings, Custom Labels, Static Resources, JavaScript, Wrapper Classes, Salesforce Sites, Integration API, Lightning(Basics), Test classes, Apex based sharing.\n \nSalesforce CRM Configuration: Queue, Public Group, Work flow & Approvals, Process Builder, Validation Rules, Profile Permission, Role, Permission Set, Sharing Settings, Change Set Deployment, Assignment Rules, Record Level sharing.\n\n\t             \n\t             \n\n\t             \n\t             \n\n\t Tools\n\tData Loader, Eclipse\n\n\n\n\n \n \n\t         \n\t \n\t \n\t       \n\t      \n\n\t         \n\t1. Project\n\t         \n\tAllergan International\n\n\t           \n\t            \n\t            \n\t           \n\t           \n\n\t         \n\tRole\n\t         \n\tSFDC Developer\n\n\t           \n\t            \n\t            \n\t           \n\t           \n\n\t         \n\tTeam size\n\t                      7\n\t        \n\t     \n\n\t           \n\t           \n\t            \n\t          \n\t           \n\n\t         \n\tTechnologies used\n\t         \n\tVEEVA CRM, Zeltiq, Apex, Saleforce.com, Salesforce Configuration, Salesforce Customization\n\n\t           \n\t            \n\t            \n\t           \n\t          \n\n\t\n\t\n\t\n\t\n\t\n\t\n\n\n \n\n\n\n\nDescription:\n \nData migration from Zeltiq(A Allergan company) to Allergan VEEVA CRM. Design and developed components in Lightning framework for CoolSculpting data.\n \nResponsibilities:\n1. Worked on Veeva CRM.\n2. Worked on Email to Case Premium.\n3.      Worked on Case merge premium and case flag.\n4.      Workflows were used to send the emails timely.\n5.      Created sharing setting, profiles, roles and permission set.\n6.      Created Lightning components.\n7.      Created Visualforce pages for my dashboard and added it to the home component.\n\n\n\t         \n\t2. Project\n\t         \n\tManual Life Canada (MLC)\n\n\t           \n\t            \n\t            \n\t           \n\t           \n\n\t         \n\tRole\n\t         \n\tSFDC Developer\n\n\t           \n\t            \n\t            \n\t           \n\t           \n\n\t         \n\tTeam size\n\t                      5\n\t        \n\t     \n\n\t           \n\t           \n\t            \n\t          \n\t           \n\n\t         \n\tTechnologies used\n\t         \n\tApex, Saleforce.com, Salesforce Configuration, Salesforce Customization, Lightning Development\n\n\t           \n\t            \n\t            \n\t           \n\t          \n\n\t\n\t\n\t\n\t\n\t\n\t\n\n\n \n\nDescription:\n \nSupport process for customer regarding cases, Entitlement process and milestones.\n\n \nResponsibilities:\n1. Worked on Entitlement process and milestones\n2. Worked on Email to Case.\n3.      Worked on Phone to Case.\n4.      Workflows were used to send the emails timely.\n5.      Created sharing setting, profiles, roles and permission set.\n6.      Created Lightning components.\n\n\n\t\n     \n\t \n\t \n\t         \n\t    \n\n\t         \n\t3. Project\n\t         \n\tCongunity\n\n\t           \n\t            \n\t           \n\t           \n\t           \n\n\t         \n\tRole\n\t         \n\tSFDC Developer\n\n\t           \n\t            \n\t            \n\t           \n\t           \n\n\t         \n\tTeam size\n\t                        4\n\n\t           \n\t            \n\t            \n\t           \n\t           \n\n\t         \n\tTechnologies used\n\t         \n\tApex, Saleforce.com, Salesforce Configuration, Salesforce Customization\n\n\t           \n\t           \n\t            \n\t           \n\t           \n\n\t\n\t\n\t\n\t\n\t\n\t\n\n\n \n\n\n\nDescription:\n \nDevelopment and management of the business to business sales flow for the organization that helps to provide customer service management for electronics equipment in Salesforce. Below are my roles:\n \n\nResponsibilities:\n1.      Created the object structure and their relationships.\n2.      Apex triggers, batches, classes.\n3.      Created overridden visualforce pages for the standard objects (Opportunity, Account, Contact, Case) and custom objects detailed pages and to create custom visualforce page reports.\n4.      Workflows were used to send the emails timely.\n5.      Created sharing setting, profiles, roles and permission set.\n6.      Created reports using custom report types as per their business requirement. The report includes custom report types, formula fields, and bucket fields based on the amount filter. The reports that were created are summary, and matrix.\n7.      Created visualforce pages for my dashboard and added it to the home component.\n8.      Created record types and page layouts and give access to the appropriate picklist values as per the business requirement.\n \n\n\n\nANDROID PROJECTS :\n\n1. Contact Creator Salesforce:\nhttps://play.google.com/store/apps/details?id=com.contactcreatorsalesforce\nIt is an application that provide all contact of salesforce org into android mobile application and their details. There is also a functionality to insert new contacts and update existing contacts from android mobile application to salesforce org.\n\n2. New Castle County Police Department App:\nhttps://play.google.com/store/apps/details?id=com.project.newcastleappp\nIt is a informatics universal app in which user can see information of Police Department and can also submit some information ,Unique feature of this app is Push notification and Tweet List is. Push is coming for two things first  is for Twitter Tweet from Police Department and Second is for sending some alerts to Users.\n\n\n2. Yataxi Application:\nDeveloped app for Yataxi in Spanish and English languages\nYataxi Customer App- \nhttps://play.google.com/store/apps/details?id=akhil.ls.app.yataxicustomer_spanish\n\nIn this app customer can search his nearby drivers and can hail them for a taxi. Customer can view ride on map and can provide feedback.\nYataxi Driver App- \nhttps://play.google.com/store/apps/details?id=akhil.ls.app.yataxi_driver_spanish\nIn this app driver can get request from customer and can accept or reject request. Driver can view pickup location on map and can view ride too. \n\n\n\nCORDOVA PROJECTS :\n\t\n1. EQwaybill :\nPlay Store Link :https://play.google.com/store/apps/details?id=com.eqwaybill\niTunesLink: https://itunes.apple.com/us/app/eq-waybill/id1145874282?ls=1HYPERLINK \"https://itunes.apple.com/us/app/eq-waybill/id1145874282?ls=1&mt=8\"&HYPERLINK \"https://itunes.apple.com/us/app/eq-waybill/id1145874282?ls=1&mt=8\"mt=8\n\n2. Fifit's 14 Day Fat Burn Challenge : \nPlay Store Link :https://play.google.com/store/apps/details?id=com.apHYPERLINK \"https://play.google.com/store/apps/details?id=com.app.fifit\"p.fifit\niTunesLink :\nhttps://itunes.apple.com/in/app/fifits-14-day-fat-burn-challenge/id1154512410?mt=8\n\n\nREACT NATIVE PROJECTS :\n\t\n1. ShopchocoApp :\nPlay Store Link :https://play.google.com/store/apps/details?id=com.shopchoco\niTunesLink: https://itunes.apple.com/us/app/chocolate-therapy/id1290655503?ls=1&mt=8\n\n2. Simply Helping: \nPlay Store Link :https://itunes.apple.com/us/app/simply-helping-services/id1269465009?mt=8\niTunesLink :\nhttps://itunes.apple.com/us/app/simply-helping-services/id1269465009?mt=8\n\n\n\n\n\n\n\t\n\tTechnical Qualification\n\n\t\n\n\n\nDegree\t\t               : B.Tech.\nBatch\t\t\t: 2008-2012\nInstitute\t\t: Shekhawati Engg. College Jjn, Rajasthan\nUniversity\t\t: Rajasthan Technical University Kota, Rajasthan\nB.Tech. Percentage\t: 70.82%.\n12th Percentage               : 74.46%.\n10th Percentage               : 81.33%  \n\n\tPERSONAL PROFILE\n\n\t\n\n\n\nName\t\t\t:  Sanjeev Dotasara\t\nDate of birth               \t:  25-12-1992\nGender\t                              :  Male\t\t\t\nMarital Status\t               :  Married\t\nNationality\t\t:  Indian\nLanguages Known\t:  English, Hindi\n\n\tDECLARATION\n\n\n\nI hereby declare that the above information furnished is true to the best of my knowledge and belief.\n\nPlace\t: Gurgaon\nDate\t: \t\t\t\t\t\t\t\t                       SIGNATURE","annotation":[{"label":["Name"],"points":[{"start":9852,"end":9867,"text":"Sanjeev Dotasara"}]},{"label":["Degree"],"points":[{"start":9707,"end":9712,"text":"B.Tech"}]},{"label":["Degree"],"points":[{"start":9565,"end":9570,"text":"B.Tech"}]},{"label":["Skills"],"points":[{"start":7114,"end":7123,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":6264,"end":6267,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":6141,"end":6150,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":5848,"end":5857,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":5822,"end":5831,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":5801,"end":5804,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":4859,"end":4868,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":4833,"end":4842,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":4812,"end":4815,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":3737,"end":3746,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":3711,"end":3720,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":3690,"end":3693,"text":"Apex"}]},{"label":["Tools"],"points":[{"start":3194,"end":3204,"text":"Data Loader"}]},{"label":["Skills"],"points":[{"start":2890,"end":2899,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":2868,"end":2885,"text":"Apex based sharing"}]},{"label":["Skills"],"points":[{"start":2854,"end":2865,"text":"Test classes"}]},{"label":["Skills"],"points":[{"start":2835,"end":2851,"text":"Lightning(Basics)"}]},{"label":["Skills"],"points":[{"start":2818,"end":2832,"text":"Integration API"}]},{"label":["Skills"],"points":[{"start":2800,"end":2815,"text":"Salesforce Sites"}]},{"label":["Skills"],"points":[{"start":2783,"end":2797,"text":"Wrapper Classes"}]},{"label":["Skills"],"points":[{"start":2771,"end":2780,"text":"JavaScript"}]},{"label":["Skills"],"points":[{"start":2753,"end":2768,"text":"Static Resources"}]},{"label":["Skills"],"points":[{"start":2738,"end":2750,"text":"Custom Labels"}]},{"label":["Skills"],"points":[{"start":2721,"end":2735,"text":"Custom Settings"}]},{"label":["Skills"],"points":[{"start":2711,"end":2718,"text":"Triggers"}]},{"label":["Skills"],"points":[{"start":2696,"end":2705,"text":"Batch Apex"}]},{"label":["Skills"],"points":[{"start":2665,"end":2693,"text":"Visual force Pages/Components"}]},{"label":["Skills"],"points":[{"start":2638,"end":2662,"text":"Apex Classes/ Controllers"}]},{"label":["Skills"],"points":[{"start":2608,"end":2617,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":2585,"end":2594,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":2542,"end":2551,"text":"Salesforce"}]},{"label":["Operating_Systems"],"points":[{"start":2468,"end":2474,"text":"Windows"}]},{"label":["Skills"],"points":[{"start":2411,"end":2414,"text":"Java"}]},{"label":["Skills"],"points":[{"start":2394,"end":2397,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":2051,"end":2060,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":1849,"end":1858,"text":"Salesforce"}]},{"label":["Tools"],"points":[{"start":1530,"end":1540,"text":"Data Loader"}]},{"label":["Skills"],"points":[{"start":1108,"end":1117,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":923,"end":932,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":892,"end":895,"text":"Java"}]},{"label":["Skills"],"points":[{"start":848,"end":857,"text":"JavaScript"}]},{"label":["Skills"],"points":[{"start":781,"end":798,"text":"Apex based sharing"}]},{"label":["Skills"],"points":[{"start":512,"end":515,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":472,"end":481,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":302,"end":305,"text":"Java"}]},{"label":["Skills"],"points":[{"start":201,"end":210,"text":"Salesforce"}]},{"label":["Years_of_Experience"],"points":[{"start":177,"end":182,"text":"4 year"}]},{"label":["Mobile_No"],"points":[{"start":64,"end":77,"text":"+91-8003580632"}]},{"label":["Email_Address"],"points":[{"start":25,"end":49,"text":"sanjeevdotasara@gmail.com"}]},{"label":["Name"],"points":[{"start":0,"end":15,"text":"Sanjeev Dotasara"}]}],"extras":null,"metadata":{"first_done_at":1564211556000,"last_updated_at":1564211556000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "SARAVANAN RK       \t\t\t\t       Phone: 91-8667534743\n\t\t\t\t\t\t\t       Email:  saravananrk14@gmail.com\nProfessional Summary:\n· 4+ years of experience as a Java Developer\n· Good Knowledge on OOP’s concepts\n· Experience in writing Restful Web Services\n· Having good experience in Microservices using Spring boot and Play framework\n· Working experience on building Web Applications using Spring\n· Having good experience in MySQL\n· Having good experience writing Junit using Power Mockito\n· Good exposure in AGILE methodology.\n· Having hands on experience in handling Use Cases, Functional Specification and Knowledge Transfers \n· Good familiarity on using Eclipse, GitHub and Jenkins deployments\nExperience Profile:\n\n· Presently working as a Senior Software Engineer in Mindtree Ltd from Apr 2017 to Till Date\n· Worked as an Associate Software Engineer in Stixis Technologies Pvt Ltd from Feb 2015 to March 2017.\n\nScholastic Profile:\n· Master of Computer Applications (MCA) from Karpagam College of Engineering (Anna University, Chennai) in 2014 with an aggregate of 79%.\n· Bachelor of Computer Applications (BCA) from Vidyasagar College of Arts and Science (Bharathiar University, Coimbatore) in 2011 with an aggregate of 67%.\n· 12 Standard from RVG Higher Sec School in 2008 with an aggregate of 67%\n· 10th Standard from Govt High School in 2006 with an aggregate of 68%\n\nTechnical Expertise:\nLanguages\t\t\t:  Java\nFramework   \t\t:  Spring, Spring boot, Play, Hibernate\nDatabase\t\t\t:  MySQL, \nWeb Services\t\t:  Restful\nTools/Software\t\t:  Eclipse, Jenkins, JIRA\nOperating System\t\t:  Windows \nVersion Control\t\t:  Subversion (SVN) and GitHub\n\nProject #1: \n\nTitle: SWI Loyalty Harmonization                                                                   Dec-17 to Present\t                                                       Team Size: 25\n      Role     : Developer \n      Technologies: Java 8, Micro Services, Rest Web Services, Play Framework, Couch Base, Json\n\n \tMeetings are a major driver of Marriott's global revenue and room nights. Marriott has integrated with Starwood and merged both Starwood and Marriot integration in a project.\nRoles & Responsibilities:\n· Understanding the requirement and Story Analysis and assignments.\n· Coding.\n· Unit Testing.\n· Coordinating with team in completing stories.\n· Tracking stories development.\n· Performing build and Integration activities\n\n\nProject #2: \n\nTitle: DCF(Distribution Controls Framework)                            Sep-18 to January-2019\t                                                       \nTeam Size: 7\n      Role     : Developer \n      Technologies: Play framework, Java, Akka Actor Model, Google Protobuf, JSON, Restful and SOAP Web Services, Apache-Kafka\n\n \tDistribution Controls Framework used to control distribution of Marriott product availability (rates and availability) in the short term, as well as images and content in the long term, for internal and external distribution partners.\nRoles & Responsibilities:\n· Involved in discussions with client to understand the requirement and involved in the design and Development of the Application.\n· Understanding the requirement and Story Analysis and assignments.\n· Coordinating with team in completing stories.\n· Tracking stories development.\n· Performing build and Integration activities\n\n\n\n\n\nProject #3: \n\nTitle: MSA GB(Meeting Service Application Group Billing)               May-17 to Nov-17\t                                                       Team Size: 25\n      Role     : Developer \n      Technologies: Java, Spring, Angular Js, Json, JPA, My SQL, Tomcat\n\n \tGroup Billing (MSA GB) is an extension to existing functionality provided by Meeting Service Application (MSA 2.0). The additional functionalities provided in MSA GB are related to Group billing and Reporting functionalities. As a part of this, meeting planner would be able to perform View Group Charges Data, Create Custom Views for Group Charges Data, Create Charge Flags – self alerting mechanism, Print and Extract & Email Group Charges, Add Notes to Group Charges, Dispute Group Charges, Approve Group Charges. Meeting Services Application would integrate to BTR (Billing Transaction Repository) as a part of MSA GB functionalities. MSA would integrated with BTR for getting the Group billing charge details on daily basis for events happening at GB enabled properties\nRoles & Responsibilities:\n· Developing the core functionality of the application, user Interface design and\n            Implementation.\n· As a Team member, I am responsible for development of code using Action classes,      \n            Controller classes and Business Logic. \n· Developed DataAccessObjects (DAO) layer using JPA Template.\n· Worked in UI side as well using html and Angular Js.\n· Involved in defect fixing and performance tuning. \n\nProject #4: \n\n      Title: Mobile Job Force\t                                                                         Jul-16 to March-17\n      Team Size: 4\n      Role     : Developer \n      Technologies: J2EE, Spring boot, Tomcat, RESTful Web Service Angular JS and  \n\t\t      Hibernate.\n      Mobile Job Force has created with managers in mind. Among all of the responsibilities that managers carry out on a daily basis, they just do not have time to spend sorting and organizing paper applications. Through the app and on the web, managers can access permanently stored applications. From there they can organize each application into a corresponding folder as well as contact applicants on the spot if they are interested.\n\n\n\nRoles & Responsibilities:\n· Involved in analysis, detail design, documentation, and testing phases of the Software development life cycle (SDLC).\n· Involved in discussions with client to understand the requirement\n· Involved in the design and Development of the Application\n\nProject #5: \n\n      Title: Tangerine Cloud Service   \t                                              Feb -15 to April-16\n      Team Size: 6\n      Role     : Developer \n     Technologies: J2EE, Spring, Tomcat, RESTful Web Service, MySql, Ruby on Rails, HTML and   CSS.\n\n      Web Admin Console is for Tangerine Service Platform Provider (such as DoCoMo) to manage variety of service elements, Service Registration and Management, BMA Account Registration and Management, Developer Registration & Management, App Registration & Management, Beacon Registration & Management, Assign to Service/Store , Beacon OEM/Products Registration & Management.\nThe Tangerine application used to show the offers of shopping mall to the customers directly to their mobile app based on their location and it is used to track user preferences based on their activities while shopping, it works based on sensor.\nRoles & Responsibilities:\n· Involved in analysis, detail design, documentation, and testing phases of the Software development life cycle (SDLC).\n· Involved in discussions with client to understand the requirement\n· Involved in the design and development of  Web Console Application \n\n\n\n\n\n\n\n\n\n\n\n                                                                                                                                                                   Saravanan RK","annotation":[{"label":["Skills"],"points":[{"start":5995,"end":6000,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":5076,"end":5084,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":5010,"end":5020,"text":"Spring boot"}]},{"label":["Skills"],"points":[{"start":3529,"end":3534,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":3523,"end":3526,"text":"Java"}]},{"label":["Skills"],"points":[{"start":2619,"end":2622,"text":"Java"}]},{"label":["Skills"],"points":[{"start":2603,"end":2606,"text":"Play"}]},{"label":["Skills"],"points":[{"start":1876,"end":1879,"text":"Java"}]},{"label":["Tools"],"points":[{"start":1620,"end":1625,"text":"GitHub"}]},{"label":["Tools"],"points":[{"start":1599,"end":1608,"text":"Subversion"}]},{"label":["Operating_Systems"],"points":[{"start":1570,"end":1576,"text":"Windows"}]},{"label":["Tools"],"points":[{"start":1544,"end":1547,"text":"JIRA"}]},{"label":["Tools"],"points":[{"start":1535,"end":1541,"text":"Jenkins"}]},{"label":["Tools"],"points":[{"start":1526,"end":1532,"text":"Eclipse"}]},{"label":["Skills"],"points":[{"start":1474,"end":1478,"text":"MySQL"}]},{"label":["Skills"],"points":[{"start":1450,"end":1458,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":1444,"end":1447,"text":"Play"}]},{"label":["Skills"],"points":[{"start":1431,"end":1441,"text":"Spring boot"}]},{"label":["Skills"],"points":[{"start":1423,"end":1428,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":1401,"end":1404,"text":"Java"}]},{"label":["Degree"],"points":[{"start":927,"end":963,"text":"Master of Computer Applications (MCA)"}]},{"label":["Tools"],"points":[{"start":667,"end":673,"text":"Jenkins"}]},{"label":["Tools"],"points":[{"start":656,"end":661,"text":"GitHub"}]},{"label":["Tools"],"points":[{"start":647,"end":653,"text":"Eclipse"}]},{"label":["Skills"],"points":[{"start":414,"end":418,"text":"MySQL"}]},{"label":["Skills"],"points":[{"start":379,"end":384,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":308,"end":311,"text":"Play"}]},{"label":["Skills"],"points":[{"start":292,"end":302,"text":"Spring boot"}]},{"label":["Skills"],"points":[{"start":149,"end":152,"text":"Java"}]},{"label":["Years_of_Experience"],"points":[{"start":121,"end":128,"text":"4+ years"}]},{"label":["Email_Address"],"points":[{"start":73,"end":95,"text":"saravananrk14@gmail.com"}]},{"label":["Mobile_No"],"points":[{"start":37,"end":49,"text":"91-8667534743"}]},{"label":["Name"],"points":[{"start":0,"end":12,"text":"SARAVANAN RK "}]}],"extras":null,"metadata":{"first_done_at":1564055557000,"last_updated_at":1564055557000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "Satheesh Reddy L\t\t\t\t\tEmail\t: Pandu.satheeshL@gmail.com\nBigData-Hadoop Developer\t\t\t\tMobile\t: +91-9700917927\nCareer Objective\n\nServing Notice period and can join within 30 days. Working in BT E-Serv popularly known as British Telecom as a BigData Hadoop Developer. Looking for an intellectually challenging opportunity in Hadoop development and Project Management that requires Core software experience in all phases of software development life cycle (SDLC) starting from requirements, analysis, architecture, technical design, implementation, performance tuning. \n\nExperience Summary\n\n· Having 5 years of experience in BigData technology (Spark, Scala, Hive, MapReduce, Sqoop, Pig, Oozie, Shell, Kibana) as a Developer and Project Lead with multiple teams.\n· Working on CDH distribution and good command on Hadoop cluster.\n· Extracting Structured and Semi-Structured data from Oracle and other File sources from upstream systems into HDFS using scheduled jobs (Sqoop and shell actions).\n· Worked on GPG encryption. Feeds which are coming from upstream systems will be zipped and encrypted using 64 bit algorithm. Need to Decrypt and process into DAA.\n· Used Pig as ETL to process all our Entities and Extracts. Once done will update Oracle using Sqoop and will send the feeds to Down Stream using Local Shell actions.\n· Worked on Oozie scheduler for unloads and regular jobs (Workflows and Co-ord).\n· Used Hive as reporting for end user / business to perform ad-hoc analysis. Also used to analyze Broken Hierarchy between SAC, CUG, CUS and organization hierarchy.\n· Implemented Partitions, Bucketing for monthly unloads and Legacy data in-order to improve the performance of Pig and spark jobs.\n· Developed existing pig jobs into Spark Scala to as part of migration and to reduce the exec timings (Build time has been reduced to 3 days from 15 days)\n· Data Lake story: Bringing the consumer data from various sources and expose the same to multiple systems. Biggest development and crucial part in all our releases.\n· Designed and Developed Kibana dashboard. Generating Json files with index for Logstash. Having excellent command on ELK Stack. Productionised the same.\n· Working on CRM systems as it holds business data and these are destinations.\n· Working in Agile methodology and chairing the scrum calls as part of Agile and make sure everything is going smooth for each release.\n\nWorkspace Environment\n\n\n\tBig Data Ecosystem \n\tSpark, Scala, HDFS, Map Reduce, Hive, Pig, Sqoop, Oozie, Kibana\n\n\tProgramming Languages\n\tCore Java, Pig Latin, Scala\n\n\tDatabases\n\tOracle, CRM systems\n\n\tScripting Languages\n\tUnix and Shell Scripting\n\n\tTools\n\tEclipse, Win SCP, Putty, Webextam, Winmerge\n\n\tVisualization\n\tKibana, Thoughspot\n\n\tVersioning Tools\n\tSVN, Git(Local), Jira.\n\n\n\nEmployer Details\n\n\tEmployer Name\n\tDuration\n\tRole\n\n\tBT E- serv, Bangalore\n\tDec-2016 to Till date\n\tLead & Developer\n\n\tWipro technologies, Bangalore\n\tApr-2014 to Nov-2016\n\tdeveloper\n\n\n\nAcademic Details\n\n\tB.Tech\n(Computer science engineering)\n\tSiddhartha Institute of Engineering and technology, Puttur\n\t76.00%\n\tMay-2013\n\n\tIntermediate\n(MPC)\n\tSri Chaitanya Jr. College, Tirupathi\n\t94.50%\n\tMay-2009\n\n\tHigher education\n(SSC)\n\tAPR School, Gyarampalli\n\t93.00%\n\tMay-2007\n\n\n\n\nProject Summary Details\n\nProject #3:\n\nProject Name\t\t\t: CSS OUK (EE Orange)\nClient\t\t\t\t: British Telecom && EE Orange\nTechnical Environment   \t: HDFS, Spark, Scala, Hive, Sqoop, Oozie, UNIX and shell scripting\nTeam Members\t\t\t : 14\nDuration \t\t\t: January 2018 to till date\nRole\t\t\t\t: Lead and Developer\n\nDescription: EE is a British mobile network operator, internet service provider and acquired by BT Business. It is having main three data areas specified as Orange, Red and green. Each divided based on importance of data it has. We have multiple vendors for data acquisition. We have a systems where feed is being generated using Abinito and the same will be transferred to PIS for GPG encryption for high security. Once encryption completes it will be transferred to SDEDS, from where we BT will be receiving the data. Once we receive the data from SDEDS the processing will starts from here.\n\nRoles and Responsibilities:\n· As the data is highly critical, it is being encrypted with 64 bit cryptographic gpg encryption. Public key will be shared with PIS which they will be used for encryption.\n· Will use Private key to decrypt the same, Unzip it the local and move the files into HDFS post processing store into HDFS (Using Oozie shell action to do this)\n· Once receive the data into HDFS, process the same using Spark Scala with certain conditions and create Feeds and Data Lake into CSS Haas.\n· These feeds are being used by all our DS. We have regular ETL jobs in spark which are written in Scala. After processing storing the same into Hive for other jobs. \n· Hive has been used as a reporting and auditing purpose. \n· Broken Hierarchy has been created using Pig in-order to populate missing linkages.\n· The same thing has been visualized in ETL stack (Kibana) and is self-reflective.\n· Used External Hive tables on top of the output and used for business analysis.\n· Use of Oozie workflow to run multiple serial and parallel action using workflows and co-ord.\n· Chairing the Scrum calls as part of integration with various vendors as part of Agile.\n· Post Data Lake, we have design documents received from architects which is in progress of project development phase-2. \n\nProject #2:\n\nProject Name\t\t\t: SIM REWRITE (BT E- serv)\nClient\t\t\t\t: BT\nTechnical Environment   \t: Spark, Scala, HDFS, Pig, Hive, Sqoop, Oozie, Spark, Scala\nTeam Members\t\t\t : 28 \nDuration \t\t\t: December 2016 to January 2018\nRole\t\t\t\t: Developer\n\nDescription: BT is huge telecom retailer which is maintaining billing accounts, address and\nAssets of customer groups in CRM systems to maintain customer info. We are working with various vendors to rewrite all existing legacy applications into Hadoop. Developed new unload framework with complete architecture and redesign. Storing all existing data from various sources into Hadoop as a part of data lake story. Working on various technologies In Hadoop to implement all new design and stability. Using agile methodology and chairing Scrum to discuss the status with vendors.\n\nRoles and Responsibilities:\n· As a part of rewrite, All AI jobs (ETL) has been successfully written into Spark with Scala which reduces project cost and increased execution time (15 days to 1 day).\n· Developing various Pig scripts for ETL processing and scheduled as part of automation.\n· Created multiple hive jobs to create audit tables to identify broken hierarchy.\n· Automated the Unload framework with redesign and new architecture.\n· Using Sqoop to import and export data from ORACLE to HDFS and vice-versa.\n· Developed shell scripts in order to curl data from AI servers and vice versa.\n· Created various Shell scripts for various monitoring reports (Edge node space, queue size, delta count)—scheduled using Sqoop as shell default jobs disabled.\n· Used External Hive tables on top of the output and used for business analysis.\n· Use of Oozie workflow to run multiple serial and parallel action using workflows and co-ord.\n· Chairing the Scrum calls as part of integration with various vendors as part of Agile\n· Developed Spark applications as part of rewrite and done POC before implementing the same.\n\nProject #1:\n\nProject Name\t\t\t: Shell FMO Build (Wipro Project)\nClient\t\t\t\t: Shell\nTechnical Environment   \t: HDFS, Map Reduce, Pig, Hive, Sqoop\nProject Duration\t\t: September 2014 to till date\nRole\t\t\t\t: Hadoop Developer & Production Support\n\nDescription: Royal Dutch Shell plc commonly known as Shell is an Anglo-Dutch multinational oil and gas company headquartered in the Netherlands. It is the seventh largest company in the world as of 2016, in terms of revenue, and one of the six oil and gas \"super majors\". It is having its business unit entire world which is categorized Asia-Pac, Europe, Americas, Australia and Coastline. The data is getting generated from all there areas and is populated in different databases for different regions.\n\tIn Shell FMO Build the aim is to merge all these domains and collect data into a single system. The data is stored in Hadoop cluster of 22 nodes (may be increase in future). Data is collected from all existing RDMS systems into HDFS and from different staging locations. The raw data will be ingested into HDFS by using Sqoop from various sources. Data transformations are done by using Pig and SSIS ETL tools. Later on it is moved to hive External tables for reports. Reports are being generated on weekly and monthly basis based on customer requirement.\n\nRoles and Responsibilities:\n· Moving data files generated from various location into HDFS using Sqoop and Shell scripts.\n· Developing Hive and Pig queries for data analysis and reporting.\n· Developed PIG scripts to convert the semi structured data to structured data.\n· Worked on Hive partitioning and bucketing to improve the performance of system.\n· Monitoring Hadoop scripts that will load data into HDFS at regular intervals.\n· Writing custom Pig functions as per customer request to clean the data and provide results.\n· Monitoring all Pig and Sqoop jobs day to day.\n· Submitting the Test case and Test result document to the clients.\n· Getting connected with the clients and onshore team to review the code and validation of test results.\n\nAchievements and Appreciations\n\n· Rewarded BMPS award for two quarters continuously. Posted in Hal of Fame for six months.\n· Awarded with Brilliant performance and promoted within 18 months (Least period in BT).\n· Got best team lead award for SIM-Rewrite in BT TSO year end awards.\n· Migrated/developed completed AI to Hadoop within span of six months in BT which reduces project cost and saved execution time (Build reduced from 15 days to 3 days). \n· Received multiple e-thank cards and appreciations in various all hands meet. \n· Two continuous outstanding awards in Wipro for excellent performance in shell.\n\nPersonal Information\n\nName  \t\t\t\t: Satheesh Reddy L\t\nMarital Status\t\t\t: Unmarried\nPassport\t\t\t: L2858858\nNationality\t\t\t: Indian\nLanguages known\t\t: Hindi, English, Telugu\nNotice Period\t\t\t: Serving Notice (30 Days)\n\nDeclaration\n\nI hereby confirm that the information furnished above is true to the best of my knowledge.\n\nPlace: Bangalore\t\t\t\t\t\t\t(Satheesh Reddy)\nL Satheesh Reddy\tBigdata Developer\t9700917927","annotation":[{"label":["Name"],"points":[{"start":10037,"end":10052,"text":"Satheesh Reddy L"}]},{"label":["Skills"],"points":[{"start":9193,"end":9197,"text":"Sqoop"}]},{"label":["Skills"],"points":[{"start":9185,"end":9188,"text":"Pig "}]},{"label":["Skills"],"points":[{"start":9091,"end":9094,"text":"Pig "}]},{"label":["Skills"],"points":[{"start":9047,"end":9050,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":8923,"end":8927,"text":" Hive"}]},{"label":["Skills"],"points":[{"start":8787,"end":8790,"text":"Pig "}]},{"label":["Skills"],"points":[{"start":8777,"end":8781,"text":" Hive"}]},{"label":["Skills"],"points":[{"start":8740,"end":8744,"text":"Sqoop"}]},{"label":["Skills"],"points":[{"start":8729,"end":8732,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":8474,"end":8477,"text":"Pig "}]},{"label":["Skills"],"points":[{"start":8407,"end":8411,"text":"Sqoop"}]},{"label":["Skills"],"points":[{"start":8393,"end":8396,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":8315,"end":8318,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":7479,"end":7483,"text":"Sqoop"}]},{"label":["Skills"],"points":[{"start":7472,"end":7476,"text":" Hive"}]},{"label":["Skills"],"points":[{"start":7468,"end":7470,"text":"Pig"}]},{"label":["Skills"],"points":[{"start":7456,"end":7465,"text":"Map Reduce"}]},{"label":["Skills"],"points":[{"start":7450,"end":7453,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":7261,"end":7265,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":7075,"end":7079,"text":"Oozie"}]},{"label":["Skills"],"points":[{"start":7000,"end":7004,"text":" Hive"}]},{"label":["Skills"],"points":[{"start":6947,"end":6951,"text":"Sqoop"}]},{"label":["Skills"],"points":[{"start":6724,"end":6727,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":6677,"end":6681,"text":"Sqoop"}]},{"label":["Skills"],"points":[{"start":6450,"end":6453,"text":"Pig "}]},{"label":["Skills"],"points":[{"start":6450,"end":6452,"text":"Pig"}]},{"label":["Skills"],"points":[{"start":6347,"end":6351,"text":"Scala"}]},{"label":["Skills"],"points":[{"start":6336,"end":6340,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":5773,"end":5783,"text":"CRM systems"}]},{"label":["Skills"],"points":[{"start":5559,"end":5563,"text":"Scala"}]},{"label":["Skills"],"points":[{"start":5552,"end":5556,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":5545,"end":5549,"text":"Oozie"}]},{"label":["Skills"],"points":[{"start":5538,"end":5542,"text":"Sqoop"}]},{"label":["Skills"],"points":[{"start":5531,"end":5535,"text":" Hive"}]},{"label":["Skills"],"points":[{"start":5527,"end":5529,"text":"Pig"}]},{"label":["Skills"],"points":[{"start":5521,"end":5524,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":5514,"end":5518,"text":"Scala"}]},{"label":["Skills"],"points":[{"start":5507,"end":5511,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":5112,"end":5116,"text":"Oozie"}]},{"label":["Skills"],"points":[{"start":5037,"end":5041,"text":" Hive"}]},{"label":["Skills"],"points":[{"start":4990,"end":4995,"text":"Kibana"}]},{"label":["Skills"],"points":[{"start":4896,"end":4899,"text":"Pig "}]},{"label":["Skills"],"points":[{"start":4896,"end":4898,"text":"Pig"}]},{"label":["Skills"],"points":[{"start":4796,"end":4800,"text":" Hive"}]},{"label":["Skills"],"points":[{"start":4772,"end":4776,"text":" Hive"}]},{"label":["Skills"],"points":[{"start":4727,"end":4731,"text":"Scala"}]},{"label":["Skills"],"points":[{"start":4552,"end":4556,"text":"Scala"}]},{"label":["Skills"],"points":[{"start":4546,"end":4550,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":4517,"end":4520,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":4457,"end":4461,"text":"Oozie"}]},{"label":["Skills"],"points":[{"start":4445,"end":4448,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":4413,"end":4416,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":3407,"end":3411,"text":"Oozie"}]},{"label":["Skills"],"points":[{"start":3400,"end":3404,"text":"Sqoop"}]},{"label":["Skills"],"points":[{"start":3393,"end":3397,"text":" Hive"}]},{"label":["Skills"],"points":[{"start":3387,"end":3391,"text":"Scala"}]},{"label":["Skills"],"points":[{"start":3380,"end":3384,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":3374,"end":3377,"text":"HDFS"}]},{"label":["Degree"],"points":[{"start":2966,"end":2971,"text":"B.Tech"}]},{"label":["Tools"],"points":[{"start":2756,"end":2759,"text":"Jira"}]},{"label":["Tools"],"points":[{"start":2744,"end":2753,"text":"Git(Local)"}]},{"label":["Tools"],"points":[{"start":2739,"end":2741,"text":"SVN"}]},{"label":["Skills"],"points":[{"start":2708,"end":2717,"text":"Thoughspot"}]},{"label":["Skills"],"points":[{"start":2700,"end":2705,"text":"Kibana"}]},{"label":["Tools"],"points":[{"start":2674,"end":2681,"text":"Winmerge"}]},{"label":["Tools"],"points":[{"start":2664,"end":2671,"text":"Webextam"}]},{"label":["Tools"],"points":[{"start":2657,"end":2661,"text":"Putty"}]},{"label":["Tools"],"points":[{"start":2648,"end":2654,"text":"Win SCP"}]},{"label":["Tools"],"points":[{"start":2639,"end":2645,"text":"Eclipse"}]},{"label":["Skills"],"points":[{"start":2614,"end":2628,"text":"Shell Scripting"}]},{"label":["Skills"],"points":[{"start":2605,"end":2608,"text":"Unix"}]},{"label":["Skills"],"points":[{"start":2570,"end":2580,"text":"CRM systems"}]},{"label":["Skills"],"points":[{"start":2562,"end":2567,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":2543,"end":2547,"text":"Scala"}]},{"label":["Skills"],"points":[{"start":2532,"end":2535,"text":"Pig "}]},{"label":["Skills"],"points":[{"start":2532,"end":2534,"text":"Pig"}]},{"label":["Skills"],"points":[{"start":2531,"end":2540,"text":" Pig Latin"}]},{"label":["Skills"],"points":[{"start":2521,"end":2529,"text":"Core Java"}]},{"label":["Skills"],"points":[{"start":2488,"end":2494,"text":" Kibana"}]},{"label":["Skills"],"points":[{"start":2482,"end":2486,"text":"Oozie"}]},{"label":["Skills"],"points":[{"start":2475,"end":2479,"text":"Sqoop"}]},{"label":["Skills"],"points":[{"start":2470,"end":2472,"text":"Pig"}]},{"label":["Skills"],"points":[{"start":2463,"end":2467,"text":" Hive"}]},{"label":["Skills"],"points":[{"start":2452,"end":2461,"text":"Map Reduce"}]},{"label":["Skills"],"points":[{"start":2446,"end":2449,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":2439,"end":2443,"text":"Scala"}]},{"label":["Skills"],"points":[{"start":2432,"end":2436,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":2411,"end":2418,"text":"Big Data"}]},{"label":["Skills"],"points":[{"start":2183,"end":2193,"text":"CRM systems"}]},{"label":["Skills"],"points":[{"start":2040,"end":2046,"text":" Kibana"}]},{"label":["Skills"],"points":[{"start":1736,"end":1740,"text":"Scala"}]},{"label":["Skills"],"points":[{"start":1730,"end":1734,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":1675,"end":1678,"text":"Pig "}]},{"label":["Skills"],"points":[{"start":1675,"end":1677,"text":"Pig"}]},{"label":["Skills"],"points":[{"start":1405,"end":1409,"text":" Hive"}]},{"label":["Skills"],"points":[{"start":1330,"end":1334,"text":"Oozie"}]},{"label":["Skills"],"points":[{"start":1246,"end":1250,"text":"Sqoop"}]},{"label":["Skills"],"points":[{"start":1233,"end":1238,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":1158,"end":1161,"text":"Pig "}]},{"label":["Skills"],"points":[{"start":1158,"end":1160,"text":"Pig"}]},{"label":["Skills"],"points":[{"start":961,"end":965,"text":"Sqoop"}]},{"label":["Skills"],"points":[{"start":934,"end":937,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":877,"end":882,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":695,"end":701,"text":" Kibana"}]},{"label":["Skills"],"points":[{"start":682,"end":686,"text":"Oozie"}]},{"label":["Skills"],"points":[{"start":677,"end":679,"text":"Pig"}]},{"label":["Skills"],"points":[{"start":670,"end":674,"text":"Sqoop"}]},{"label":["Skills"],"points":[{"start":652,"end":656,"text":" Hive"}]},{"label":["Skills"],"points":[{"start":646,"end":650,"text":"Scala"}]},{"label":["Skills"],"points":[{"start":639,"end":643,"text":"Spark"}]},{"label":["Years_of_Experience"],"points":[{"start":593,"end":600,"text":" 5 years"}]},{"label":["Mobile_No"],"points":[{"start":92,"end":105,"text":"+91-9700917927"}]},{"label":["Email_Address"],"points":[{"start":29,"end":53,"text":"Pandu.satheeshL@gmail.com"}]},{"label":["Name"],"points":[{"start":0,"end":15,"text":"Satheesh Reddy L"}]}],"extras":null,"metadata":{"first_done_at":1564244329000,"last_updated_at":1564293808000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "SHAMANTH \t\t\t\t        \t  \tshamanth_uppia20@yahoo.co.in\n\t\t                  1176 1st C Cross / Hampinagara / Bengaluru / +91-9845492515\n\nExperience\n\na) August 2012 – July 2017: Project Engineer | Wipro Technologies | Bangalore\n1) Web Mining: Click Stream Analysis\n· Preparing Design Document\n· Creating Dimension tables and loading data \n· Creating Fact table and loading Click Stream data using business logic given\n· Performing Unit testing and generating unit test document\n· Analysing Click Stream data and generating various reports\n2) Customer Visibility\n\n· Creating Dimension tables and loading data, using Datastage\n· Performing Unit testing and generating unit test document\n3) SIAM Commercial Bank (POC)\n· Creating Target tables and loading sample source data \n· Unit testing\n4) Data Integration Practice | Center of Excellence lab: Administration and Solution\n\n· Installing and Administering Informatica PowerCenter and also various other softwares in the server\n· Providing software access to various accounts based on their requirement\n· Administering Oracle Database, Amazon Web Services\n· Providing solutions for various accounts, in case of issues faced while ETL development\n\n5) Telecom Insights\n\n· Metadata Management using Oracle\n\n6) OPTUS cMDM\n\n· Data Analyst\n· ETL Developer, using Informatica Power Center\n· Framework Developer, using Linux and Oracle\n\nb) July 2017 – October 2018: Data Analyst | Sampradaaya | Bangalore\n\n· Enabled Sales Team by creating a Website\n· Did Data Analysis, Business Intelligence for sales improvement\n· Automated procurement and delivery process\n\nc) October 2018 – Present: Software Engineer | NTT Data | Bangalore\n\n1) SEI Investments\n\n· Participated in Data Analysis, Design, Development and Implementation\n· Developed ETL mappings using Informatica Power Center and Oracle\n· Performed Unit Testing and created Technical documents\n· Performed System Integration Testing\n· Created Runbooks for Control-M Scheduling\nEducation\nSikkim Manipal University for Distance Education\nMaster's degree in Information Technology | August 2014 – February 2016 | Bangalore\n\nRNS Institute of Technology\nBachelor of Engineering in Information Science | August 2009 – June 2012 | Bangalore\n\nTechnical Skills\n· Programming Skills\t: C, C++, C#, Java, HTML. \n· Cloud Services\t: AWS EC2, VPC, RDS, S3, Route 53, Citrix. \n· Operating Systems\t: Windows, Linux. \n· Databases\t\t: Oracle, Teradata. \n· ETL Tools\t\t: IBM DataStage, Informatica PowerCenter. \n· Areas of Interest\t: Cloud Computing, Artificial  Intelligence, Python\nLanguages\nKannada, Hindi, Telugu, Tamil, English\nAdditional Info\nInterests\n\nDigital Photography, Archeology/History, Traveling, Theatre\n\nPersonal Details\n\n\tBirthday\n\t: August 20, 1990\n\n\tMarital Status\n\t: Single","annotation":[{"label":["Tools"],"points":[{"start":2451,"end":2474,"text":"Informatica PowerCenter."}]},{"label":["Tools"],"points":[{"start":2436,"end":2448,"text":"IBM DataStage"}]},{"label":["Skills"],"points":[{"start":2410,"end":2418,"text":"Teradata."}]},{"label":["Skills"],"points":[{"start":2402,"end":2407,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":2379,"end":2383,"text":"Linux"}]},{"label":["Skills"],"points":[{"start":2370,"end":2376,"text":"Windows"}]},{"label":["Skills"],"points":[{"start":2338,"end":2345,"text":" Citrix."}]},{"label":["Skills"],"points":[{"start":2329,"end":2336,"text":"Route 53"}]},{"label":["Skills"],"points":[{"start":2325,"end":2326,"text":"S3"}]},{"label":["Skills"],"points":[{"start":2320,"end":2322,"text":"RDS"}]},{"label":["Skills"],"points":[{"start":2315,"end":2317,"text":"VPC"}]},{"label":["Skills"],"points":[{"start":2306,"end":2312,"text":"AWS EC2"}]},{"label":["Skills"],"points":[{"start":2280,"end":2283,"text":"HTML"}]},{"label":["Skills"],"points":[{"start":2274,"end":2277,"text":"Java"}]},{"label":["Skills"],"points":[{"start":2270,"end":2271,"text":"C#"}]},{"label":["Skills"],"points":[{"start":2265,"end":2267,"text":"C++"}]},{"label":["Skills"],"points":[{"start":2262,"end":2262,"text":"C"}]},{"label":["Degree"],"points":[{"start":2023,"end":2063,"text":"Master's degree in Information Technology"}]},{"label":["Skills"],"points":[{"start":1817,"end":1822,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":1365,"end":1370,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":1355,"end":1359,"text":"Linux"}]},{"label":["Skills"],"points":[{"start":1240,"end":1245,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":1063,"end":1068,"text":"Oracle"}]},{"label":["Years_of_Experience"],"points":[{"start":150,"end":172,"text":"August 2012 – July 2017"}]},{"label":["Mobile_No"],"points":[{"start":119,"end":132,"text":"+91-9845492515"}]},{"label":["Email_Address"],"points":[{"start":25,"end":52,"text":"shamanth_uppia20@yahoo.co.in"}]},{"label":["Name"],"points":[{"start":0,"end":7,"text":"SHAMANTH"}]}],"extras":null,"metadata":{"first_done_at":1564056560000,"last_updated_at":1564056560000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "Curriculum Vitae      Shashank Shekhar  \n\n  \n\n1 | P a g e  \n \n\n  \n\nShashank Shekhar  \n\n  \n  \n\nExperience Summary  \n\n  \n\n• 60+ months of experience in Development, Testing and implementation of Veeva CRM/CLM and \n\nSalesforce application \n\n• 2 years’ prior experience in Siebel application for development and support role  \n\n• Total IT experience of 7+ years \n\n• Veeva Certified Administrator \n\n• Salesforce Admin Certified \n\n• Understanding of CRM and its business application in the Pharmaceutical industry \n\n• Played Team leader role for supporting and enhancing the Veeva CRM application for a major \n\npharma client \n\n• Played role of CLM consultant and administrator \n\n• Played major role in supporting the Veeva CRM application along with downstream iPad \n\napplication  \n\n• Gave demo to client side about new features of Veeva CRM  \n\n• An Enthusiastic & Energetic, Highly Self-motivated, Quick Learner, Committed to Work, Efficient  \n\nTeam Member with Good Communication Skills and Highly Competent  \n\n• Currently working as “Senior Consultant” at Capegmini, Bangalore \n\n \n\n  \n\n  \n\n  \n\n   \n\n\n\n  \n\nCurriculum Vitae      Shashank Shekhar  \n\n  \n\n2 | P a g e  \n \n\nTechnical Skills  \n \n\nHardware / Platforms  Windows 98/XP/2000/Win7/Vista/Win 8/10  \n\nProgramming  \n\nLanguages  \n\nSQL ,SOQL, Java  \n\nScripting Languages  HTML, CSS, JavaScript  \n\nTools  \n\n  \n\nCertifications  \n\n   \n\nSiebel 7.x, Toad for Oracle, SQL Developer  \n\n  \n\nVeeva Certified Administrator  \n\nSalesforce Certified Administrator  \n\nContact Details  \n \n\nMobile No.: +91 9674182742 / +91 9831185263    Email-Id: coolshashank76@gmail.com  \n\nPresent address: 394, Ground floor, Kengeri main road, Uttaragalli, Bangalore, Karnataka-  \n\n560061 \n\n \n\nAcademic Qualification  \n\n  \n\nNAME OF \n\nEXAMINATION  \nINSTITUTE  \n\nYEAR OF \n\nPASSING  \n\nM. B. A(Part-Time) Annamalai University  2014  \n\nB.C.A  BIT Mesra, Ranchi  2011  \n\nI.S.C  \nDAV Public School, Bariatu, \n\nRanchi  \n2008  \n\nC.B.S.E Board  BNS DAV Public School, Giridih  2006  \n\n  \n\n  \n\n \n\n\n\n  \n\nCurriculum Vitae      Shashank Shekhar  \n\n  \n\n3 | P a g e  \n \n\nRelevant Project Experience  \n\nOrganization: Capgemini  \n\n  \n\nINDUSTRY  Pharmaceuticals  \n\nCLIENT  PHARMA CLIENT  \n\nDURATION & \n\nLOCATION  \n\nOct 2018 – Present \n\nPRIMARY ROLE  Veeva Senior Consultant \n\nPROJECT  \n\nDESCRIPTION  \n\n  \n\nPharma client implemented few module of Veeva CRM solution to be used \n\namong the US market so that workforce can use regulated mail channels \n\nthroughout their business process \n\nROLES &  \n\nRESPONSIBILITIES  \n\n• Handling approved email and CLM loading requests \n\n• Supporting team to load other data models related requests \n\n \n\n \n\nOrganization: Accenture  \n\n  \n\nINDUSTRY  Pharmaceuticals  \n\nCLIENT  PHARMA CLIENT  \n\nDURATION & \n\nLOCATION  \n\nApr 2017 – Oct 2018 \n\nBangalore  \n\nPRIMARY ROLE  Veeva Consultant \n\nPROJECT  \n\nDESCRIPTION  \n\n  \n\nPharma client implemented an Veeva CRM solution to be deployed across \n\nregions to enable the Sales force to utilize the CRM solution for a better \n\nbusiness.  \n\nROLES &  \n\nRESPONSIBILITIES  \n\n• Veeva CRM implementation and enhancement  \n\n• Handling new business requests and giving demo/projection  \n\n• Veeva CRM issue resolution as per SLA  \n\n• Monitoring and data maintenance to a downstream iPad app to CRM \napplication  \n\n• Enhancement in Veeva CRM based application as requested by Client  \n\n• Actively following up and providing updates on issues, maintaining SLA, \nhandling escalations, resolving issues with minimum downtime  \n\n• Leading and helping the team members  \n\n• Reporting of the project maintenance activities to leadership  \n\n  \n\n  \n\n\n\n  \n\nCurriculum Vitae      Shashank Shekhar  \n\n  \n\n4 | P a g e  \n \n\n Organization: Tech Mahindra  \n\n  \n\nINDUSTRY  Pharmaceuticals  \n\nCLIENT  PHARMA CLIENT  \n\nDURATION & \n\nLOCATION  \n\nJune 2016 – Apr 2017 \n\nHyderabad/Bangalore \n\nPRIMARY ROLE  Veeva Consultant  \n\nPROJECT  \n\nDESCRIPTION  \n\n  \n\nPharma client implemented an Veeva CRM solution to be deployed across \n\nregions to enable the Sales force to utilize the CRM solution for a better \n\nbusiness.  \n\nROLES &  \n\nRESPONSIBILITIES  \n\n• Veeva CRM issue resolution as per SLA  \n\n• Monitoring and data maintenance to a downstream iPad app to CRM \n\napplication  \n\n• Enhancement in Veeva CRM based application as requested by Client  \n\n• Actively following up and providing updates on issues, maintaining SLA, \n\nhandling escalations, resolving issues with minimum downtime  \n\n  \n\n  \n\nOrganization : ValueLabs  \n\n  \n\nINDUSTRY  Pharmaceuticals  \n\nCLIENT  PHARMA CLIENT  \n\nDURATION & \n\nLOCATION  \n\nFebruary 2016 – June 2016 \n\nHyderabad  \n\nPRIMARY ROLE  Veeva CLM Consultant \n\nPROJECT  \n\nDESCRIPTION  \n\n  \n\nRenowned pharmaceuticals company implemented an integrated  \n\nSalesforce.com based Veeva solution to be deployed across regions to enable \n\nthe Sales force to utilize the CRM solution for a better business.  \n\nROLES &  \n\nRESPONSIBILITIES  \n\n• Veeva CLM content migration and Testing  \n\n• Bug Fixing  \n\n• Monitoring and data maintenance to a downstream iPad app to CRM \napplication  \n\n• Enhancement in Veeva CLM based application as requested by Client  \n\n• Actively following up and providing updates on issues, maintaining SLA, \n\nhandling escalations, resolving issues with minimum downtime  \n\n  \n\n  \n\n\n\n  \n\nCurriculum Vitae      Shashank Shekhar  \n\n  \n\n5 | P a g e  \n \n\n Organization:   Cognizant Technology Solutions  \n\n  \n\nINDUSTRY  Pharmaceuticals  \n\nCLIENT  PHARMA CLIENT  \n\nDURATION & \n\nLOCATION  \n\nSeptember 2013 – February 2016 \n\nPRIMARY ROLE  Veeva Application support analyst  \n\nPROJECT  \n\nDESCRIPTION  \n\n  \n\nRenowned pharmaceuticals company implemented an integrated  \n\nSalesforce.com based solution to be deployed across EMEA, LATAM and APAC \n\nregions to enable the Sales force to realize full benefits by enabling the \n\nbusiness (sales) processes in the proposed CRM solution.  \n\nROLES &  \n\nRESPONSIBILITIES  \n\n• Veeva Configuration development and handling  \n\n• Bug Fixing and Support  \n\n• Monitoring and data maintenance to a downstream iPad app to CRM \n\napplication  \n\n• Enhancement in Veeva application  \n\n• Incident Management  \n\n• Actively triaging and resolving issues  \n\n• Provided Technical Support to AstraZeneca international sales market \nin EMEA, APAC and LATAM countries  \n\n• Actively following up and providing updates on issues, maintaining SLA, \n\nhandling escalations, resolving issues with minimum downtime  \n\n  \n\n  \n\nINDUSTRY  Pharmaceuticals  \n\nCLIENT  PHARMA CLIENT  \n\nDURATION & \n\nLOCATION  \n\nSeptember 2011 – August 2013 \n\nKolkata   \n\nPRIMARY ROLE  Siebel Developer  \n\nPROJECT  \n\nDESCRIPTION  \n\n  \n\nRenowned pharmaceuticals company implemented an integrated global Siebel \nSFA and SaaS solution to be deployed across EMEA, LATAM and APAC regions to \nenable the Salesforce to realize full benefits by enabling the business (sales) \nprocesses in the proposed CRM solution.  \n\n  \n\nInsight is based on Siebel software– the market leading On-premise in \n\nCustomer Relationship Management systems.  \n\n\n\n  \n\nCurriculum Vitae      Shashank Shekhar  \n\n  \n\n6 | P a g e  \n \n\nROLES &  \n\nRESPONSIBILITIES  \n\n• Siebel Configuration development and handling  \n\n• Bug Fixing and Support  \n\n• Monitoring and data maintenance to a downstream I Pad app to Siebel.  \n\n• Enhancement in Siebel application  \n\n• Incident Management  \n\n• Actively triaging and resolving issues.  \n\n• Provided Technical Support to AstraZeneca international sales market \nin EMEA, APAC and LATAM countries  \n\n• Actively following up and providing updates on issues, maintaining SLA, \n\nhandling escalations, resolving issues with minimum downtime  \n\n   \n\n  \n\n  \n\n  \n\n  \n\nShashank Shekhar              Place: Bangalore","annotation":[{"label":["Name"],"points":[{"start":7614,"end":7629,"text":"Shashank Shekhar"}]},{"label":["Name"],"points":[{"start":7010,"end":7025,"text":"Shashank Shekhar"}]},{"label":["Name"],"points":[{"start":5281,"end":5296,"text":"Shashank Shekhar"}]},{"label":["Name"],"points":[{"start":3628,"end":3643,"text":"Shashank Shekhar"}]},{"label":["Name"],"points":[{"start":2032,"end":2047,"text":"Shashank Shekhar"}]},{"label":["Degree"],"points":[{"start":1800,"end":1806,"text":"M. B. A"}]},{"label":["Email_Address"],"points":[{"start":1579,"end":1602,"text":"coolshashank76@gmail.com"}]},{"label":["Mobile_No"],"points":[{"start":1551,"end":1564,"text":"+91 9831185263"}]},{"label":["Mobile_No"],"points":[{"start":1534,"end":1548,"text":"+91 9674182742 "}]},{"label":["Certifications"],"points":[{"start":1463,"end":1496,"text":"Salesforce Certified Administrator"}]},{"label":["Certifications"],"points":[{"start":1430,"end":1458,"text":"Veeva Certified Administrator"}]},{"label":["Tools"],"points":[{"start":1409,"end":1421,"text":"SQL Developer"}]},{"label":["Tools"],"points":[{"start":1392,"end":1406,"text":"Toad for Oracle"}]},{"label":["Tools"],"points":[{"start":1380,"end":1389,"text":"Siebel 7.x"}]},{"label":["Skills"],"points":[{"start":1330,"end":1339,"text":"JavaScript"}]},{"label":["Skills"],"points":[{"start":1325,"end":1327,"text":"CSS"}]},{"label":["Skills"],"points":[{"start":1319,"end":1322,"text":"HTML"}]},{"label":["Skills"],"points":[{"start":1290,"end":1293,"text":"Java"}]},{"label":["Skills"],"points":[{"start":1284,"end":1287,"text":"SOQL"}]},{"label":["Skills"],"points":[{"start":1279,"end":1281,"text":"SQL"}]},{"label":["Operating_Systems"],"points":[{"start":1209,"end":1215,"text":"Windows"}]},{"label":["Name"],"points":[{"start":1124,"end":1139,"text":"Shashank Shekhar"}]},{"label":["Certifications"],"points":[{"start":362,"end":390,"text":"Veeva Certified Administrator"}]},{"label":["Years_of_Experience"],"points":[{"start":122,"end":131,"text":"60+ months"}]},{"label":["Name"],"points":[{"start":67,"end":82,"text":"Shashank Shekhar"}]},{"label":["Name"],"points":[{"start":22,"end":37,"text":"Shashank Shekhar"}]}],"extras":null,"metadata":{"first_done_at":1564051289000,"last_updated_at":1564051289000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "Shiv Pratap Singh Bhadauria \n\nDurga Vihar Colony, \nCentral Jail Road, Mobile: +91-8884173157 \nGwalior,Madhya Pradesh Email: ShivPratapSinghBhadauria@gmail.com \n\nOBJECTIVE \nTo become a valued and respected professional and contribute as a cohesive team member by aligning my goals in consonance with the corporate objectives of the organization. \n\nSYNOPSIS \n\n• 6 years of IT experience in analysis, design and development of applications using Salesforce.com CRM, Servicemax, Veeva CRM, Force.com platform with proficiency as a developer and have worked in deployment activities in multiple environments. \n• Presently associated with General Electric, Bengaluru as a Senior Application Developer. \n• Sound ability to develop strategies to progress from a verbal problem statement to an innovative Salesforce application. \n• Excellent & Timely decision making skills with a sharp positive approach and with a “Never Give Up” attitude. \n• Strong believer & follower of Team work & Timely delivery with utmost quality. \n• Self-motivated and Self-driven towards the solutions and ability to learn new technologies to bring innovation in it. \n\nEDUCATION \n\n• MCA (Masters of Computer Applications), VIT University, Vellore with 8.2 CGPA in 2013. \n• BCA (Bachelors of Computer Applications), GICTS College, Gwalior with 67.8% in 2009. \n• XII, Govt. Gorkhi Higher Secondary School, Gwalior (Affiliated to Board of Secondary Education , Bhopal) with 61% in 2005. \n• X, National Convent Higher Secondary School, Gwalior (Affiliated to Board of Secondary Education , Bhopal ) with 73.2% in 2003. \n\nTECHNICAL SKILLS \n\nSalesforce.com:\n Apex, Visual Force, SOQL, SOSL, Apex Triggers, Workflows, Approvals, Email Templates, Formulas, Validation Rules,auto-response rules, Email-to-Case, Assignment rules, Custom settings, Eclipse, Force.com Explorer, Apex Data Loader, Work Bench , Process Builder,Translation workbench,Territory management. \n\nVeeva: \nApex, Visual Force, Call management , Order management,SOQL, SOSL, Apex Triggers, Workflows, Approvals, Email Templates, Formulas, Validation Rules,assignment rules, Eclipse, Force.com Explorer, Apex Data Loader, Work Bench , Process Builder. \n\n\n\nServicemax: \nServicemax SFM transactions , SFM Wizards , SFM Custom actions , SFM screen design,SFM profile configuration,SFM mappings,Dispatch Console , Servicemax Setup configuration. \n\n\nIDE: Eclipse \nOperating Systems: Windows XP/Vista/7/8/10. \n\nCertifications: \n· Salesforce Certified Platform Developer 1\n· Salesforce Certified Administrator \n· Salesforce Certified Platform App Builder \n· Servicemax Certified Administrator \n\nWORK HISTORY \n\nOrganization: General Electric, Bengaluru. \nTenure: May 2018 – Present. \nJob Title: Senior Application Developer \nWork Summary: \n\n• End to end design and development of a SFDC. \n• Developed critical reusable modules for usage across projects. \n• Got hands on experience of Requirement Analysis, Estimation, Development and Post production support of SFDC. \n\nPROJECT PORTFOLIO \n\nProject Title : Baker Hughes \nClient : Baker Hughes \nDuration : May 2018 - Present \nRole : Senior Application Developer \nEnvironment : Salesforce.com, Servicemax , Veeva\n\nDescription : The purpose of the Baker Hughes project was to provide a complete platform for Baker Hughes Sales Division starting from Pre-Sales to After Sales service. Enhancements & adding new functionalities and maintenance of the code was performed in this project. \n\nRole: Senior Application Developer \n• Designed, developed and deployed Apex Classes, Controller Classes, Batch class and Apex Triggers for various functional needs in the application. \n• Created multiple visual force pages for various requirement needs. \n• Designed multiple SFM screens for various business requirements using standard Servicemax SFM transaction functionality. \n• Configured the SFM mappings & availability of SFM screens based on record criteria. \n• Implemented multiple data validation rules enforcement on the SFM screen feature. \n• Created various test classes to ensure code coverage needed for the successful deployment process. \n• Integrated Salesforce application with another SAP Next Connect system using REST API. \n• Used SOQL & SOSL for data fetching and manipulation needs of the application using platform database objects. \n• Implemented Security access to the user profiles by creating Object level security, field level security, and record level security. \n• Implemented various Business logics with the use of Process Builder ,Workflow rules and perform relevant operations with the use of it. \n• Implemented various complex business approval logics and created single approval process, parallel approval process for Quote approval , Custom Product approval functionality. \n• Created various auto-response rules , assignment rules , Email to Case for Case management process. \n• Experience in using Apex Data Loader, workbench, developer console , force.com explorer for various data related operations like data inserting, updating, upserting, deleting, exporting. \n• Worked on creating custom E-Mail templates like Mass Email, Email Templates, and Communication Templates to be sent out to customers. \n\nOrganization: Accenture Solutions Private Ltd, Bengaluru. \nTenure: May 2013 – May 2018. \nJob Title: Application Development Senior Analyst \nWork Summary: \n\n• End to end design and development of a SFDC using Veeva CRM. \n• Developed critical reusable modules for usage across projects. \n• Got hands on experience of Requirement Analysis, Estimation, Development and Post production support of SFDC. \n\nPROJECT PORTFOLIO \n\nProject Title: Merck Millipore \nClient: Merck \nDuration: Feb 2014 – May 2018 \nRole: Senior Application Developer \nEnvironment: Salesforce.com, Servicemax , Veeva CRM\n\nDescription: The purpose of the Merck Millipore project was to provide a complete platform for Merck Millipore Sales Division starting from Pre-Sales to After Sales service. Enhancements, adding new functionalities and maintenance of the code was performed in this project. \n\nRole: Senior Application Developer\n \n• Designed, developed and deployed Apex Classes, Controller Classes,Batch class and Apex Triggers for various functional needs in the application. \n• Created multiple visual force pages for various requirement needs. \n• Designed multiple SFM screens for various business requirements using standard servicemax SFM transaction functionality. \n• Configured the SFM mappings & availability of SFM screens based on record criteria. \n• Implemented multiple data validation rules enforcement on the SFM screen feature. \n• Created various test classes to ensure code coverage needed for the successful deployment process. \n• Integrated Salesforce application with another SAP Next Connect system using REST API. \n• Used SOQL & SOSL for data fetching and manipulation needs of the application using platform database objects. \n• Implemented Security access to the user profiles by creating Object level security, field level security, and record level security. \n• Implemented various Business logics with the use of Process Builder ,Workflow rules and perform relevant operations with the use of it. \n• Implemented various complex business approval logics and created single approval process, parallel approval process for Quote approval , Custom Product approval functionality. \n• Created various auto-response rules , assignment rules , Email to Case for Case management process. \n• Experience in using Apex Data Loader, Workbench ,Developer console , force.com explorer for various data related operations like data inserting, updating, upserting, deleting, exporting. \n• Worked on creating custom E-Mail templates like Mass Email, Email Templates, and Communication Templates to be sent out to customers. \n\nProject Title : Pfizer WBB \nClient : Pfizer \nDuration : July 2013 – Jan 2014 \nRole : Application Developer \nEnvironment : Salesforce.com, Veeva CRM \nDescription : The purpose of the Pfizer WBB project was to provide a complete platform for Pfizer Sales Division starting from Pre-Sales to After Sales service. Enhancements, adding new functionalities in the code was performed in this project. \n\nRole: Software Developer \n\n• Designed, developed and deployed Apex Classes, Controller Classes, Batch class and Apex Triggers for various functional needs in the application. \n• Worked on Call management & Order management module of Veeva. \n• Implementation of Translations for various languages using Translation workbench. \n• Used SOQL & SOSL for data fetching and manipulation needs of the application using platform database objects. \n• Implemented Security access to the user profiles by creating Object level security, field level security, and record level security. \n• Implemented various complex business approval logics and created single approval process for DCR approval , Order approval functionality. \n• Created various auto-response rules , assignment rules , Email to Case for Case management process. \n• Created multiple visual force pages for various requirement needs. \n• Experience in using Apex Data Loader,workbench,Force.com explorer for various data related operations like data inserting, updating, upserting, deleting, exporting. \n• Worked on creating custom E-Mail templates like Mass Email, Email Templates, and Communication Templates to besent out to customers. \n\nACHIEVEMENTS \n• Well appreciated by client for excellent performance and timely delivery of projects. \n• Successfully automated Salesforce user account management for multiple Business markets which \nSmoothens the business process to a greater extent. \n• Awarded by Accenture’s highest individual award for excellence for successful and timely delivery of critical modules to business and was solely involved in it. \n• Appreciated by client as one of the top performer of year 2017 for timely & successful delivery. \n• Awarded by Organization multiple time as star performer for taking end to end ownership and delivering complex requirements as per the time line. \n\n(Shiv Pratap Singh Bhadauria)","annotation":[{"label":["Name"],"points":[{"start":10054,"end":10080,"text":"Shiv Pratap Singh Bhadauria"}]},{"label":["Skills"],"points":[{"start":9311,"end":9325,"text":"Email Templates"}]},{"label":["Skills"],"points":[{"start":9103,"end":9118,"text":"Apex Data Loader"}]},{"label":["Skills"],"points":[{"start":8948,"end":8963,"text":"assignment rules"}]},{"label":["Skills"],"points":[{"start":8926,"end":8944,"text":"auto-response rules"}]},{"label":["Skills"],"points":[{"start":8532,"end":8535,"text":"SOSL"}]},{"label":["Skills"],"points":[{"start":8525,"end":8528,"text":"SOQL"}]},{"label":["Skills"],"points":[{"start":8494,"end":8514,"text":"Translation workbench"}]},{"label":["Skills"],"points":[{"start":8425,"end":8429,"text":"Veeva"}]},{"label":["Skills"],"points":[{"start":8398,"end":8413,"text":"Order management"}]},{"label":["Skills"],"points":[{"start":8380,"end":8394,"text":"Call management"}]},{"label":["Skills"],"points":[{"start":8304,"end":8316,"text":"Apex Triggers"}]},{"label":["Skills"],"points":[{"start":8254,"end":8257,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":7934,"end":7938,"text":"Veeva"}]},{"label":["Skills"],"points":[{"start":7918,"end":7931,"text":"Salesforce.com"}]},{"label":["Skills"],"points":[{"start":7720,"end":7734,"text":"Email Templates"}]},{"label":["Skills"],"points":[{"start":7490,"end":7505,"text":"Apex Data Loader"}]},{"label":["Skills"],"points":[{"start":7405,"end":7420,"text":"assignment rules"}]},{"label":["Skills"],"points":[{"start":7383,"end":7401,"text":"auto-response rules"}]},{"label":["Skills"],"points":[{"start":7101,"end":7115,"text":"Process Builder"}]},{"label":["Skills"],"points":[{"start":6812,"end":6815,"text":"SOSL"}]},{"label":["Skills"],"points":[{"start":6805,"end":6808,"text":"SOQL"}]},{"label":["Skills"],"points":[{"start":6451,"end":6462,"text":"SFM mappings"}]},{"label":["Skills"],"points":[{"start":6176,"end":6188,"text":"Apex Triggers"}]},{"label":["Skills"],"points":[{"start":6127,"end":6130,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":5768,"end":5772,"text":"Veeva"}]},{"label":["Skills"],"points":[{"start":5755,"end":5764,"text":"Servicemax"}]},{"label":["Skills"],"points":[{"start":5739,"end":5752,"text":"Salesforce.com"}]},{"label":["Skills"],"points":[{"start":5400,"end":5404,"text":"Veeva"}]},{"label":["Skills"],"points":[{"start":5116,"end":5130,"text":"Email Templates"}]},{"label":["Skills"],"points":[{"start":4886,"end":4901,"text":"Apex Data Loader"}]},{"label":["Skills"],"points":[{"start":4801,"end":4816,"text":"assignment rules"}]},{"label":["Skills"],"points":[{"start":4779,"end":4797,"text":"auto-response rules"}]},{"label":["Skills"],"points":[{"start":4497,"end":4511,"text":"Process Builder"}]},{"label":["Skills"],"points":[{"start":4208,"end":4211,"text":"SOSL"}]},{"label":["Skills"],"points":[{"start":4201,"end":4204,"text":"SOQL"}]},{"label":["Skills"],"points":[{"start":3847,"end":3858,"text":"SFM mappings"}]},{"label":["Skills"],"points":[{"start":3787,"end":3796,"text":"Servicemax"}]},{"label":["Skills"],"points":[{"start":3572,"end":3584,"text":"Apex Triggers"}]},{"label":["Skills"],"points":[{"start":3522,"end":3525,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":3172,"end":3176,"text":"Veeva"}]},{"label":["Skills"],"points":[{"start":3159,"end":3168,"text":"Servicemax"}]},{"label":["Skills"],"points":[{"start":3143,"end":3156,"text":"Salesforce.com"}]},{"label":["Certifications"],"points":[{"start":2578,"end":2611,"text":"Servicemax Certified Administrator"}]},{"label":["Certifications"],"points":[{"start":2533,"end":2574,"text":"Salesforce Certified Platform App Builder "}]},{"label":["Certifications"],"points":[{"start":2495,"end":2528,"text":"Salesforce Certified Administrator"}]},{"label":["Certifications"],"points":[{"start":2451,"end":2491,"text":"Salesforce Certified Platform Developer 1"}]},{"label":["Skills"],"points":[{"start":2377,"end":2383,"text":"Eclipse"}]},{"label":["Skills"],"points":[{"start":2337,"end":2367,"text":"Servicemax Setup configuration."}]},{"label":["Skills"],"points":[{"start":2318,"end":2333,"text":"Dispatch Console"}]},{"label":["Skills"],"points":[{"start":2305,"end":2316,"text":"SFM mappings"}]},{"label":["Skills"],"points":[{"start":2279,"end":2303,"text":"SFM profile configuration"}]},{"label":["Skills"],"points":[{"start":2261,"end":2277,"text":"SFM screen design"}]},{"label":["Skills"],"points":[{"start":2240,"end":2257,"text":"SFM Custom actions"}]},{"label":["Skills"],"points":[{"start":2226,"end":2236,"text":"SFM Wizards"}]},{"label":["Skills"],"points":[{"start":2196,"end":2222,"text":"Servicemax SFM transactions"}]},{"label":["Skills"],"points":[{"start":2183,"end":2192,"text":"Servicemax"}]},{"label":["Skills"],"points":[{"start":2162,"end":2176,"text":"Process Builder"}]},{"label":["Skills"],"points":[{"start":2149,"end":2158,"text":"Work Bench"}]},{"label":["Skills"],"points":[{"start":2131,"end":2146,"text":"Apex Data Loader"}]},{"label":["Skills"],"points":[{"start":2111,"end":2128,"text":"Force.com Explorer"}]},{"label":["Skills"],"points":[{"start":2102,"end":2108,"text":"Eclipse"}]},{"label":["Skills"],"points":[{"start":2084,"end":2099,"text":"assignment rules"}]},{"label":["Skills"],"points":[{"start":2067,"end":2082,"text":"Validation Rules"}]},{"label":["Skills"],"points":[{"start":2057,"end":2064,"text":"Formulas"}]},{"label":["Skills"],"points":[{"start":2040,"end":2054,"text":"Email Templates"}]},{"label":["Skills"],"points":[{"start":2029,"end":2037,"text":"Approvals"}]},{"label":["Skills"],"points":[{"start":2018,"end":2026,"text":"Workflows"}]},{"label":["Skills"],"points":[{"start":2003,"end":2015,"text":"Apex Triggers"}]},{"label":["Skills"],"points":[{"start":1997,"end":2000,"text":"SOSL"}]},{"label":["Skills"],"points":[{"start":1991,"end":1994,"text":"SOQL"}]},{"label":["Skills"],"points":[{"start":1974,"end":1989,"text":"Order management"}]},{"label":["Skills"],"points":[{"start":1956,"end":1970,"text":"Call management"}]},{"label":["Skills"],"points":[{"start":1942,"end":1953,"text":"Visual Force"}]},{"label":["Skills"],"points":[{"start":1936,"end":1939,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":1928,"end":1932,"text":"Veeva"}]},{"label":["Skills"],"points":[{"start":1904,"end":1923,"text":"Territory management"}]},{"label":["Skills"],"points":[{"start":1882,"end":1902,"text":"Translation workbench"}]},{"label":["Skills"],"points":[{"start":1866,"end":1880,"text":"Process Builder"}]},{"label":["Skills"],"points":[{"start":1853,"end":1862,"text":"Work Bench"}]},{"label":["Skills"],"points":[{"start":1835,"end":1850,"text":"Apex Data Loader"}]},{"label":["Skills"],"points":[{"start":1815,"end":1832,"text":"Force.com Explorer"}]},{"label":["Skills"],"points":[{"start":1806,"end":1812,"text":"Eclipse"}]},{"label":["Skills"],"points":[{"start":1789,"end":1803,"text":"Custom settings"}]},{"label":["Skills"],"points":[{"start":1771,"end":1786,"text":"Assignment rules"}]},{"label":["Skills"],"points":[{"start":1756,"end":1768,"text":"Email-to-Case"}]},{"label":["Skills"],"points":[{"start":1735,"end":1753,"text":"auto-response rules"}]},{"label":["Skills"],"points":[{"start":1718,"end":1733,"text":"Validation Rules"}]},{"label":["Skills"],"points":[{"start":1708,"end":1715,"text":"Formulas"}]},{"label":["Skills"],"points":[{"start":1691,"end":1705,"text":"Email Templates"}]},{"label":["Skills"],"points":[{"start":1680,"end":1688,"text":"Approvals"}]},{"label":["Skills"],"points":[{"start":1669,"end":1677,"text":"Workflows"}]},{"label":["Skills"],"points":[{"start":1654,"end":1666,"text":"Apex Triggers"}]},{"label":["Skills"],"points":[{"start":1648,"end":1651,"text":"SOSL"}]},{"label":["Skills"],"points":[{"start":1642,"end":1645,"text":"SOQL"}]},{"label":["Skills"],"points":[{"start":1628,"end":1639,"text":"Visual Force"}]},{"label":["Skills"],"points":[{"start":1622,"end":1625,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":1605,"end":1618,"text":"Salesforce.com"}]},{"label":["Degree"],"points":[{"start":1152,"end":1189,"text":"MCA (Masters of Computer Applications)"}]},{"label":["Skills"],"points":[{"start":475,"end":479,"text":"Veeva"}]},{"label":["Skills"],"points":[{"start":463,"end":472,"text":"Servicemax"}]},{"label":["Skills"],"points":[{"start":443,"end":456,"text":"Salesforce.com"}]},{"label":["Years_of_Experience"],"points":[{"start":360,"end":366,"text":"6 years"}]},{"label":["Email_Address"],"points":[{"start":124,"end":158,"text":"ShivPratapSinghBhadauria@gmail.com "}]},{"label":["Mobile_No"],"points":[{"start":78,"end":91,"text":"+91-8884173157"}]},{"label":["Name"],"points":[{"start":0,"end":26,"text":"Shiv Pratap Singh Bhadauria"}]}],"extras":null,"metadata":{"first_done_at":1564049958000,"last_updated_at":1564049958000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "Shivani Jaiswal\nBengaluru\n\n+919535081927\nshivanijaiswal417@gmail.com\n\nSummary\n\nEducation\n\nEmployment History\n\nSix years of expertise in Tableau, Informatica ETL Tool (Informatica Power Center 9.X), Informatica Cloud, ICRT  and MS Excel in a\nData Warehouse environment and SQL\n\nGood experience in Process, Guides and Service Connector in ICRT\n\nWorked on Telecom and Health domain\n\nKnowledge in Salesforce which requires for Informatica Cloud debugging\n\nData Integration Platform - Informatica Cloud service (with all relevant connectors)\n\nExperience in performance tuning ETL mappings and high-volume databases\n\nExtensive experience in Design & Development of Mappings, Sessions and Workflows\n\nWorked on troubleshooting the mappings for improving performance by identifying bottlenecks\n\nDeveloped advanced charts like Waffle chart,Lollipop Chart, Barcode Chart,Donut Chart,Gauge Chart,Dynamic Histogram,Butterfly\nChart, Bursting Bubble Chart,Bullet Chart, Sparkline,Strip Chart etc.\n\nAbility to work on multiple projects/tasks simultaneously and still commit to stringent deadlines; thereby enabling good business, right\nfirst time deliverable and client trust\n\nIIMT Institute of Engineering & Technology\nB.Tech (Information Technology)\n\nMeerut, Uttar Pradesh\nGraduated May 2012\n\nCapgemini Technology Services India Limited\nConsultant\n\nBengaluru, Karnataka\nSeptember 2016 - Present\n\nProject Name- BITMAN\n\nBITMAN application will reduce the effort to manage User, Territory and ProductManagement Business Processes. The application is\nglobal, which means it can be utilised for all the regions/Orgs as well as the change management will be more manageable.\n\nContributions\n\n1. Working with advanced visualization software Tableau  to create meaningful graphical displays of data in reports. \n2. Involved in designing, developing and documenting of the ETL (Extract, Transformation and Load) strategy to populate the\n\ndata from various source systems feeds using Informatica Cloud and Power Center.\n3. Working with the data team in the data preparation, data quality, reporting, and analytic tasks required to meet business\n\nneeds and requirements. \n4. Created ICRT process to read /write SFDC metadata through REST API's.Created ICRT process to read /write Oracle tables data\n\nthrough connectors.\n5. Developed code in ICRT using process, guide, REST API's of Informatica cloudCreated Data Replication Task to replicate the\n\nexact structure of Salesforce objects to Oracle database.\n6. Involved in creating stored procedures, triggers, functions and schedulers\n\n\n\nProfessional Skills\n\nAwards & Recognition\n\nProject Name- BI-ODSBI (Boehringer Ingelheim) \n\nA customer-focused business providing a differentiated basket of pharmaceutical products and services to institutional customers,\nwhich include group purchasing organisations, wholesalers, hospitals, surgical and radiology services.\n\nContributions\n1. Supporting current production issues and bug fixes.\n2. Generate weekly monthly management reports, metric reporting and issue tracking.\n3. Designed mappings from the scratch and also did performance tuning of the mappings that were previously implemented. \n4. Implemented Apex Data Loader to load data into Salesforce.Worked with session logs, Informatica Debugger, and\n\nPerformance logs for error handling when we had workflows and session fails.\n\nVirtusa Software Services\nEngineer - Technology\n\nChennai, Tamil Nadu\nMarch 2013 - September 2016\n\nProject Name - GSEDWEDW \n\nThis project  deals with the Lead to Cash and Trouble to resolve related entities so that customers experience measures are calculated\nat reporting end. The data comes from many components of L2c and T2R components which help the user to get the detailed\ninformation of their order journey and the issues they faced. The Database has lots of historical data.\n\nContributions\n\n1. Understand and analyse the business scope and technical & functional specification documents.\n2. Extracted data from sources such as Oracle/Flat Files, Transformed as per mapping specification documents and loaded into\n\nOracle DWHImplement the SCD type1 and SCD type2 to keep track of historical dataUsed the mapping parameter to make the\nmapping more flexible.\n\nProject Name - DECOMP\n\nBT MPLS systems have no concept of Service package built into it. They only have ServiceVPNs, SAF's and Resilience. Therefore\ndecomposition of the order using the rules will need to be done before placing an order for BT MPLS on the eDCA. These rules are\npredefined and are in DECOMP and keep on changing as per the Business requirements. \n\nContributions\n1. Preparation of analysis, design and business documents.\n2. Developed dashboard on Tableau and Publishing on Tableau server.\n3. Troubleshooting the existing PL/SQL procedures, functions, packages. Resolving user issues. Monitoring the jobs, Schedulers\n\nand Interfaces running to ensure proper functioning.\n\nInformatica cloud Competent\nICRT Intermediate\nSQL Competent\nPL/SQL Intermediate\nUnix Scripting Intermediate\nTableau Intermediate\n\nReceived a Certificate of Exemplary Performance in Virtusa Software Services Pvt. Ltd. in 2016","annotation":[{"label":["Skills"],"points":[{"start":5011,"end":5030,"text":"Tableau Intermediate"}]},{"label":["Skills"],"points":[{"start":4983,"end":5009,"text":"Unix Scripting Intermediate"}]},{"label":["Skills"],"points":[{"start":4963,"end":4981,"text":"PL/SQL Intermediate"}]},{"label":["Skills"],"points":[{"start":4949,"end":4961,"text":"SQL Competent"}]},{"label":["Skills"],"points":[{"start":4931,"end":4947,"text":"ICRT Intermediate"}]},{"label":["Skills"],"points":[{"start":3209,"end":3218,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":2440,"end":2449,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":2356,"end":2372,"text":"Informatica cloud"}]},{"label":["Skills"],"points":[{"start":1959,"end":1975,"text":"Informatica Cloud"}]},{"label":["Degree"],"points":[{"start":1204,"end":1209,"text":"B.Tech"}]},{"label":["Skills"],"points":[{"start":571,"end":582,"text":"ETL mappings"}]},{"label":["Skills"],"points":[{"start":480,"end":496,"text":"Informatica Cloud"}]},{"label":["Skills"],"points":[{"start":423,"end":439,"text":"Informatica Cloud"}]},{"label":["Skills"],"points":[{"start":393,"end":402,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":198,"end":214,"text":"Informatica Cloud"}]},{"label":["Years_of_Experience"],"points":[{"start":110,"end":118,"text":"Six years"}]},{"label":["Email_Address"],"points":[{"start":41,"end":67,"text":"shivanijaiswal417@gmail.com"}]},{"label":["Mobile_No"],"points":[{"start":27,"end":39,"text":"+919535081927"}]},{"label":["Name"],"points":[{"start":0,"end":14,"text":"Shivani Jaiswal"}]}],"extras":null,"metadata":{"first_done_at":1564055886000,"last_updated_at":1564055886000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "SIVA \nEmail-id : ssfdccom@gmail.com\nMobile No: +91 9948305355\n\n\tProfessional Summary\n\n\n• Around 5 Years of Experience in IT Industry and around 3+ years of experience in Salesforce Implementation, Development, and Customization, Lightning and over 1+ Year of Experience in  Testing.\n• Extensive experience in developing Apex Classes, Triggers, Visual force pages, writing Workflows, Force.com API.\n•Worked with Lightning components.\n•Worked with Lightning Design System.\n• Experience in Communities.\n• Extensive Experience in Process Builders, Workflows.\n• Experience in creating Flows.\n• Experienced in analyzing business requirements and implementing them to Salesforce custom objects, master-detail relationships, lookup relationships.\n\n• Experienced in Creating Roles, Profiles, Page Layouts, Workflow Alerts and Actions and Approval Workflow.\n\n• Experience in Importing & Exporting the data with the help of Import Wizard & Data Loader.\n\n• Worked extensively on various salesforce.com standard objects like Accounts, Contacts, Cases, Opportunities, and Products.\n\n• Experience in Deployments through Change Sets, Eclipse.\n\n• Experience in Service Cloud , Cases, Entitlements, Milestones, Live Agent, Omni Channels, Snap – Ins.\n• Proficiency in SFDC Administrative tasks like creating Profiles, Roles, Users, Page Layouts, Record Types, Email alerts, Reports, Dashboards and Tasks.\n\n•Worked with Sesame RDB Middle-ware for Migrating the Bulk Data from one org to another.\n\tEducational Qualification\n\n\nB.TECH from JNTU, KAKINADA - 2012.\n\tTechnical Expertise:\n\n\n· CRM\n                        : \n Salesforce.com CRM.\n\n· SFDC Technologies\n: \n Apex, Visual force, Triggers, workflow & Approvals.\n· Web Technologies\n: \n HTML, CSS and Java Script.\n· Tools\n\n\n: \n Force.com,IDE(Eclipse), DataLoader, Force.com Explorer.\n\tExperience Summary\n\n\nWorked with “Abhyudaya HR Business Solutions Pvt Ltd “  as a Software Engineer, Hyderabad from 2014 to 2016.\n\nWorked with “Tech Mahindra ” as a Sr .Software Engineer , Hyderabad from 2016 to 2018.\nWorking with L@T Infotech (LTI) Bangalore as a Associate Consultant from s2018 to Till Date.\n\tProject Summery\n\n\nProject #04(LTI):\nRole                                         :   Salesforce Administration/ Developer/Lightning.\nClient\n               :    UTC\nProject\n             :     United Technologies Carrier.\nDescription:\nUnited Technologies Corporation (UTC) is an American multinational conglomerate, it is engaged in providing high technology products and services to the building systems and aerospace industries around the world.\nResponsibilities:\n· Created Formula Fields, workflows and approvals\n· Created master-detail relationships, lookup relationships.\n\n· Worked on bulk imports and exports of data using Apex Data Loader and Import Wizard for regular data loads and backup purposes.\n\n· Writing and developing Apex Classes, Triggers, custom Email templates, custom visual force components, Configuring Custom Object, fields, Workflow rules, Custom Settings and Validation Rules.\n· Experience in Building Lightning Components.\n\n· Experience in Lightning Design system.\n\n· Worked with Guided Actions for Cases.\n· Worked with Communities.\n\n· Writing Batch classes.\n· Loading the Bulk Data through Data Loader and Import wizards.\nProject #03(Tech Mahindra):\nRole                                         :   Salesforce Administration and Developer\nClient\n               :    Citi Bank.\n\nProject\n             :     Citi Commercial Bank\n\nDescription:\nCitibank is the consumer division of financial services multinational Citigroup. Citibank was founded in 1812 as the City Bank of New York, later First National City Bank of New York. Citibank provides credit cards, mortgages, personal loans, commercial loans, and lines of credit.\n\nResponsibilities:\n· Worked on bulk imports and exports of data using Apex Data Loader and Import Wizard for regular data loads and backup purposes.\n\n· Production Support.\n· Writing and developing Apex Classes, Triggers, custom Email templates, custom visual force components, Configuring Custom Object, fields, Workflow rules, Custom Settings and Validation Rules.\n· Created Formula Fields, workflows and approvals\n         Created master-detail relationships, lookup relationships\n· Used SOQL Queries with consideration to Governor Limits for data manipulation needs of the application using platform database objects.\n\nProject #02(Tech Mahindra):\nRole                                         :   Salesforce Administration and Developer\nClient\n               :   General Electric.\nProject\n             :   Renewables Salesforce Carve-Out\nDescription:\nGE is the world’ Digital Industrial Company, transforming industry with software-defined machines and solutions that are connected, responsive and predictive.\n\nWith the transition of GE Renewable Energy to a Tier 1 business, there is a strategic push to ensure that the Renewable Energy is not dependent on GE Power for its IT systems/infrastructure. Salesforce has been identified as a business critical application for the business and Renewable Energy needs to separate from GE Power to create its own instance.\n\nResponsibilities:\n· Writing and developing Apex Classes, Triggers, custom email templates, custom visual force components, Configuring Custom Object, fields, Workflow rules, Custom Settings, Field Sets, and Validation Rules.\n\n· Created Formula Fields, Visual flows and approvals\n\n· Created Reports and Dash boards.\n\n· Developed Apex Classes, Controllers, extensions and Apex Triggers for various functional needs in the application.\n\n· Used SOQL Queries with consideration to Governor Limits for data manipulation needs of the application using platform database objects.\n\n· Worked on bulk imports and exports of data using Apex Data Loader and Import Wizard for regular data loads and backup purposes.\n\n· Used middle ware tool called Sesame RDB for bulk Data Migration from one org to another.\n\n· Created master-detail relationships, lookup relationships.\n\n· Used SOQL & SOSL Queries with consideration to Governor Limits for data manipulation needs of the application using platform database objects.\nProject #04(Abhyudaya):\n\n       Role                                 :   Salesforce Admin\\Tester\n       Client\n               :   Baxalta\n       Duration\n\n\n:   July-2015 to Till Date\n\n       Team\n\n\n:   10\n\nDescription:\n\n          Baxalta is a leading biopharmaceutical company advancing innovative therapies in hematology, immunology, and oncology. Baxalta see a world with endless possibilities where our imagination is inspired and harnessed with purpose. As a responsible corporate citizen, Baxalta's commitment to patients extends beyond the local and global communities in which Baxalta operate. Baxalta business is about improving people's lives, and Baxalta is dedicated to creating healthy communities through our corporate responsibility programs.\nResponsibilities:\n\n· Writing and developing Apex Classes, Triggers, Batch processing, custom email templates, custom visual force components, asynchronous jobs, callouts etc.\n\n· Configuring Custom Object, fields, Workflow rules, Custom Settings, Field Sets, and Validation Rules. \n\n· Created Formula Fields, Visual flows and approvals, Process builders for the flexibility and functionality of force platform application.\n\n· Supporting the Sales cloud implementation, Client interfacing and Chatter when converting leads and dealing with cases.\n\n· Worked with development and deployment of code through change sets from sandbox to production environments.\n· Implemented Daily Notification Mailing System using Batch and Schedule Apex.\n\n· Designed and developed Apex Classes, Controllers, extensions and Apex Triggers for various functional needs in the application.\n\n· Used SOQL & SOSL Queries with consideration to Governor Limits for data manipulation needs of the application using platform database objects.\n· Worked on bulk imports and exports of data using Apex Data Loader and Import Wizard for regular data loads and backup purposes.\nProject #1(Abhyudaya): \n\nTitle \n:  Easy Travel System (ETS)\n\nClient\n\n :  Omega, Malaysia.\n\nRole  \n :  Team member.    \n\nEnvironment\n\n:  Servlet, JSP, JDBC\nDescription :     \n\n                   Easy Travel System is a web-based product for the travel industry, which allows corporate clients to make their reservation and booking for all their travel needs which includes flight and hotel bookings. The application provides interface to the online reservation system, which contacts the various Global Distribution Systems for making the reservations and bookings for all the travel needs.\n\nResponsibilities:\n\n· Designed Servlets and JSPs as per the requirements.\n\n· Implemented Validations as per the requirements.\n\n· Implemented Persistence Logic with JDBC.\n\n· Involved in debugging the issues reported by the end users.\n\nPAGE","annotation":[{"label":["Skills"],"points":[{"start":7968,"end":7971,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":7713,"end":7720,"text":"Triggers"}]},{"label":["Skills"],"points":[{"start":7708,"end":7711,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":7666,"end":7669,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":7634,"end":7637,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":6963,"end":6970,"text":"Triggers"}]},{"label":["Skills"],"points":[{"start":6949,"end":6952,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":5769,"end":5772,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":5520,"end":5527,"text":"Triggers"}]},{"label":["Skills"],"points":[{"start":5515,"end":5518,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":5473,"end":5476,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":5202,"end":5209,"text":"Triggers"}]},{"label":["Skills"],"points":[{"start":5188,"end":5191,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":4168,"end":4175,"text":"workflow"}]},{"label":["Skills"],"points":[{"start":3987,"end":3994,"text":"Triggers"}]},{"label":["Skills"],"points":[{"start":3973,"end":3976,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":3846,"end":3849,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":2874,"end":2881,"text":"Triggers"}]},{"label":["Skills"],"points":[{"start":2860,"end":2863,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":2755,"end":2758,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":2618,"end":2625,"text":"workflow"}]},{"label":["Skills"],"points":[{"start":1795,"end":1803,"text":"Force.com"}]},{"label":["Skills"],"points":[{"start":1783,"end":1792,"text":"DataLoader"}]},{"label":["Skills"],"points":[{"start":1769,"end":1780,"text":"IDE(Eclipse)"}]},{"label":["Skills"],"points":[{"start":1759,"end":1767,"text":"Force.com"}]},{"label":["Skills"],"points":[{"start":1732,"end":1742,"text":"Java Script"}]},{"label":["Skills"],"points":[{"start":1724,"end":1726,"text":"CSS"}]},{"label":["Skills"],"points":[{"start":1718,"end":1721,"text":"HTML"}]},{"label":["Skills"],"points":[{"start":1684,"end":1692,"text":"Approvals"}]},{"label":["Skills"],"points":[{"start":1673,"end":1680,"text":"workflow"}]},{"label":["Skills"],"points":[{"start":1663,"end":1670,"text":"Triggers"}]},{"label":["Skills"],"points":[{"start":1649,"end":1660,"text":"Visual force"}]},{"label":["Skills"],"points":[{"start":1643,"end":1646,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":1613,"end":1615,"text":"CRM"}]},{"label":["Skills"],"points":[{"start":1598,"end":1611,"text":"Salesforce.com"}]},{"label":["Skills"],"points":[{"start":1566,"end":1568,"text":"CRM"}]},{"label":["Degree"],"points":[{"start":1505,"end":1510,"text":"B.TECH"}]},{"label":["Skills"],"points":[{"start":383,"end":391,"text":"Force.com"}]},{"label":["Skills"],"points":[{"start":344,"end":355,"text":"Visual force"}]},{"label":["Skills"],"points":[{"start":334,"end":341,"text":"Triggers"}]},{"label":["Skills"],"points":[{"start":320,"end":323,"text":"Apex"}]},{"label":["Years_of_Experience"],"points":[{"start":96,"end":102,"text":"5 Years"}]},{"label":["Mobile_No"],"points":[{"start":47,"end":60,"text":"+91 9948305355"}]},{"label":["Email_Address"],"points":[{"start":17,"end":34,"text":"ssfdccom@gmail.com"}]},{"label":["Name"],"points":[{"start":0,"end":4,"text":"SIVA "}]}],"extras":null,"metadata":{"first_done_at":1564139814000,"last_updated_at":1564139814000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "Contact: +91-8709137258\n    Sohail Anwar                                                      Mail ID: soanwar.a@gmail.com                                                \n\nProfessional Summary:                              \n· Having 6 years of experience in Business Intelligence domain with over 5 years in Informatica Development.\n· Extensive experience as an ETL Developer with Informatica Power Center.\n· Good Experience with Informatica Cloud (IICS).      \n· Worked with heterogeneous data sources like Relational DBs and Flat Files. \n· Experienced in data analysis, development, and data warehousing ETL process and data conversions.\n· Experienced in Full Life Cycle of Data warehouse Application.\n· Emphasized on performance tuning in Informatica Power Center.\n· Experienced with cloud services like AWS and Snowflake.\n· Involved in complex mappings like Slowly Changing Dimensions and Incremental load.    \n· Involved in Requirement gathering and analysis from client and driving status meetings.\n· Strong Knowledge of Software Development Life Cycle (SDLC).\n· Knowledge of both Agile and Waterfall model for Software development process.\n· Having Experience in Banking, Telecom, Life Sciences-Health Care sector.\n\n\nTechnical Skills\n\tOperating System\n\tWindows and Unix\n\n\tETL Tools\n\tInformatica 9.X/10.X, Informatica Intelligent Cloud Service\n\n\tDatabases\n\tOracle, Teradata, SQL Server, Amazon Redshift, Snowflake\n\n\tLanguages\n\tSQL\n\n\n\n\n\n\nAlma Mater:\n· Bachelors of Technology in Electronics & Communication Engineering from Dr. M.G.R. University, Chennai (2008 - 2012)\n· 1 Year Certificate Program in Business Management from IIT Bombay.\n\nCertifications:\n\n· Lean Six Sigma White belt.\n\nAwards & Recognition:\n· Awarded with Rising Star Award for outstanding contribution in customer delivery in I&D for Q4 2015(Capgemini)\n· Awarded BIM Spot Award in 2014(Capgemini)\n\nProfessional Experience:\n\nOrganization                    : Deloitte\nDesignation\t           : Consultant\nPeriod\t\t           : April 17 to till date\n\nPast Professional Experience:\n\nOrganization                    : Capgemini \nDesignation\t           : Consultant\nPeriod\t\t           : October 2012 to March 17\n\nKey Projects:\n\nProject # 1:\nProject Title      : 24 HF Data Lake\nOrganization    :  Deloitte\nClient                 :  24 HR Fitness\nRole                    :  ETL Developer\n\nProject Description:\n24 Hour Fitness is a privately owned and operated fitness center chain headquartered in San Ramon, California. It is the world's largest fitness chain based on memberships and the second in number of clubs, operating 420 clubs in 13 U.S. states with four million clients. 24 HR wants to replace their existing data warehouse with a Cloud data lake using snowflake.\n \n\nResponsibilities and contribution:\n· Involved in Requirement analysis and Preparation of STTM documents.\n· Utilized Informatica Power Center 10.2 in this project. Designed and developed ETL mappings, sessions and work flows for loading the data from source systems to target systems.\n· Built complex Informatica mappings using transformation logic such as Update Strategy, Lookup, Filter, Sorter, Joiner, Aggregator and Expression and also implemented CDC logic.\n· Prepared DDL scripts for tables and modified table structure for existing tables.\n· Performed Unit Testing and validated the data by writing test cases for the developed mappings.\n· Changing the existing Informatica mappings based on requirements.\nTools: Informatica 10.2, Snowflake, Oracle, UNIX, Control M.\n\n\nProject # 2:\nProject Title      : NNI ICM\nOrganization    :  Deloitte\nClient                 :  Novo Nordisk Inc\nRole                    :  ETL Developer\n\nProject Description:\nNovo Nordisk is a global healthcare company with 95 years of innovation and leadership in diabetes care. Novo wants to merge the disparate HCP and Patient Marketing Campaign Management Processes on to a unified state of art platform supported by a common operating model enabling future analytics and OmniChannel strategic business capabilities to the NNI Relationship Marketing and overall NNI commercial organization.\nResponsibilities and contribution:\n· Understand the business requirement as per the client to create the mappings.\n· Developed mappings, transformations using Informatica Intelligent Cloud Services (IICS).\n· Created task flows using mapping task, Decision and Assignment task.\n· Involved in ETL Testing to test the code before moving into production environment.\n· Prepared Mapping document, Technical Design document and SIT Test cases.\n· Involved in End to End Implementation of ETL solution with client.\n\nTools: IICS, SQL Server, Amazon Redshift, S3.\n\nProject # 3:\nProject Title      :  Anthem HCA Mandates APCD \nOrganization    :  Deloitte\nClient                 :  Anthem Inc.\nRole                    :  Informatica Developer\n\nProject Description:\nAnthem Inc. is an American health insurance company founded in the 1940s, prior to 2014 known as WellPoint, Inc. APCD Mandates project requires creation of extract from Data warehouse Edward. Edward is a Teradata legacy system. The Extract include information about eligible members and their claims. The file extracts are send to the state.\nResponsibilities and contribution:\n· Utilized Informatica Power Center 9.6.1 in this project. Designed and developed ETL mappings, sessions and work flows for loading the data from source systems to target systems.\n· Built complex Informatica mappings using transformation logic such as Update Strategy, Lookup, Filter, Sorter, Joiner, Aggregator and Expression.\n· Used Variables and Parameters in mappings and workflows to pass the values between sessions.\n· Prepared DDL scripts for tables and modified table structure for adding partition logic.\n· Performed Unit Testing and validated the data by writing test cases for the developed mappings.\n· Changing the Informatica mappings based on requirements.\nTools: Informatica 9.6, Teradata, UNIX, IP Switch, Reflection, Control M.\n\nProject # 4: \nProject Title      :  Customer Level View project\t\t               \nOrganization    :  Capgemini\t               \nClient                 :  ANZ Bank\nRole                    :  ETL Developer\n\nProject Description:\nThe ANZ Bank is one of the largest banks in the world. The current project is about their Consumer Finance, which includes providing credit cards and personal loans. In order to the business requirement this project is to cater many to many relationships between an ANZ customer and Account and to identify duplicates in ANZ source customer. To store different types of addresses for a customer and also to store history of address changes for an ANZ customer.              \nThey have maintained a data mart by name Customer level view project using Informatica where the data comes from various types of sources like Oracle, MS Excel and flat files. In this project different types of reports are generated for end user support to know about the total sales of the Products and about the interest paying details and month end balance details of the customers.\nResponsibilities and contribution:\n· Extracted Data from Oracle and Flat files using Informatica \n· Design of ETL processes using Informatica tool to transform data from source system to target database\n· Designed and developed various Mappings using Transformations like aggregator, joiner, Lookup, Filter, Source Qualifier and Update Strategy, Stored Procedure\n· Improving the performance of Mappings and sessions\n· Worked with various types of Tasks like Session, Event wait, Command task\n· Monitoring the Data load on Daily basis\n· Involved in unit testing of ETL Applications\n· Reviewed the mappings developed by other team members.\n\nTools: Informatica 9.5, Oracle, Teradata, UNIX.\n\nProject # 5:\nProject Title     :  T-Mobile- BOBI\nOrganization    :  Capgemini\t               \nClient                 :  T-Mobile\nRole                    :  ETL Developer\n\nProject Description:\nBROADBAND OPERATIONS BUSINESS INTELLEGENCE (BOBI) is a data warehouse that integrates the data from various source systems for analyzing the Broadband activities of T-Mobile and provides key business information to make informed decisions. BOBI was created by reengineering of business owned application BIMA (Broadband Internet Metrics & Analysis) as part of legacy BellSouth Broadband Transformation (BBT) program.\n\nResponsibilities and contribution:\n· Utilized Informatica Power Center 9.5 in this project. Designed and developed ETL mappings, sessions and work flows for loading the data from source systems to target systems.\n· Built complex Informatica mappings using transformation logic such as Update Strategy, Lookup, Filter, Sorter, Joiner, Sequence Generator and Router.\n· Involved in development of Stage table mappings using Expression, Filter, Aggregator, Joiner and Lookup.\n· Used Variables and Parameters in mappings and workflows to pass the values between sessions.\n· Used Mapplets in the mappings.\n· Developed complex mappings with Slowly Changing Dimensions Type I and II.\n· Worked with various types of Tasks like Session, Email task.\n· Performed Unit Testing and validated the data by writing test cases for the developed mappings.\n· Changing the Informatica mappings based on requirements.\n· Reviewed the mappings developed by other team members.\nTools: Informatica 9.5, Oracle, Teradata, UNIX, Toad for Oracle, Cisco Tidal.\n\nPersonal Profile:\n      Name \t\t: \tSohail Anwar\n      Languages known\t: \tEnglish, Hindi & Urdu\n      Hobbies                   :            Reading News Paper, Cooking, Social Activities.\n      Address\t\t: \tNo 168 9th F Main, BTM Layout\n\t\t\t  \tStage 1, Bangalore 560029","annotation":[{"label":["Name"],"points":[{"start":9440,"end":9451,"text":"Sohail Anwar"}]},{"label":["Skills"],"points":[{"start":9384,"end":9389,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":9359,"end":9366,"text":"Teradata"}]},{"label":["Skills"],"points":[{"start":9351,"end":9356,"text":"Oracle"}]},{"label":["Tools"],"points":[{"start":9334,"end":9344,"text":"Informatica"}]},{"label":["Tools"],"points":[{"start":9226,"end":9236,"text":"Informatica"}]},{"label":["Tools"],"points":[{"start":8603,"end":8613,"text":"Informatica"}]},{"label":["Tools"],"points":[{"start":8420,"end":8430,"text":"Informatica"}]},{"label":["Skills"],"points":[{"start":7747,"end":7754,"text":"Teradata"}]},{"label":["Skills"],"points":[{"start":7739,"end":7744,"text":"Oracle"}]},{"label":["Tools"],"points":[{"start":7722,"end":7732,"text":"Informatica"}]},{"label":["Tools"],"points":[{"start":7206,"end":7216,"text":"Informatica"}]},{"label":["Tools"],"points":[{"start":7161,"end":7171,"text":"Informatica"}]},{"label":["Skills"],"points":[{"start":7133,"end":7138,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":6833,"end":6838,"text":"Oracle"}]},{"label":["Tools"],"points":[{"start":6765,"end":6775,"text":"Informatica"}]},{"label":["Skills"],"points":[{"start":5940,"end":5947,"text":"Teradata"}]},{"label":["Tools"],"points":[{"start":5923,"end":5933,"text":"Informatica"}]},{"label":["Tools"],"points":[{"start":5872,"end":5882,"text":"Informatica"}]},{"label":["Tools"],"points":[{"start":5441,"end":5451,"text":"Informatica"}]},{"label":["Tools"],"points":[{"start":5256,"end":5266,"text":"Informatica"}]},{"label":["Skills"],"points":[{"start":5072,"end":5079,"text":"Teradata"}]},{"label":["Tools"],"points":[{"start":4824,"end":4834,"text":"Informatica"}]},{"label":["Skills"],"points":[{"start":4648,"end":4662,"text":"Amazon Redshift"}]},{"label":["Skills"],"points":[{"start":4636,"end":4645,"text":"SQL Server"}]},{"label":["Tools"],"points":[{"start":4274,"end":4310,"text":"Informatica Intelligent Cloud Service"}]},{"label":["Skills"],"points":[{"start":3492,"end":3497,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":3481,"end":3489,"text":"Snowflake"}]},{"label":["Tools"],"points":[{"start":3463,"end":3473,"text":"Informatica"}]},{"label":["Tools"],"points":[{"start":3412,"end":3422,"text":"Informatica"}]},{"label":["Tools"],"points":[{"start":3043,"end":3053,"text":"Informatica"}]},{"label":["Tools"],"points":[{"start":2859,"end":2869,"text":"Informatica"}]},{"label":["Certifications"],"points":[{"start":1663,"end":1687,"text":"Lean Six Sigma White belt"}]},{"label":["Skills"],"points":[{"start":1433,"end":1435,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":1410,"end":1418,"text":"Snowflake"}]},{"label":["Skills"],"points":[{"start":1393,"end":1407,"text":"Amazon Redshift"}]},{"label":["Skills"],"points":[{"start":1381,"end":1390,"text":"SQL Server"}]},{"label":["Skills"],"points":[{"start":1371,"end":1378,"text":"Teradata"}]},{"label":["Skills"],"points":[{"start":1363,"end":1368,"text":"Oracle"}]},{"label":["Tools"],"points":[{"start":1312,"end":1348,"text":"Informatica Intelligent Cloud Service"}]},{"label":["Tools"],"points":[{"start":1290,"end":1300,"text":"Informatica"}]},{"label":["Skills"],"points":[{"start":1279,"end":1287,"text":"ETL Tools"}]},{"label":["Operating_Systems"],"points":[{"start":1272,"end":1275,"text":"Unix"}]},{"label":["Operating_Systems"],"points":[{"start":1260,"end":1266,"text":"Windows"}]},{"label":["Skills"],"points":[{"start":815,"end":823,"text":"Snowflake"}]},{"label":["Tools"],"points":[{"start":742,"end":752,"text":"Informatica"}]},{"label":["Tools"],"points":[{"start":430,"end":440,"text":"Informatica"}]},{"label":["Tools"],"points":[{"start":381,"end":391,"text":"Informatica"}]},{"label":["Tools"],"points":[{"start":308,"end":318,"text":"Informatica"}]},{"label":["Years_of_Experience"],"points":[{"start":233,"end":239,"text":"6 years"}]},{"label":["Email_Address"],"points":[{"start":103,"end":121,"text":"soanwar.a@gmail.com"}]},{"label":["Name"],"points":[{"start":28,"end":39,"text":"Sohail Anwar"}]},{"label":["Mobile_No"],"points":[{"start":9,"end":22,"text":"+91-8709137258"}]}],"extras":null,"metadata":{"first_done_at":1564206624000,"last_updated_at":1564206624000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "RESUME\n\nSravani Gadde                                                                                     sravanigadde.SF@gmail.com\n:+91 9886916030\n\n\n\nProfessional Experience Summary \n\n\nCertified SalesForce Platform Developer 1 with total 9+ Years of IT experience. Working as Salesforce Developer from 4+ years in Sales Cloud and 7+ years as an Oracle Senior Application Developer in phases of Product Development in modules of Oracle Ebusiness Suite Application(R12) and Oracle Fusion Projects. \n\nHaving technical knowledge in SalesForce Development, Apex Coding, Visual Force pages development, Core Java, Oracle OAF and Oracle ADF Basics. \n\n\nTechnical Skills\n\tSalesForce:\n\tApex Triggers, Visual Force Pages, Workflow, DataLoader\n\n\tOracle Applications:\n\tOracle Applications 12.x(Financials) and Fusion\n\n\tLanguages:\n\tSQL, PL/SQL, Java, JSP, JSF, and Shell Scripting\n\n\tRDBMS:\n\tOracle 10g\n\n\tOracle Tools:\n\tJDeveloper, XML/BI Publisher, OAF,ADF\n\n\tDatabase Tools and Utilities:\n\tSQL developer, Sql Loader, Putty, Winscp\n\n\tOperating Systems:\n\tUNIX, LINUX, Windows 98/NT/2000\n\n\n\n\nProfessional Experience\n· Working as Sales Force Developer in Oracle India Pvt Ltd from January-2015 to till date. (Certified Salesforce Platform Developer 1)\n· Working as Senior Applications Engineer in Oracle India Pvt Ltd from October 2011 to till date.\n· Worked as Project Engineer in Wipro Technologies from February 2009 to September 2011.\n\n\nEducational Qualifications \n· B.E in Electronics & Communication Engineering with an aggregate of 78.6 from JNTU University, Kukatpally, Hyderabad, Telangana in June,2008.\n· Intermediate with an aggregate of 94.8 from Suresh Junior College, Guntur, AP in May 2004.\n· SSC with an aggregate of 90.67 from St Ignatius School, Guntur, AP in May 2002.\n\n\n\n\nCertifications, Academic and Professional Achievements\n· Salesforce Certified Platform Developer I\n· Primary POC for critical issues in Sales Cloud and Accounting in Fusion Payables/GL Transfer/OAF of sub-ledger accounting module.\n· Been to USA for 6 months on L1 Visa in Oracle for handling critical issues.\n· Achieved Best Performance Award in Wipro for AMX project.\n· Ranked 3rd during Campus Training Exams Conducted in Wipro. \n· Secured All India rank of 935 for AIEEE entrance.\n· Secured Prathibha Award from state govt for Eamcet-04 rank 277.\n· Stood 1st at the school level in SSC Board Examination. \n\n\nProject Experience\n\nProject:  Sales Cloud CRM development\t\t\t\t\t          Jan 2015 – Till Date\nClient:    Confidential, Responsys Team\nRole:      Sales Force Developer\nProject Description: It is the common architecture which involves entering the products, leads and accounts.\n\nResponsibilities:\n· Worked on Visual Force pages development based on the client requirement\n· Business requirement implementation in Apex.\n· CRM functional understanding and gave session to the new joiners in the team\n· Importing the product information, lead information using loader.\n· Worked on salesforce objects, fields, Apex triggers based on the client requirement.\n\t\t\t\n\nProject:  Fusion Payables Product Development\t\t\t\t\t    Oct 2016 – Till Date\nClient:    Confidential, Oracle India Pvt Ltd, US\nRole:      Oracle Senior Application Engineer\nProject Description: It is the common architecture which involves entering the invoices, payments.\n\nResponsibilities:\n· Identified and resolved the Fusion AP bugs which involve majorly on Invoices Import, Prepayments, Payments.\n· Playing a POC role for the accounting issues in Fusion Payables.\n· Have written automation scripts for testing the Standard functional AP/IBY flows which are named as BVT which involve usage of Selenium API’s.\n· Tested the Standard Functional AP flows in Oracle fusion applications and detecting issues in corner cases.\n· Worked on Fusion new Enhancement project AP One Time Payments project where developed a new FBDI, RTF report for showing success/rejections, ODI which is Oracle Data Integrator for Invoice import process.\n· Developed generic data fixes for Fusion Payables accounting issues.\n· Documented the required fixes on its usage so that it can be repetitively used without dev intervention. \n\n\nProject:  Oracle EBS Subledger Accounting, Fusion Payables Product Development     Oct 2011 – Oct 2016\nClient:    Confidential, Oracle India Pvt Ltd, US\nRole:      Oracle Senior Application Engineer\nProject Description: It is the common architecture for generation of accounting entries based on certain rules.\n\nResponsibilities:\n· Identified and resolved the Fusion AP bugs which involve majorly on Invoices Import, Prepayment\n· Identified and resolved the Sub-ledger Accounting (SLA) product related bugs.\n· Provided the generic data fixes for R12 upgrade customers related to AP/AR/CE accounting issues.\n· Worked on OAF page issues and helped customers for immediate solution.\n· Helped and interacted with end users for the SLAM setups.\n· Attend functional trainings on OAF required changes for the bug approach, and giving the presentation to the team on the same.\n· Have given functional trainings to the new Joinees on the SLA Product and organized technical trainings on OAF, Java for the team.\n\n\n\nProject:  TIM Anatel and Interface Unica                                                                Jan 2011 – Sep 2011\n                TIM – Brazil Integration                                                                                        Jan 2010 – Jan 2011\nClient:    Confidential, Wipro Technologies\nRole:      Application Developer\nProject Description: TIM Gateway is an interface to invoke the external services like TIM OLM, Messaging Service and TIM Customer care and provide the response back to the core component\n\nResponsibilities:\n· Developing the Billing Gateway and interface Unica applications as per the requirements specified by the client.\n· Worked on developing the Flow Charts design documents necessary for the project with the help Visio tool.\n· Worked on running the code quality tools –Wipro Style/Sonar/Hudson for maintaining the code quality.\n· Direct interaction with the client in the weekly meetings and giving the updates and suggestions.\n· Writing High Level Design documents, detailed design documents and Unit Test Case documents.\n· Familiar with Build tool - Maven\n\nProject:  AMX Project                                                                                                 Jun 2010 – Dec 2010\n                HP Integration                                                                                                       May 2009 – Dec 2009\nClient:    Confidential, Wipro Technologies\nRole:      Application Tester\nProject Description: America Movil (AMX) is using the Qualcomm’s Plaza Retail™ solution\nPlaza Retail that has to be integrated with AMX Operator network for the billing of the subscribers when they purchase and download content. Key aspects of this project include implementing all those necessary supporting components to achieve this integration\n\nResponsibilities:\n· Testing the Billing Plugin application using automation springs framework \n· Designing and executing Unit Test cases for testing the application integrity at design phase\n· Guiding the new joiners in understanding the project and how to use the tools that are required in the project.\n· Worked on running the performance tool Jmeter for performance testing with 100 users configuration setup.\n· Developed automation Tool Mlats for Functional Testing(Http frame works)\n· Prepared with different test cases required for testing the complete flow.\n· Daily status updates are provided in timely manner and updating the critical bugs to the developers for immediate action.\n· Participated in Preparing Test Plan documents like Functional Test Plan, Performance Test Plan. \n\n\n\n\n\n\n\n\n\nDECLARATION \n\nAll the information provided above is true to my knowledge. For any reference or detailed information required, please request the same. \n\n\nSravani Gadde","annotation":[{"label":["Name"],"points":[{"start":7931,"end":7943,"text":"Sravani Gadde"}]},{"label":["Skills"],"points":[{"start":5132,"end":5135,"text":"Java"}]},{"label":["Tools"],"points":[{"start":5127,"end":5129,"text":"OAF"}]},{"label":["Tools"],"points":[{"start":4922,"end":4924,"text":"OAF"}]},{"label":["Tools"],"points":[{"start":4768,"end":4770,"text":"OAF"}]},{"label":["Skills"],"points":[{"start":4509,"end":4514,"text":"Fusion"}]},{"label":["Skills"],"points":[{"start":4313,"end":4318,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":4277,"end":4282,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":4192,"end":4197,"text":"Fusion"}]},{"label":["Skills"],"points":[{"start":4159,"end":4164,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":4004,"end":4009,"text":"Fusion"}]},{"label":["Skills"],"points":[{"start":3918,"end":3923,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":3774,"end":3779,"text":"Fusion"}]},{"label":["Skills"],"points":[{"start":3697,"end":3702,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":3490,"end":3495,"text":"Fusion"}]},{"label":["Skills"],"points":[{"start":3360,"end":3365,"text":"Fusion"}]},{"label":["Skills"],"points":[{"start":3177,"end":3182,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":3141,"end":3146,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":3051,"end":3056,"text":"Fusion"}]},{"label":["Skills"],"points":[{"start":2047,"end":2052,"text":"Oracle"}]},{"label":["Tools"],"points":[{"start":1969,"end":1971,"text":"OAF"}]},{"label":["Skills"],"points":[{"start":1941,"end":1946,"text":"Fusion"}]},{"label":["Skills"],"points":[{"start":1280,"end":1285,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":1138,"end":1143,"text":"Oracle"}]},{"label":["Operating_Systems"],"points":[{"start":1053,"end":1059,"text":"Windows"}]},{"label":["Operating_Systems"],"points":[{"start":1046,"end":1050,"text":"LINUX"}]},{"label":["Operating_Systems"],"points":[{"start":1040,"end":1043,"text":"UNIX"}]},{"label":["Tools"],"points":[{"start":1011,"end":1016,"text":"Winscp"}]},{"label":["Tools"],"points":[{"start":1004,"end":1008,"text":"Putty"}]},{"label":["Tools"],"points":[{"start":992,"end":1001,"text":"Sql Loader"}]},{"label":["Tools"],"points":[{"start":977,"end":989,"text":"SQL developer"}]},{"label":["Tools"],"points":[{"start":940,"end":942,"text":"ADF"}]},{"label":["Tools"],"points":[{"start":936,"end":938,"text":"OAF"}]},{"label":["Tools"],"points":[{"start":918,"end":933,"text":"XML/BI Publisher"}]},{"label":["Tools"],"points":[{"start":906,"end":915,"text":"JDeveloper"}]},{"label":["Skills"],"points":[{"start":891,"end":896,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":878,"end":883,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":852,"end":866,"text":"Shell Scripting"}]},{"label":["Skills"],"points":[{"start":843,"end":845,"text":"JSF"}]},{"label":["Skills"],"points":[{"start":838,"end":840,"text":"JSP"}]},{"label":["Skills"],"points":[{"start":832,"end":835,"text":"Java"}]},{"label":["Skills"],"points":[{"start":824,"end":829,"text":"PL/SQL"}]},{"label":["Skills"],"points":[{"start":819,"end":821,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":798,"end":803,"text":"Fusion"}]},{"label":["Skills"],"points":[{"start":757,"end":775,"text":"Oracle Applications"}]},{"label":["Skills"],"points":[{"start":735,"end":753,"text":"Oracle Applications"}]},{"label":["Skills"],"points":[{"start":722,"end":731,"text":"DataLoader"}]},{"label":["Skills"],"points":[{"start":712,"end":719,"text":"Workflow"}]},{"label":["Skills"],"points":[{"start":692,"end":709,"text":"Visual Force Pages"}]},{"label":["Skills"],"points":[{"start":677,"end":689,"text":"Apex Triggers"}]},{"label":["Skills"],"points":[{"start":664,"end":673,"text":"SalesForce"}]},{"label":["Tools"],"points":[{"start":631,"end":633,"text":"ADF"}]},{"label":["Skills"],"points":[{"start":624,"end":629,"text":"Oracle"}]},{"label":["Tools"],"points":[{"start":616,"end":618,"text":"OAF"}]},{"label":["Skills"],"points":[{"start":609,"end":614,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":603,"end":606,"text":"Java"}]},{"label":["Skills"],"points":[{"start":529,"end":538,"text":"SalesForce"}]},{"label":["Skills"],"points":[{"start":480,"end":485,"text":"Fusion"}]},{"label":["Skills"],"points":[{"start":473,"end":478,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":429,"end":434,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":346,"end":351,"text":"Oracle"}]},{"label":["Years_of_Experience"],"points":[{"start":239,"end":246,"text":"9+ Years"}]},{"label":["Certifications"],"points":[{"start":196,"end":224,"text":"SalesForce Platform Developer"}]},{"label":["Mobile_No"],"points":[{"start":133,"end":146,"text":"+91 9886916030"}]},{"label":["Email_Address"],"points":[{"start":106,"end":130,"text":"sravanigadde.SF@gmail.com"}]},{"label":["Name"],"points":[{"start":8,"end":20,"text":"Sravani Gadde"}]}],"extras":null,"metadata":{"first_done_at":1564140210000,"last_updated_at":1564140210000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "SRINIVAS ADEPU\n      Email: srinivas.aadepu9009@gmail.com\n                                                                                      Phone : +91-9652584359   \nEXPERIENCE SUMMARY\n\n\t•\t7.8 years of IT experience including around 4+ years of experience in salesforce.com development, SFDC CRM platform as an administrator and developer.\n\t•\tStrong working experience with various salesforce.com standard objects such as Accounts, Contacts, Opportunities, Products and Price books, Cases, Leads, Campaigns, Forecasting, Reports, and Dashboards.\n\t•\tExtensive experience in developing APEX Classes, Triggers, Visualforce Pages, Controllers, writing Workflows, Integration, and Force.com API.\n\t•\tImplemented security and sharing rules at object, field, and record level for different users at different levels of organization.\n\t•\tHands-on experience in querying salesforce.com database using SOQL and SOSL queries using Force.com explorer.\n\t•\tExperience in working on Sales Cloud as well as Service Cloud.\n\t•\tProficiency in administrative tasks such as creating profiles, roles, users, email services, Approvals, Workflows, Reports, Dashboards, Developed Formula fields, Work Flow rules, and Validation rules.\n\t•\tExtensive work experience on designing of custom objects, custom fields, role based page layouts, custom Tabs, custom reports, report folders, report extractions to various formats, and Dashboards.\n\t•\tGood knowledge in writing test classes before deploying into production.\n              •\tExperience in extracting data from SQL and Oracle database and loading into Salesforce with Data-loader and mapping the relationship to the data.   \n              •\tExperience in developing Lightning components with Force.com IDE. \n\t•\tExcellent understanding of OOPS concepts and Design Patterns such as Model View Controller (MVC).\nEDUCATIONAL QUALIFICATION\n\n\t•\tMaster of Computer Application from Osmania University, Telangana.\n\t•\tBachelor of Science, Spoorthy Degree college, Osmania University, Telangana.\n\nPROFESSIONAL EXPERIENCE\t\n\n· Working as a Senior Software Engineer with Tech Mahindra Ltd., Hyderabad from June 2014 to till date.\n· Worked as a Software Engineer with UST Global Trivandrum from March 2013 to May 2014.\n· Worked as a Software Engineer with Doyen Info Solutions Mumbai from Dec’2010 to Feb 2013.\n\n\n\nTECHNICAL SKILLS\n\nSalesforce Tech\nAPEX Language, SOQL, SOSL, APEX Trigger, APEX Classes, APEX Web Services, Visualforce (Pages, Components, Controllers), APEX data loader, Force.com APEX Explorer, AJAX, Workflow, Approvals, Dashboards, Reports, Analytic Snapshots, Custom Objects, Force.com Eclipse IDE Plug-in\nCustom Integration\nOutbound Messages, Workflow, Approvals, Field Updates, Reports, Custom Objects, Custom Settings, Custom Labels/ Tabs, Account Management, Contact Management, Email Services, AppExchange Package, Custom Application, Sandbox Data Loading\nForce.com Tools\nApex Data Loader, Force.com Apex Explorer, Force.com Migration Tool, Force.com Excel Connector, Force.com Eclipse IDE Plug-in\nProgramming Languages\nJDK 1.5/ 1.6, C, C++, SQL, PL/SQL\n\nProject Summary \n \n\nProject#: MCT\n           \nTitle    : GE My Customer Training                                  Aug’18 to till date\nRole     : Sales force Developer & Administrator\nClient    : General Electric\n \nClient Description:\n\n   My Customer Training is a combined package application comprising Salesforce and Servicemax functionalities. It is one kind of single platform where the trainings/schedules pertaining to list of products purchased can be effectively managed and also useful to understand the business from the revenue perspective as well. This collaborative tool helps business to manage the assignments/schedules of different trainings Work orders using an effective user friendly Dispatch Console tool. Trainers should be able to take the training through one of the convenient and available training mode after the trainings are assigned to set of users. This entire end to end process being administered and run through privileged users say Schedulers & Admins who actually helps trainers by managing their schedules via calendars and assisting them in some of the access related challenges within the platform.\n            \n \nResponsibilities:\n· Involved in creating the custom objects, tabs, fields, formulas.\n· Creating Validation rules for business needs.\n· Involved in creating reports and dashboards.\n· Involved in writing the triggers and apex class.\n· Worked on Lightning components and events.\n· Worked on establishing communication between VF Pages and Lightning Components.\n· Developed application using Apex Controller, Visual Force pages.\n· Design and Invoking the Workflows to automate the Business Process.\n· Implemented custom setting and custom labels and experienced in agile methodology \n\n· Worked on roll-up summary and calculation part as per requirement.\n\n· Worked on Test classes for code coverage.\n· Worked on different Objects and created validation rules, triggers as per the requirements.\n· Performed Unit Testing in System mode and User mode along with Deployment.\n\n\nTech Mahindra Ltd., Hyderabad\tNov’16 to July 2018\n1. Project: AUM Billing System \tNov’16 to July 2018\nDescription: MSCI Emerging Markets Index is an index created by Morgan Stanley Capital International (MSCI) designed to measure equity market performance in global emerging markets. The AUM Billing System is a Force.com Custom Application residing inside the MSCI Inc. Salesforce.com instance.  A Custom Application is defined as a collection of Tabs. This application agenda is access to billing information, loading valuation data and configuration and billing information for client AUM deals.\n\nResponsibilities:\n\t•\tCreated Page Layouts and make as read-only for specific users.\n\t•\tDisable the custom buttons in the visualforce pages for AUM Billing System application.\n\t•\tCreated Custom button and added into the Page layout.\n\t•\tUsed various Aggregate functions with SOQL to create custom report using Apex Controllers enforcing Governor’s limits.\n\t•\tCrated Catalogues for Custom objects, Standard objects and Managed package objects into the MSCI Salesforce instance.\n\t•\tDeveloped Approval process for the application by implementing custom formulas in different stages of approval.\n\t•\tStrictly maintained Model-View-Controller (MVC) architecture throughout the application.\n\t•\tDeveloped Triggers for applying the business logic on Database events.\n\t•\tMake as read-only for AUM Billing System application at profile level and Permission sets.\n\t•\tCreated workflow rules and defined related field updates to implement business logic.\n\t•\tUsed SOQL and SOSL with in Governor Limits for data manipulation needs of the application.\n\t•\tUsed Data Loader for insert, update and bulk import or export of data from Salesforce.com subjects.\n\t•\tDesigned and deployed the Custom Objects, Validation Rules, Page Layouts, Custom Tabs and Components to suit to the needs of the application.\n\t•\tCreated the Solution design document.\n\t•\tSuccessfully implemented Salesforce – Data loader integration.\n\nEnvironment: Salesforce.com, Apex, Forec.com IDE, Workflows, Data Loader and Visualforce pages.\n\nTech Mahindra Ltd., Hyderabad\tAug’14 to Oct’16\nProject: MSCI Budgeting & Forecasting \tAug’14 to Oct’16\nDescription: MSCI Emerging Markets Index is an index created by Morgan Stanley Capital International (MSCI) designed to measure equity market performance in global emerging markets. This application is used for data migration and implementing the Income Statements for Product as well as MSCI. Mainly it contains HC Forecasting, Comp, Non-Comp, Location and Reference Cost Center. An income statement is a financial statement that measures a financial performance over a specific BMU Product as well as at overall MSCI level.\nResponsibilities:\n\t•\tImplemented the Income Statement reports for Product as well as MSCI level\n\t•\tSuccessfully implemented Salesforce – Data loader integration\n\t•\tCreated Page Layouts and assigned different layouts based on Record Types\n\t•\tCreated a report type based on the client requirements\n\t•\tCreated the Solution design document\n\t•\tUsed various Aggregate functions with SOQL to create custom report using Apex Controllers enforcing Governor’s limits\n\t•\tCreated workflow rules and defined related field updates to implement business logic\n\t•\tUsed SOQL and SOSL with in Governor Limits for data manipulation needs of the application\n\t•\tUsed Data Loader for insert, update and bulk import or export of data from Salesforce.com subjects\n\t•\tUsed it to read, extract, and load data from CSV files\n\t•\tDesigned and deployed the Custom Objects, Validation Rules, Page Layouts, Custom Tabs and Components to suit to the needs of the application\n\nEnvironment: Salesforce.com, Apex, Forec.com IDE, Workflows, Data Loader, Import Wizard\n\n\nUST Global., Trivandrum \t\n\n2. Project: Multichannel Project: \tMar‘13 to May‘14\nRole: Software Engineer\nDescription: Multichannel project is all about Mainframes Support and Maintenance using tools such as CTM(Job monitoring tool). We monitor and fix Jobs in 12 regions. We have team size of eighteen members which one sits in Onsite and remaining team work from offshore.\nResponsibilities:\n\t•\tWorked on monitoring the jobs and fixed.\n\t•\tWorked on finding RCA for the failed jobs and informed the same to developers.\n\t•\tResponsible for creating the validation rules for standard and custom fields\n\t•\tWorked on automation process of few daily activities as part of the team.\n\t•\tWorked on shifts as per requirement.\n\nEnvironment:  COBOL, JCL, VISAM, DB2, CICS, CTM tool\n\n\nDoyen Info Solutions Pvt Ltd., Bangalore\tDec’10 to Jan’13\n1. Project: Revenue Management System, NY\tDec’10 to Jan’13\nDescription: Client is a leader in Revenue Management NY — an emerging enterprise application category that enables companies to align and improve the processes of pricing and quoting, contract development and management, trade settlements and channel incentives, in order to eliminate the revenue leakage and reduce the financial regulatory compliance risk that can cost companies millions per year. Revenue Management has become a core, strategic focus for companies from all industries that deal with competitive pricing, complex contracts, and multiple channels. Client offers a unique approach, optimized for life sciences and high tech companies that combines industry specific solutions and best practices expertise to enable our customers to better plan and control the processes that drive the entire revenue life cycle for their business.\n\nResponsibilities:\n\t•\tCreating Scrum boards, Kanban boards based on user requirement, configuring customized workflow statuses to board columns.\n\t•\tTroubleshooting board issues, providing permissions to Scrum master for creating sprints, scheduling sprints and managing the boards.\n\t•\tGenerating Agile reports such as Burndown charts, velocity charts and burnup charts.\n\t•\tProvided training for end users on usage of JIRA Agile and worked in 24/7 Production support, responsible for creating new users and groups in Jira based on request.\n\t•\tResponsible for creation of custom workflows, managing Jira – Issue Types with specified custom fields.\n\t•\tProviding permissions to the users for Confluence Wiki Pages and other required permissions based on request as per organization policies.\n\t•\tTracking timesheet TEMPO for each issue for reporting, resolving tickets based on SLA (Service Level Agreement)\n\t•\tCreate complex JIRA workflows including project workflows, field configurations, screen schemes, permission schemes and notification schemes in JIRA.\n\t•\tMaintaining backup in DEV environment, prepared projects, dashboards, reports and questions for all JIRA related services.\n\t•\tPreparing succession of build by replicating of build data from original site to workplace using Jenkins build tool\nEnvironment: Jira, Confluence, Oracle 10g, Win XP, Cent OS","annotation":[{"label":["Skills"],"points":[{"start":8822,"end":8829,"text":"Workflow"}]},{"label":["Skills"],"points":[{"start":8785,"end":8794,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":8711,"end":8714,"text":"Tabs"}]},{"label":["Skills"],"points":[{"start":8656,"end":8669,"text":"Custom Objects"}]},{"label":["Skills"],"points":[{"start":8545,"end":8554,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":8391,"end":8394,"text":"SOSL"}]},{"label":["Skills"],"points":[{"start":8382,"end":8385,"text":"SOQL"}]},{"label":["Skills"],"points":[{"start":8206,"end":8209,"text":"SOQL"}]},{"label":["Skills"],"points":[{"start":7953,"end":7962,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":7180,"end":7190,"text":"Visualforce"}]},{"label":["Skills"],"points":[{"start":7153,"end":7160,"text":"Workflow"}]},{"label":["Skills"],"points":[{"start":7116,"end":7125,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":7064,"end":7073,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":6934,"end":6937,"text":"Tabs"}]},{"label":["Skills"],"points":[{"start":6879,"end":6892,"text":"Custom Objects"}]},{"label":["Skills"],"points":[{"start":6825,"end":6834,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":6670,"end":6673,"text":"SOSL"}]},{"label":["Skills"],"points":[{"start":6661,"end":6664,"text":"SOQL"}]},{"label":["Skills"],"points":[{"start":6168,"end":6177,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":5987,"end":5990,"text":"SOQL"}]},{"label":["Skills"],"points":[{"start":5562,"end":5565,"text":"Tabs"}]},{"label":["Skills"],"points":[{"start":5513,"end":5530,"text":"Custom Application"}]},{"label":["Skills"],"points":[{"start":5485,"end":5494,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":5436,"end":5453,"text":"Custom Application"}]},{"label":["Skills"],"points":[{"start":5426,"end":5434,"text":"Force.com"}]},{"label":["Skills"],"points":[{"start":4697,"end":4704,"text":"Workflow"}]},{"label":["Skills"],"points":[{"start":3398,"end":3407,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":3086,"end":3091,"text":"PL/SQL"}]},{"label":["Skills"],"points":[{"start":3081,"end":3083,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":3076,"end":3078,"text":"C++"}]},{"label":["Skills"],"points":[{"start":3073,"end":3073,"text":"C"}]},{"label":["Skills"],"points":[{"start":3059,"end":3061,"text":"JDK"}]},{"label":["Tools"],"points":[{"start":3007,"end":3035,"text":"Force.com Eclipse IDE Plug-in"}]},{"label":["Tools"],"points":[{"start":2980,"end":3004,"text":"Force.com Excel Connector"}]},{"label":["Tools"],"points":[{"start":2954,"end":2977,"text":"Force.com Migration Tool"}]},{"label":["Tools"],"points":[{"start":2929,"end":2951,"text":"Force.com Apex Explorer"}]},{"label":["Tools"],"points":[{"start":2911,"end":2926,"text":"Apex Data Loader"}]},{"label":["Skills"],"points":[{"start":2895,"end":2903,"text":"Force.com"}]},{"label":["Skills"],"points":[{"start":2874,"end":2893,"text":"Sandbox Data Loading"}]},{"label":["Skills"],"points":[{"start":2854,"end":2871,"text":"Custom Application"}]},{"label":["Skills"],"points":[{"start":2833,"end":2851,"text":"AppExchange Package"}]},{"label":["Skills"],"points":[{"start":2817,"end":2830,"text":"Email Services"}]},{"label":["Skills"],"points":[{"start":2797,"end":2814,"text":"Contact Management"}]},{"label":["Skills"],"points":[{"start":2777,"end":2794,"text":"Account Management"}]},{"label":["Skills"],"points":[{"start":2771,"end":2774,"text":"Tabs"}]},{"label":["Skills"],"points":[{"start":2756,"end":2768,"text":"Custom Labels"}]},{"label":["Skills"],"points":[{"start":2739,"end":2753,"text":"Custom Settings"}]},{"label":["Skills"],"points":[{"start":2723,"end":2736,"text":"Custom Objects"}]},{"label":["Skills"],"points":[{"start":2714,"end":2720,"text":"Reports"}]},{"label":["Skills"],"points":[{"start":2699,"end":2711,"text":"Field Updates"}]},{"label":["Skills"],"points":[{"start":2688,"end":2696,"text":"Approvals"}]},{"label":["Skills"],"points":[{"start":2678,"end":2685,"text":"Workflow"}]},{"label":["Skills"],"points":[{"start":2659,"end":2675,"text":"Outbound Messages"}]},{"label":["Tools"],"points":[{"start":2610,"end":2638,"text":"Force.com Eclipse IDE Plug-in"}]},{"label":["Skills"],"points":[{"start":2594,"end":2607,"text":"Custom Objects"}]},{"label":["Skills"],"points":[{"start":2574,"end":2591,"text":"Analytic Snapshots"}]},{"label":["Skills"],"points":[{"start":2565,"end":2571,"text":"Reports"}]},{"label":["Skills"],"points":[{"start":2553,"end":2562,"text":"Dashboards"}]},{"label":["Skills"],"points":[{"start":2542,"end":2550,"text":"Approvals"}]},{"label":["Skills"],"points":[{"start":2532,"end":2539,"text":"Workflow"}]},{"label":["Skills"],"points":[{"start":2526,"end":2529,"text":"AJAX"}]},{"label":["Skills"],"points":[{"start":2501,"end":2523,"text":"Force.com APEX Explorer"}]},{"label":["Skills"],"points":[{"start":2483,"end":2498,"text":"APEX data loader"}]},{"label":["Skills"],"points":[{"start":2437,"end":2447,"text":"Visualforce"}]},{"label":["Skills"],"points":[{"start":2418,"end":2434,"text":"APEX Web Services"}]},{"label":["Skills"],"points":[{"start":2404,"end":2415,"text":"APEX Classes"}]},{"label":["Skills"],"points":[{"start":2390,"end":2401,"text":"APEX Trigger"}]},{"label":["Skills"],"points":[{"start":2384,"end":2387,"text":"SOSL"}]},{"label":["Skills"],"points":[{"start":2378,"end":2381,"text":"SOQL"}]},{"label":["Skills"],"points":[{"start":2363,"end":2366,"text":"APEX"}]},{"label":["Skills"],"points":[{"start":2347,"end":2356,"text":"Salesforce"}]},{"label":["Degree"],"points":[{"start":1868,"end":1897,"text":"Master of Computer Application"}]},{"label":["Skills"],"points":[{"start":1721,"end":1729,"text":"Force.com"}]},{"label":["Skills"],"points":[{"start":1581,"end":1590,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":1540,"end":1542,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":1401,"end":1410,"text":"Dashboards"}]},{"label":["Skills"],"points":[{"start":1320,"end":1323,"text":"Tabs"}]},{"label":["Skills"],"points":[{"start":1135,"end":1144,"text":"Dashboards"}]},{"label":["Skills"],"points":[{"start":1126,"end":1132,"text":"Reports"}]},{"label":["Skills"],"points":[{"start":1115,"end":1122,"text":"Workflow"}]},{"label":["Skills"],"points":[{"start":1104,"end":1112,"text":"Approvals"}]},{"label":["Skills"],"points":[{"start":922,"end":930,"text":"Force.com"}]},{"label":["Skills"],"points":[{"start":903,"end":906,"text":"SOSL"}]},{"label":["Skills"],"points":[{"start":894,"end":897,"text":"SOQL"}]},{"label":["Skills"],"points":[{"start":680,"end":688,"text":"Force.com"}]},{"label":["Skills"],"points":[{"start":652,"end":659,"text":"Workflow"}]},{"label":["Skills"],"points":[{"start":612,"end":622,"text":"Visualforce"}]},{"label":["Skills"],"points":[{"start":588,"end":599,"text":"APEX Classes"}]},{"label":["Skills"],"points":[{"start":538,"end":547,"text":"Dashboards"}]},{"label":["Skills"],"points":[{"start":525,"end":531,"text":"Reports"}]},{"label":["Years_of_Experience"],"points":[{"start":193,"end":201,"text":"7.8 years"}]},{"label":["Mobile_No"],"points":[{"start":152,"end":166,"text":"+91-9652584359 "}]},{"label":["Email_Address"],"points":[{"start":28,"end":56,"text":"srinivas.aadepu9009@gmail.com"}]},{"label":["Name"],"points":[{"start":0,"end":13,"text":"SRINIVAS ADEPU"}]}],"extras":null,"metadata":{"first_done_at":1564210720000,"last_updated_at":1564210720000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "SRINIVASULU DEVARASETTY\n\n\n \n\n\nEmail( : dsrinivasulu.sfdc@gmail.com\n\n\n               Mobile# +91-9665354987                                                     \nProfessional Summary\n· Senior Sales force developer with 8.7 years of IT experience that includes around 5.8 years of experience in Salesforce.com. 2.9 years of experience in developing web applications using PHP/HTML technologies. \n· Having 10 Months of Experience in Sales force Lightning. \n· Experience in Administration, Configuration, Implementation, Lightning and Support experience with SFDC Platform.\n· Analyzed Sales, and Service (Customer Support) business processes used by salesforce.com customers and recommended ways to improve their processes using salesforce.com. \n\n· Experience in SFDC development using Lightning Components, Lightning Events, and Lightning Applications, Visual Force Pages, Apex Classes, Controllers, Triggers, Components, Tabs, Apex RESTAPI Web Services, Custom Objects, Reports and Dashboards, Profiles, Creating Roles, Relationships, Page Layouts, Org-wide default, Sharing Rules, Work flows, Process Builders, Record Types.\n· Working on Lightning Process Builder, Data List, LDS, Events and quick Action.\n· Experience in writing Optimized SOQL queries in Triggers and Apex Classes for better performance and user experience.\n· Good experience in using Force.com Web services REST API for implementing Web services and call outs for retrieving data from different third party systems.\n· Good experience in debugging and fixing all sorts of Development issues ,production bugs, Testing issues, Governor limits, user privilege issues, Data issues, web service callout issues, and other issues.\n· Involved in various stages of Software Development Life Cycle (SDLC) including analysis, requirement engineering, development, deployment & maintenance of standalone, Multi-tier, web and portal based enterprise applications. \n· Good Experience to use Meta – data migration tools like Eclipse IDE, Apex Data Loader, workbench, Change set, and ANT.\n· Committed to excellence, self – motivator, quick – learner, team – player, and prudent developer with strong problem – solving, analytical skills and communication skills.\nEducational Summary\n\n \n\n· Acquired MCA (Master of Computer Applications) From “Savitribai Phule Pune University”, Pune, and Maharashtra.\nTechnical Skills\n \n\n· Salesforce.com \n\n: Apex class & Triggers, Visual force (Pages, Components & Controllers), SOQL, RESTFULL API,\n\n     Email Services, AppExchange, Process Builder, Lightning applications & Web service callouts.\n· Tools\n\n:  Data Loader, Workbench, Change Set, Bit bucket, SoapUI & Jira.\n· Languages\n\n: Apex, HMTL, Java Script, PHP, Dot Net.\nProfessional Experience\n\n \n· Working as Senior Software Developer “Amdocs India Pvt. Ltd”, Pune since June, 2017 to till date.\n\n· Worked as Software Engineer in “Uttara Foods and Feeds Pvt. Ltd.”, Pune Since Feb, 2012 to May, 2017.\n\n· Worked as Software Developer (PHP) in “Country Club India Ltd”, Pune since Oct, 2010 to Jan, 2012.\nProject Summary\n\n \nProject #01: “IFULLFILL”\n\n\n\n\n\n\n\n\n\nJune, 2017 – Till Date\n\nClient: Sensis \n\n\n\n\n\n\n\n\n\nTitle: Senior Developer\nProject Description: Salesforce.com is used by the Sales, Support and Marketing employees of Sensis. It is integrated with Amdocs Product “AdSell & CSS” which is used by the sales representatives for selling ads on products of Sensis. In this role I regularly interact with the Sensis technical team based out of Australia for gathering business requirements that are dynamic in nature, and ensuring that the CRM application used by their sales/service staff remains agile and supports their daily activities.\n· Responsibilities:\n\n· Involved in requirement gathering throughout the planning and implementation.\n· Upgrading some Apps from Salesforce Classic to Lightning Experience to develop rich user interface and better interaction of pages.\n· Created modern Enterprise Lightning Apps combining Lightning Design System, Lightning App Builder and Lightning Component features.\n· Worked on Salesforce 1 Platform to build Mobile App by enabling Lightning Components for use in Salesforce1 mobile platform to make Lightning Application Mobile.\n\n· Created Custom Objects, tabs, custom fields, custom formulas, Page Layouts, Profiles, Permission sets and Email to Case based on the client requirements.\n· Created Visual force Pages, Apex Triggers, classes and Batch apex as per business requirement.\n· Interacting with Sales team and customers to resolved their issues while using the application. \n\n· Created color coded cases using priority flag image as a formula field to prioritize cases with comments as a first priority based on the flag image for easier access.\n\n· Created and used Email templates for workflow and approval process to get approved from top level management as per business requirements.\n· Used SOQL, SOSL with consideration to Governor Limits for data manipulating needs of the application using platform database objects.\n\n· Used Data Loader for insert, update for bulk import, and export of data from Salesforce.com objects. Used it to read, extract and load data from comma separated values (CSV) Files.\n\n· Developed and configured various Custom Reports and Dashboards as per organizations requirement.\n\n· Have Created Flow charts on various business flows in Salesforce for Objects Leads, Accounts and some of the custom Objects.\n· Created REST services to integrate with salesforce to legacy system.\n· Used the sandbox for testing and migrated the code to the deployment instance after testing.\n· Attended Daily standup, Sprint planning, Release meeting, Technical review meeting and other meetings periodically.\n· Involved in mentoring junior salesforce resources in the team. \n· Environment & Tools: Salesforce.com Platform, Lightning Experience, Lightning Components, Visual Force (Pages, Component ), Apex Language, Data Loader, REST API, HTML, Java Script, @Future and Batch Apex, Changes Set.\nProject #02: “UTTARA FEED MANAGEMENT”\n\n\n\n\n\n\nJune, 2016 – May, 2017\nClient: Venkys India Ltd\n\nTitle: Software Engineer\n· Project Description: “Feed Management System” is developing to have complete information which related to the poultry feeds, their customer information and stocks available. The application maintains the information like category animal’s category, their feed types, usage etc. It also maintained feed sales, for marketing team area wise, and also has customer information across India to use this; support team will provide solutions for customer quires. \n· Responsibilities:\n\n· Involved in group for requirement gathering throughout the planning and implementation.\n· Attended serum calls.\n· Worked on opportunity Trigger to delete all open tasks automatically whenever the opportunity status gets updated to expire.\n· Worked on sharing rule, profile, and page layout as per business requirement.\n· Created Custom Objects, tabs, custom fields, custom formulas, apex classes based on the client requirements.\n\n· Worked on Batch Apex and Schedulers to mass update records in production based on the business requirement.\n· Worked on process builder for creating records, email alert, flow, record update and submit for approval.\n\n· Created Approval process as per business requirement.\n· Used SOQL with consideration to Governor Limits for data manipulating needs of the application using platform database objects.\n· Used the sandbox for testing and migrated the code to the deployment instance after testing.\n· Environment & Tools: Salesforce.com Platform, Visual Force (Pages, Component & Controllers), Apex Language, Data Loader, HTML, Java Script, Eclipse IDE, Sandbox.\n\nProject #03: “VENKY’S ITALY MARMO”\n\n\n\n\n\n\n\nOct, 2014-April, 2016.\nClient: VHPL.\n\nTitle: Salesforce Developer/Admin.\n· Project Description: “VIM” was designed and developed basically to maintain the customer’s information and stock information as well as work done on particular day (how many blocks get in a day). Sales team will get leads from different location across the world and based on the leads we will process, maintain, and give the service to people once they turned into customers. We also maintained blocks size and grade in to our Salesforce CRM for customer user friendly. Our Marketing team will conduct campaigns regularly using salesforce for better understanding about our products.\n· Responsibilities: \n· Involved in gathering and documenting requirements from users. Designed UML and use-case diagrams.\n· Worked with various Salesforce.com objects like Accounts, Contacts, Leads, Opportunities, Campaigns, Reports, and created custom objects based on Business need.\n· Developed various custom Objects, Tabs, Components and Visual force Pages and Controllers.\n· Created Profiles, Roles and permission sets and assign them to users based on the requirement.\n· Created custom links, formulas, Layouts, workflow and Approval processes. Set both Object-Level and Record-Level Security.\n· Deactivated users to release licenses when required and assigned new users to those roles.\n· Developed and Deployed workflows and Approval processes for Opportunities and product/assets management.\n\n· Developed and configured various custom Reports and Report folders for different user profiles based on the need in the organization.\n\n· Used V Lookup functionality in Excel to pull data from cells based on conditions in order to sort out in the right order from a random set of data.\n· Worked on various Internal Salesforce Development Sandboxes and testing environment.\n· Environment & Tools: Force.com Platform, Visual Force (Pages, Component & Controllers), Apex Language, MS Office, Workbench, Data Loader, CSS, Change Set.\nProject #04: “UTTARA FULLFILLMENT SYSTEM”\n\n\n\n\n\n\nAug, 2013 – Sept, 2014.\nClient: Venkys India Ltd.\n\nTitle: Salesforce Developer.\n\n· Project Description: “Uttara Fulfillment System” is the application that was conceived for the sales and service team to find new customers along with retaining existing customers. We also implemented the service cloud along with the sales so that our agents will have all the customer information on hand which will help them serve our customers better.\n· Responsibilities:\n\n· Provided Administrative level support for users of the Salesforce application.\n· Worked on Knowledge articles, Entitlement process, CTI, Omni Channel etc.\n· Created Objects, Fields, Relationships and Approval process in the organization as per requirements.\n· Defined custom profiles, user permissions and created Custom sharing Rules for Record owners with “Read-Only” Permissions granting client-requested Create/Read/Edit/Delete Capabilities.\n· Worked on triggers and apex classes as per business requirement.\n· Implemented Web-to-lead and also implemented email-to-case and web-to-case functionality.\n\n· Used Data Loader in sandbox to insert, update for bulk import, and export of data from Salesforce.com.\n· Designed and developed Visual force pages based on the business requirements.\n\n· Environment & Tools: Force.com Platform, Visual Force, Apex Language, Data Loader, CSS, Change Set.\nProject #05: “COUNTRY CLUB HOSPITALITY AND HOLIDAYS”\n\n\n\n\n\nOct, 2010-Jan, 2012.\nClient:  Country Club\nTitle: PHP Developer\nProject Description: The “Country Club Hospitality and Holidays” was basically selling holiday Packages to customers that information will get from lead people from different location across globe. We provide information to the customer while they visited locations across world. The project was basically maintain the customer information, clubbing hubs, destinations, trendy fitness centers, star-studded entertainments etc., and update daily packages that includes discounts for customer’s user friendly.\n· Responsibilities:\n\n· Contribute in all phases of the development life cycle.\n\n· Developed a web application that includes PHP 5, HTML, CSS, and Java script.\n· Created Database tables in MySQL as per requirements.\n· Created XML for Making Mani Charts for graphical representation for top level CCL.\n· Good knowledge of relational database, version control tools and of developing web services.\n· Troubleshoot, test and maintain the core product software and databases to ensure strong optimization and functionality.\n\n· Develop and deploy new features of facilitate related procedures and tools if necessary.\n\n· Prepared Mani Charts & Run Reports on Signage Player for analyze the business process to take decision to improve on appropriate area.\n· Environment: Word Press, PHP, MySQL, HTML and Tools (Mani Charts & Run PHP Reports on Signage Player).","annotation":[{"label":["Skills"],"points":[{"start":12580,"end":12583,"text":" PHP"}]},{"label":["Skills"],"points":[{"start":12534,"end":12537,"text":" PHP"}]},{"label":["Skills"],"points":[{"start":11883,"end":11886,"text":" PHP"}]},{"label":["Skills"],"points":[{"start":11237,"end":11240,"text":" PHP"}]},{"label":["Tools"],"points":[{"start":11118,"end":11127,"text":"Change Set"}]},{"label":["Tools"],"points":[{"start":11100,"end":11110,"text":"Data Loader"}]},{"label":["Skills"],"points":[{"start":11085,"end":11088,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":10972,"end":10983,"text":"Visual force"}]},{"label":["Skills"],"points":[{"start":10931,"end":10944,"text":"Salesforce.com"}]},{"label":["Tools"],"points":[{"start":10849,"end":10859,"text":"Data Loader"}]},{"label":["Tools"],"points":[{"start":9715,"end":9724,"text":"Change Set"}]},{"label":["Tools"],"points":[{"start":9697,"end":9707,"text":"Data Loader"}]},{"label":["Tools"],"points":[{"start":9686,"end":9694,"text":"Workbench"}]},{"label":["Skills"],"points":[{"start":9660,"end":9663,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":8737,"end":8748,"text":"Visual force"}]},{"label":["Skills"],"points":[{"start":8539,"end":8552,"text":"Salesforce.com"}]},{"label":["Skills"],"points":[{"start":7657,"end":7667,"text":"Java Script"}]},{"label":["Tools"],"points":[{"start":7638,"end":7648,"text":"Data Loader"}]},{"label":["Skills"],"points":[{"start":7623,"end":7626,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":7551,"end":7564,"text":"Salesforce.com"}]},{"label":["Skills"],"points":[{"start":7310,"end":7313,"text":"SOQL"}]},{"label":["Skills"],"points":[{"start":7046,"end":7049,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":5978,"end":5981,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":5947,"end":5957,"text":"Java Script"}]},{"label":["Tools"],"points":[{"start":5918,"end":5928,"text":"Data Loader"}]},{"label":["Skills"],"points":[{"start":5903,"end":5906,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":5800,"end":5813,"text":"Salesforce.com"}]},{"label":["Skills"],"points":[{"start":5095,"end":5108,"text":"Salesforce.com"}]},{"label":["Tools"],"points":[{"start":5023,"end":5033,"text":"Data Loader"}]},{"label":["Skills"],"points":[{"start":4886,"end":4889,"text":"SOQL"}]},{"label":["Skills"],"points":[{"start":4400,"end":4403,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":4380,"end":4391,"text":"Visual force"}]},{"label":["Skills"],"points":[{"start":3191,"end":3204,"text":"Salesforce.com"}]},{"label":["Skills"],"points":[{"start":2701,"end":2707,"text":"Dot Net"}]},{"label":["Skills"],"points":[{"start":2695,"end":2698,"text":" PHP"}]},{"label":["Skills"],"points":[{"start":2683,"end":2693,"text":"Java Script"}]},{"label":["Skills"],"points":[{"start":2677,"end":2680,"text":"HMTL"}]},{"label":["Skills"],"points":[{"start":2671,"end":2674,"text":"Apex"}]},{"label":["Tools"],"points":[{"start":2641,"end":2653,"text":"SoapUI & Jira"}]},{"label":["Tools"],"points":[{"start":2629,"end":2638,"text":"Bit bucket"}]},{"label":["Tools"],"points":[{"start":2617,"end":2626,"text":"Change Set"}]},{"label":["Tools"],"points":[{"start":2606,"end":2614,"text":"Workbench"}]},{"label":["Tools"],"points":[{"start":2593,"end":2603,"text":"Data Loader"}]},{"label":["Skills"],"points":[{"start":2534,"end":2578,"text":"Lightning applications & Web service callouts"}]},{"label":["Skills"],"points":[{"start":2517,"end":2531,"text":"Process Builder"}]},{"label":["Skills"],"points":[{"start":2504,"end":2514,"text":"AppExchange"}]},{"label":["Skills"],"points":[{"start":2488,"end":2501,"text":"Email Services"}]},{"label":["Skills"],"points":[{"start":2468,"end":2479,"text":"RESTFULL API"}]},{"label":["Skills"],"points":[{"start":2462,"end":2465,"text":"SOQL"}]},{"label":["Skills"],"points":[{"start":2414,"end":2425,"text":"Visual force"}]},{"label":["Skills"],"points":[{"start":2391,"end":2411,"text":"Apex class & Triggers"}]},{"label":["Skills"],"points":[{"start":2372,"end":2385,"text":"Salesforce.com"}]},{"label":["Degree"],"points":[{"start":2248,"end":2250,"text":"MCA"}]},{"label":["Tools"],"points":[{"start":1994,"end":2004,"text":"Data Loader"}]},{"label":["Skills"],"points":[{"start":1989,"end":1992,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":1267,"end":1270,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":1238,"end":1241,"text":"SOQL"}]},{"label":["Skills"],"points":[{"start":1146,"end":1160,"text":"Process Builder"}]},{"label":["Skills"],"points":[{"start":1091,"end":1105,"text":"Process Builder"}]},{"label":["Skills"],"points":[{"start":924,"end":927,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":869,"end":872,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":368,"end":371,"text":" PHP"}]},{"label":["Skills"],"points":[{"start":292,"end":305,"text":"Salesforce.com"}]},{"label":["Years_of_Experience"],"points":[{"start":217,"end":225,"text":"8.7 years"}]},{"label":["Mobile_No"],"points":[{"start":92,"end":105,"text":"+91-9665354987"}]},{"label":["Email_Address"],"points":[{"start":39,"end":65,"text":"dsrinivasulu.sfdc@gmail.com"}]},{"label":["Name"],"points":[{"start":0,"end":22,"text":"SRINIVASULU DEVARASETTY"}]}],"extras":null,"metadata":{"first_done_at":1564211923000,"last_updated_at":1564211923000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "Sushant Shekhar\nApplication Developer, Data Analyst (Base SAS, ETL, Python)\n3.6 yrs of professional IT work experience in data science & business analytics for technical development,\nproduction and customisation of SAS BI Application, python, data wrangling, data cleaning and mining\nwith exceptionally good organisational, analytical and interpersonal communication skills especially with\ncustomers / clients.\n\nsushant_1121@yahoo.co.in +91 9333556829 Noida, India linkedin.com/in/sushant-shekhar\n\nWORK EXPERIENCE, PROJECTS -\n\n10/2015 – Present\n\nFUJITSU CONSULTING INDIA (Oct'15 -\nPresent)\nProject # SAS - Network Data\nWarehouse,TELSTRA (Australia)\n\nNoida\nTelstra Networks, one of Australia's largest mobile network firms, whose\nNetwork data warehouse is fabricated on SAS, and NDW serves as an\naggregation point for network performance, capacity and inventory data\ncollected from a range of Telstra management systems. The processed\ndata is then utilised by Telstra product and technical specialists to\nperform reporting and dimensioning exercises for Telstra’s networks.\n\nWorked on Solaris production servers i.e. SAS UNIX\nenvironment for real time data processing to steward the\ndeveloped system named as NDW (Network Data\nwarehouse)\n\nProactively handling daily load jobs its automation, working\nwith various interfacing teams, ETL, weekly and monthly\nreports handling so as to minimize the impacts on NDW\nclients,users.\n\nExpertise in handling high severity production issues with\nrecord time resolution mostly as single resource in this\nproject.\n\nWorked on in-house and control-m schedulers for required\njob changes, handling and enhancements on production\nenvironment.\n\nSuccessfully completed various Regression Testing project\nafter enhancements or changes in business rules/logic on\nSAS.\n\nRequirement gathering from Clients and business for\nTechnical Components to be modified or any more functional\ncomponents to be developed. Delivered NDW Recovery Plan\nand also worked on many production documents for\nreference to client and business.\n\nEDUCATION\n\n2010 – 2014\n\nB.Tech. - Computer Science & Engineering\nSikkim Manipal Institute of Technology\n\n2008 – 2010\n\nSenior & Higher Secondary Education\nCBSE Board\n\nSKILLS\n\nBase SAS ETL Batch processing Unix - Solaris\n\nShell Scripting Postgres SQL\n\nPython - Numpy, Pandas\n\nData Warehouse management Data Wrangling\n\nData Mining Statistical Modelling SAS SQL\n\nService Management Team player\n\nDecision Making Data Analysis\n\nData Extraction & cleaning Data Interpretation\n\nACHIEVEMENTS\nSpecial Award by Client (08/2018)\nReceived special award by Client on Aug’18 for Service improvement\nplan with record time resolution for high severity production issues in\nTelstra-SAS project.\n\nKudos (05/2017)\nEndorsed many exceptional appreciations throughout in project from\nTelstra Networks clients, business & higher management such as KUDOS\nof contributions towards the Fujitsu Values and trusted partner to\ncustomer\n\nNomination for Apex award (2017 – 2018)\nFurther for my continuous exceptional work whole year on Telstra-SAS\nproject, also had been nominated for APEX Awards F.Y. 2017-18\n\nAsterix Award (2016 – 2017)\nFor achieving business goals in Operational & Quality Excellence on\nTelstra-SAS project\n\nCERTIFICATES\nAdvance Computing at CDAC-ACTS, Bangalore (2015)\nPG Diploma of 6 months on C, C++, Linux, Advance Web Technologies,\nCore Java and Advance Java certified at CDAC-ACTS, Bangalore\n\nLANGUAGES\nEnglish\n\nHindi\n\nAchievements/Tasks\n\nmailto:sushant_1121@yahoo.co.in\nhttps://www.linkedin.com/in/sushant-shekhar/","annotation":[{"label":["Email_Address"],"points":[{"start":3487,"end":3510,"text":"sushant_1121@yahoo.co.in"}]},{"label":["Certifications"],"points":[{"start":3372,"end":3420,"text":"Core Java and Advance Java certified at CDAC-ACTS"}]},{"label":["Certifications"],"points":[{"start":3346,"end":3369,"text":"Advance Web Technologies"}]},{"label":["Certifications"],"points":[{"start":3339,"end":3343,"text":"Linux"}]},{"label":["Certifications"],"points":[{"start":3334,"end":3336,"text":"C++"}]},{"label":["Certifications"],"points":[{"start":3305,"end":3331,"text":"PG Diploma of 6 months on C"}]},{"label":["Certifications"],"points":[{"start":3256,"end":3285,"text":"Advance Computing at CDAC-ACTS"}]},{"label":["Skills"],"points":[{"start":3230,"end":3232,"text":"SAS"}]},{"label":["Skills"],"points":[{"start":3059,"end":3061,"text":"SAS"}]},{"label":["Skills"],"points":[{"start":2711,"end":2713,"text":"SAS"}]},{"label":["Skills"],"points":[{"start":2401,"end":2403,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":2397,"end":2399,"text":"SAS"}]},{"label":["Skills"],"points":[{"start":2363,"end":2395,"text":"Data Mining Statistical Modelling"}]},{"label":["Skills"],"points":[{"start":2321,"end":2360,"text":"Data Warehouse management Data Wrangling"}]},{"label":["Skills"],"points":[{"start":2313,"end":2318,"text":"Pandas"}]},{"label":["Skills"],"points":[{"start":2306,"end":2310,"text":"Numpy"}]},{"label":["Skills"],"points":[{"start":2297,"end":2302,"text":"Python"}]},{"label":["Skills"],"points":[{"start":2283,"end":2294,"text":"Postgres SQL"}]},{"label":["Skills"],"points":[{"start":2267,"end":2281,"text":"Shell Scripting"}]},{"label":["Skills"],"points":[{"start":2230,"end":2232,"text":"ETL"}]},{"label":["Skills"],"points":[{"start":2225,"end":2228,"text":" SAS"}]},{"label":["Degree"],"points":[{"start":2071,"end":2110,"text":"B.Tech. - Computer Science & Engineering"}]},{"label":["Skills"],"points":[{"start":1790,"end":1792,"text":"SAS"}]},{"label":["Skills"],"points":[{"start":1331,"end":1333,"text":"ETL"}]},{"label":["Skills"],"points":[{"start":1115,"end":1118,"text":" SAS"}]},{"label":["Skills"],"points":[{"start":769,"end":771,"text":"SAS"}]},{"label":["Skills"],"points":[{"start":599,"end":602,"text":" SAS"}]},{"label":["Mobile_No"],"points":[{"start":437,"end":450,"text":"+91 9333556829"}]},{"label":["Email_Address"],"points":[{"start":412,"end":435,"text":"sushant_1121@yahoo.co.in"}]},{"label":["Skills"],"points":[{"start":214,"end":217,"text":" SAS"}]},{"label":["Years_of_Experience"],"points":[{"start":76,"end":82,"text":"3.6 yrs"}]},{"label":["Skills"],"points":[{"start":68,"end":73,"text":"Python"}]},{"label":["Skills"],"points":[{"start":63,"end":65,"text":"ETL"}]},{"label":["Skills"],"points":[{"start":57,"end":60,"text":" SAS"}]},{"label":["Name"],"points":[{"start":0,"end":14,"text":"Sushant Shekhar"}]}],"extras":null,"metadata":{"first_done_at":1564122433000,"last_updated_at":1564122433000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "CURRICULUM VITAE\n\nThabira Meher\n+919108370860\nthabira.meher02@gmail.com\t\t\t\t\t                    \nExperience Summary:\n· Having 3.3 years of experience in development of web-based Solutions & Application development.\n· Strong experience in Object Oriented Analysis and Design.\n· Work experience on Spring Framework (Spring MVC, Spring JDBC, Spring IOC).\n· Work experience on Servlet, JSP, JDBC with SQL, MySQL. \n· Work experience on Hibernate  ORM Framework \n· Worked with Servers like Tomcat , JBOSS, WebLogic\n· Having good exposure  various design patterns like MVC, Singleton, Factory, Value Object, Front Controller, Data Access Object  patterns.\n· Hands on experience in Microservices with Spring Boot, Spring Cloud, Eureka, Zuul, Actuator & Lombok.\nProfessional Experience:  \n.  Working as a Senior Software Engineer in Virtusa Consulting Services Pvt from  June 2018 to present.\n.  Worked as a Software Engineer in Emids Technology from  April 2017 to June 2018.\n.  Worked as  a Java Developer in  Cognizant  Technology Solutions through eTeam InfoServices Pvt Ltd since Feb 2016 to March 2017 .\n\nEducational Qualification: \n.  MCA from  North Orissa University, Odisha in 2015\n.  B.SC (Computer Science) from Sambalpur University, Odisha in 2013\nTechnical Skills: \n\nProgramming Language           :  Java 6,7\nOperating System                     :  Unix , Windows XP, 7,8,10,Mac OS\nWeb technology                        :  HTML, CSS, JavaScript\nJSE Technologies\t           :  JDBC\nJ2EE Technologies                   :  Servlet, JSP , velocity \nDistributed Technologies          :  RESTful web services, Microservices\nJEE Frameworks\t           :  Spring, Spring Security, Spring Boot, Spring Cloud  \nORM Software                          :  Hibernate\nUnit Testing                              :  Junit\nDatabase                                  :  SQL , MySQL, Oracle 10g/11c\nIDE/Tools                                 :  Eclipse, Spring Tool Suite , Intellij\nVCS                                         :  SVN, GitHub\nBuild Tools                               :  Ant , Maven, Gradle\nServer                                      :  Apache Tomcat, JBOSS, WebLogic\nTest Client                               :  SOAP-UI, Postman REST Client\nReport Tools                            :  Jasper Reports\n\nSUMMARY OF PROJECTS WORKED:\n\nProject #4\nTitle                                                 :  SPEED Online\nDomain                                                     :  Telecom\nClient                                                         :  British Telecom\nDuration                                                   :  August 2018 – present\nTools Used                                              :  Eclipse , Gradle ,  SVN, Postman Rest Client , SQL developer tool\nProgramming Languages                 :  Java, Spring, Hibernate, JPA, Apache POI, RESTful Webservice , Junit, Spring Boot\nWeb /Application Servers                 :  WebLogic \nTeam Size                                                 :  18\nRole                                                  :  Worked as  a Java Developer\nDescription                                      :  Project mainly deals to provide faster broadband speed for various services using exchange lookup, validation, physical link, logical link and router after that calculate pricing on the basis of main link distance, physical, logical link and router requirements as well provide VAS Pricing and discount to customer. To summarize all the journey generating Reports and EQM where can see all the details of customer, sites, services, traffic demand, physical and logical link, router as well pricing.\nResponsibilities                        :\n1. Created Restful Endpoint for Reports module, \n2. Worked on functionality Development as well as Junit testing.\n3. Participate in Deployments.\n\nProject #3\nTitle                                                :  CTMS (Clinical Trial Management System)\nDomain                                                    :  HealthCare\nClient                                                        :  Medidata\nDuration                                                  :  July 2017 – June 2018 \nTools Used                                            :  Intellij, Maven, Rest Client, MySQL workbench, TIBCO Jasper Studio\nProgramming Languages               :  Java, Velocity, XML, Servlet , Spring, Hibernate, JavaScript , MySQL\nWeb/Application Servers                 :  Apache Tomcat , Jenkins\nTeam Size                                               :  12\nRole                                                :  Worked as a team member\nDescription\t                        : Medidata Clinical Trial Management System (CTMS) is a Software as a Service (SaaS) web-based clinical trial management system solution for managing and tracking of clinical operations such as study management, monitoring, and clinical payments.\nIt includes methods for:\n·  building studies, country sites, and subjects;\n·  configuring a variety of templates such as visit reports and payments;\n·  accessing dashboard views for an overview of ongoing studies;\n·  and managing monitoring tasks such as schedules, visit report submission, and approval process.\nResponsibilities:\n1.Involved in Requirement Analysis, \n2.Worked on functionality Development.\n      3.Involved in 3-amigos as well as wrote Feature file.\n    4.Worked on Selenium Automation\n\nProject #2\nTitle\t                                 :  NGA (Next Generation Attenuation)\nDomain                                  :  Banking & Financial Services\nClient                                     :  Ally Financial Inc\nDuration                             :  November 2016 – March 2017.\nTools Used                            :   Eclipse, Maven, HP QC, SOAP UI, PUTTY, WINSCP\nProgramming Languages    :  Java, Servlet , Spring, WebServices, Apache CXF\nWeb/Application Servers     :   Apache Tomcat, Jboss.\nTeam Size\t                 : 10\nRole\t\t                 : Worked as a team member.\nDescription\t             : The project mainly deals with the authentication part of the online system in bank. The Authentication part will include the login using credentials, notification of one time passwords, creating the particular user for accessing Online banking systems. We also had the single sign on system for all third party vendors the bank will be using. The Project is from scratch development in which dealt with bank end of accessing services, using them, creating tokens using encryption techniques. Apache CXF is being used. REST and SOAP services are used for getting responses.\nResponsibilities:\n1.  Involved in Requirement Analysis, Design and development.\n2.  As part of coding worked on controllers, service and client use cases.\n3.  Worked on Debugging , Participate in Bug fixing .\n4.  Involved in testing Services using SOAP-UI.\n5.  Involved in defect  fixing  using QC.\n5.  Writing Junit test case using PowerMockito  and  Cobertura.\n\n\nProject #1\nTitle\t\t                : SaudiEDI\nDomain                                :  Trade\nClient                                   :  Tabadul\nDuration                           :  February 2016 – November 2016.\nTools Used                         :   Eclipse, Ant , Log4J,SVN \nProgramming Languages :  Java, JDBC,Servlet, JSP, EJB\nWeb/Application Servers  :  WebLogic\nDatatbase                           :  Toad,Oracle,SQL\nTeam Size\t              :  6\nRole\t\t              :  Programmer.\t\nDescription\t              : Single Payment Window (SPW) is a payment related service in SaudiEDI/SANAM system that interacts with stakeholders involved in the import and export process ,there by improving the overall effeciency of the process. SPW will facilitate the online payment of financial transactions performed within SaudiEDI.\n\nResponsibilities:\n1. Involved in UI-Prototype Design.\n2. Involved in Design Document.\n3. Coded JSP for view, created servlet , worked service  and DAO layers.\n\nDECLARATION:\n   I hereby declare that the information furnished above is true to the best of my knowledge.\n\n                                                                                                                      Signature:-\n                                                                                                                     Thabira Meher","annotation":[{"label":["Name"],"points":[{"start":8297,"end":8309,"text":"Thabira Meher"}]},{"label":["Skills"],"points":[{"start":7877,"end":7879,"text":"JSP"}]},{"label":["Skills"],"points":[{"start":7376,"end":7378,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":7369,"end":7374,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":7316,"end":7323,"text":"WebLogic"}]},{"label":["Skills"],"points":[{"start":7279,"end":7281,"text":"JSP"}]},{"label":["Skills"],"points":[{"start":7270,"end":7276,"text":"Servlet"}]},{"label":["Skills"],"points":[{"start":7265,"end":7268,"text":"JDBC"}]},{"label":["Skills"],"points":[{"start":7259,"end":7262,"text":"Java"}]},{"label":["Tools"],"points":[{"start":7229,"end":7231,"text":"SVN"}]},{"label":["Tools"],"points":[{"start":7217,"end":7219,"text":"Ant"}]},{"label":["Tools"],"points":[{"start":7208,"end":7214,"text":"Eclipse"}]},{"label":["Skills"],"points":[{"start":6902,"end":6906,"text":"Junit"}]},{"label":["Tools"],"points":[{"start":6839,"end":6845,"text":"SOAP-UI"}]},{"label":["Skills"],"points":[{"start":5887,"end":5899,"text":"Apache Tomcat"}]},{"label":["Skills"],"points":[{"start":5823,"end":5828,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":5813,"end":5819,"text":"Servlet"}]},{"label":["Skills"],"points":[{"start":5807,"end":5810,"text":"Java"}]},{"label":["Tools"],"points":[{"start":5742,"end":5746,"text":"Maven"}]},{"label":["Tools"],"points":[{"start":5733,"end":5739,"text":"Eclipse"}]},{"label":["Skills"],"points":[{"start":4447,"end":4459,"text":"Apache Tomcat"}]},{"label":["Skills"],"points":[{"start":4398,"end":4402,"text":"MySQL"}]},{"label":["Skills"],"points":[{"start":4385,"end":4394,"text":"JavaScript"}]},{"label":["Skills"],"points":[{"start":4374,"end":4382,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":4366,"end":4371,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":4356,"end":4362,"text":"Servlet"}]},{"label":["Skills"],"points":[{"start":4335,"end":4338,"text":"Java"}]},{"label":["Skills"],"points":[{"start":4259,"end":4263,"text":"MySQL"}]},{"label":["Tools"],"points":[{"start":4239,"end":4243,"text":"Maven"}]},{"label":["Tools"],"points":[{"start":4229,"end":4236,"text":"Intellij"}]},{"label":["Skills"],"points":[{"start":3788,"end":3792,"text":"Junit"}]},{"label":["Skills"],"points":[{"start":3081,"end":3084,"text":"Java"}]},{"label":["Skills"],"points":[{"start":2937,"end":2944,"text":"WebLogic"}]},{"label":["Skills"],"points":[{"start":2881,"end":2891,"text":"Spring Boot"}]},{"label":["Skills"],"points":[{"start":2874,"end":2878,"text":"Junit"}]},{"label":["Skills"],"points":[{"start":2825,"end":2833,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":2817,"end":2822,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":2811,"end":2814,"text":"Java"}]},{"label":["Skills"],"points":[{"start":2751,"end":2753,"text":"SQL"}]},{"label":["Tools"],"points":[{"start":2724,"end":2726,"text":"SVN"}]},{"label":["Tools"],"points":[{"start":2714,"end":2719,"text":"Gradle"}]},{"label":["Tools"],"points":[{"start":2704,"end":2710,"text":"Eclipse"}]},{"label":["Tools"],"points":[{"start":2283,"end":2296,"text":"Jasper Reports"}]},{"label":["Tools"],"points":[{"start":2220,"end":2238,"text":"Postman REST Client"}]},{"label":["Tools"],"points":[{"start":2211,"end":2217,"text":"SOAP-UI"}]},{"label":["Skills"],"points":[{"start":2157,"end":2164,"text":"WebLogic"}]},{"label":["Skills"],"points":[{"start":2150,"end":2154,"text":"JBOSS"}]},{"label":["Skills"],"points":[{"start":2135,"end":2147,"text":"Apache Tomcat"}]},{"label":["Tools"],"points":[{"start":2081,"end":2086,"text":"Gradle"}]},{"label":["Tools"],"points":[{"start":2074,"end":2078,"text":"Maven"}]},{"label":["Tools"],"points":[{"start":2068,"end":2070,"text":"Ant"}]},{"label":["Tools"],"points":[{"start":2016,"end":2021,"text":"GitHub"}]},{"label":["Tools"],"points":[{"start":2011,"end":2013,"text":"SVN"}]},{"label":["Tools"],"points":[{"start":1955,"end":1962,"text":"Intellij"}]},{"label":["Tools"],"points":[{"start":1935,"end":1951,"text":"Spring Tool Suite"}]},{"label":["Tools"],"points":[{"start":1926,"end":1932,"text":"Eclipse"}]},{"label":["Skills"],"points":[{"start":1866,"end":1871,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":1859,"end":1863,"text":"MySQL"}]},{"label":["Skills"],"points":[{"start":1853,"end":1855,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":1802,"end":1806,"text":"Junit"}]},{"label":["Skills"],"points":[{"start":1747,"end":1755,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":1691,"end":1702,"text":"Spring Cloud"}]},{"label":["Skills"],"points":[{"start":1678,"end":1688,"text":"Spring Boot"}]},{"label":["Skills"],"points":[{"start":1661,"end":1675,"text":"Spring Security"}]},{"label":["Skills"],"points":[{"start":1653,"end":1658,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":1609,"end":1622,"text":" Microservices"}]},{"label":["Skills"],"points":[{"start":1588,"end":1607,"text":"RESTful web services"}]},{"label":["Skills"],"points":[{"start":1541,"end":1548,"text":"velocity"}]},{"label":["Skills"],"points":[{"start":1535,"end":1537,"text":"JSP"}]},{"label":["Skills"],"points":[{"start":1526,"end":1532,"text":"Servlet"}]},{"label":["Skills"],"points":[{"start":1482,"end":1485,"text":"JDBC"}]},{"label":["Skills"],"points":[{"start":1440,"end":1449,"text":"JavaScript"}]},{"label":["Skills"],"points":[{"start":1435,"end":1437,"text":"CSS"}]},{"label":["Skills"],"points":[{"start":1429,"end":1432,"text":"HTML"}]},{"label":["Operating_Systems"],"points":[{"start":1381,"end":1386,"text":"Mac OS"}]},{"label":["Operating_Systems"],"points":[{"start":1362,"end":1368,"text":"Windows"}]},{"label":["Operating_Systems"],"points":[{"start":1355,"end":1358,"text":"Unix"}]},{"label":["Skills"],"points":[{"start":1306,"end":1309,"text":"Java"}]},{"label":["Degree"],"points":[{"start":1133,"end":1135,"text":"MCA"}]},{"label":["Skills"],"points":[{"start":984,"end":987,"text":"Java"}]},{"label":["Skills"],"points":[{"start":706,"end":717,"text":"Spring Cloud"}]},{"label":["Skills"],"points":[{"start":693,"end":703,"text":"Spring Boot"}]},{"label":["Skills"],"points":[{"start":673,"end":686,"text":" Microservices"}]},{"label":["Skills"],"points":[{"start":500,"end":507,"text":"WebLogic"}]},{"label":["Skills"],"points":[{"start":493,"end":497,"text":"JBOSS"}]},{"label":["Skills"],"points":[{"start":431,"end":439,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":402,"end":406,"text":"MySQL"}]},{"label":["Skills"],"points":[{"start":397,"end":399,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":387,"end":390,"text":"JDBC"}]},{"label":["Skills"],"points":[{"start":382,"end":384,"text":"JSP"}]},{"label":["Skills"],"points":[{"start":373,"end":379,"text":"Servlet"}]},{"label":["Skills"],"points":[{"start":339,"end":344,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":333,"end":336,"text":"JDBC"}]},{"label":["Skills"],"points":[{"start":326,"end":331,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":314,"end":319,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":296,"end":301,"text":"Spring"}]},{"label":["Years_of_Experience"],"points":[{"start":126,"end":134,"text":"3.3 years"}]},{"label":["Email_Address"],"points":[{"start":46,"end":70,"text":"thabira.meher02@gmail.com"}]},{"label":["Mobile_No"],"points":[{"start":32,"end":44,"text":"+919108370860"}]},{"label":["Name"],"points":[{"start":18,"end":30,"text":"Thabira Meher"}]}],"extras":null,"metadata":{"first_done_at":1564209625000,"last_updated_at":1564209625000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "05/05/2019 VARTIKA SARAN Resume - Google Docs\n\nhttps://docs.google.com/document/d/1-_jVCgvJ15U1dmol01vJIZ5Bfgoo7wIPdViqUwTdKWA/edit 1/2\n\n \n\nVARTIKA SARAN \n 2x Salesforce Certified \n\n­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­ \n  Senior Salesforce developer backboned in Java with 5 years exp. in architecturing                       \n\noptimally scalable solutions for complex problems. Lightning proficient, SFDX                 \nEvangelist, skilled in gathering and simplifying business requirements. Worked on                   \nautomation estimating tool reducing manual efforts by 90%. \n­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­ \n\n  +91 9079357910      vartika.saran28@gmail.com   \nhttps://www.linkedin.com/in/vartika‑saran \n\n \n\nAPEX  |  LIGHTNING  |  SFDX |  VISUALFORCE  |  APEX DATA LOADER  |  CONGA COMPOSER  |  DELL BOOMI \nEINSTEIN  |  JAVA  |  JAVASCRIPT  |  JQUERY  |  HTML  |  CSS  |  SOQL  |  SQL  |  ANGULAR  JS  |  GIT  |  ANT  \n \n \n\n    JOB EXPERIENCES \n  ­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­ \n\n     \n \n SENIOR SALESFORCE \n\nDEVELOPER \nJune 2018 – Present \n\n \n \n\nSALESFORCE \nDEVELOPER \n\nJune 2017 – June 2018 \n \n \n\n \n\n  APPIRIO, A Wipro Company, Jaipur, India \n \n\n● Designed, developed and deployed Apex Classes, Controller Classes &                 \nExtensions, Test classes, Batches, Asynchronous Apex, error handling and Apex                   \nTriggers using bulk design patterns for various functional needs in the                     \napplication and Salesforce customizations. \n\n● Experience in handling Salesforce Configurations like Permission setting using                 \nOWD, Profiles, Permission sets, sharing rules, Validation Rules, Workflows,                 \nprocess builder, approval processes etc. \n\n● Worked with Dynamic Apex to access sObjects and Field describe information,                     \nexecute dynamic SOQL, SOSL and DML queries. \n\n● Experience in lightning web components development. Actively worked on                 \nLightning migration of internal Org and creation of custom LEX components. \n\n● Responsible for Dell Boomi process creation and maintenance (Appirio                 \n‑Wipro(Trace) Integration / Workday ‑ SFDC Integration) \n\n \nKey Achievements  \n● Being an internal IT team member donned on multiple hats for end to end                           \n\nproject delivery starting from requirement gathering, analysis to design                 \nwalkthroughs, development, testing and deployment into production org.               \nserving multiple departments across Appirio like Finance, Marketing, HR,                 \nconsulting Ops, transport etc. \n\n● As a ADX(SFDX) evangelist, did a modularized version control POC for an                       \ninternal module and presented it to entire technology team. \n\n      ­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­ \n   \n \n\nASSOCIATE \n DEVELOPER \n\nDec 2016 – May 2017 \n \n \n\nPROGRAMMER \nANALYST \n\nMay 2015 – Dec 2016 \n \n \n\nPROGRAMMER \nANALYST \nTRAINEE \n\nMay 2014 – May 2015 \n \n\n  COGNIZANT Tech. Solutions, Pune, India \n \n\n● Developed software solutions for a leading U.S. bank as a client in Banking &                           \nFinancial Services (BFS) domain. \n\n● Design and development of project as per client requirements to create a fault                         \ntolerant code with efficiency standards on Java platform. \n\n● Wrote unit test cases for the specific unit; seek reviews from peers, resolving                         \nSonarqube/Veracode security fixes related to the project. \n\n● Worked on Java/J2EE technologies, Spring framework, Restful web services,                 \nAngular JS (basics), hibernate, Jsp, Servlets , SQL, DB2, mockito Junit framework. \n\n \nKey Achievements  \n● Enhanced trust customer capabilities through SSO to client portal for finance                     \n\nmanagement through the online banking app. Allowed offers to flow based on                       \ncustomer ‘s eligibility and business rules. \n\n● Assisted in Migration of code from JSP servlet architecture to Spring                     \nmicroservice framework for enabling the service layer for mobile banking and                     \nonline banking apps. \n\n \n \n\n \n\nhttps://www.linkedin.com/in/vartika-saran\n\n\n05/05/2019 VARTIKA SARAN Resume - Google Docs\n\nhttps://docs.google.com/document/d/1-_jVCgvJ15U1dmol01vJIZ5Bfgoo7wIPdViqUwTdKWA/edit 2/2\n\n   PROJECTS    ­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­ \n\n  APPIRIO \nGAMIFICATION ENGINE \n\n‑ SFDX & LEX \nMIGRATION \n\n \n \n \n\n \n\n  Gamification engine used for tracking a person’s “Appirio story” i.e profile \ninformation, for rewards and recognition purpose.  \n● Migrated the application on CI/CD (GitLab CI) tools using SFDX & ADX for                         \n\nversion control (GitLab) including extraction of manifests and retrieving their                   \ndependencies to create a stand alone package for automated Salesforce                   \ndeployments. \n\n● Created custom LEX components for essential modules like badges & Challenges                     \netc. \n\n      ­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­ \n  APPIRIO’S \n\nNEXT‑GENERATION \nESTIMATOR TOOL (E2) \n\n \n\n  Tool used by sales team, to produce proposals faster by calculating all estimates of                           \ndeal at one place. \n● Developed & delivered critical features for the tool. Worked extensively on                     \n\nSlickgrid, Java Script, Jquery, conga composer and Apex along with                   \nresponsibilities for testing, deployments and maintenance. \n\n● Configured Conga templates for automated SOW generation for workday and                   \nsalesforce practices, reducing manual efforts by 90%. \n\n      ­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­ \n  FLEXI TIME REQUEST \n\nPORTAL(FTR) \n \n\n  A feature added in salesforce org to facilitate employees working in Shifts to avail                           \ncompany transportation, meals by raising the requests from this portal. It notifies the                         \nTransportation & facility teams to make the necessary arrangements. \n● Provided solutions by developing approval process, workflow, email               \n\nnotifications and Triggers to implement the functionality. \n       \n\n   EDUCATION    ­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­ \n\n  BACHELORS OF \nTECHNOLOGY \n2009 – 2013 \n\n  Jodhpur Institute of Engineering & Technology, Rajasthan Technical \nUniversity \n\n     \n\n   AWARDS    ­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­ \n\n  PAT ON THE BACK    Award  for  key deliverables on Boomi Integrations and E2 Enhancements. \n\n  SHOUTOUTS    Received Gamification points from my managers, leads & product customers for quality \nand on time delivery \n\n     \n\n   CERTIFICATIONS    ­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­­ \n\n  SALESFORCE \nPLATFORM \n\nDEVELOPER I \n\n  Based on skills and experience to build custom declarative and programmatic \napplications on the Salesforce platform. \n\n  SALESFORCE \nADMINISTRATOR \n\n  Possess broad knowledge of customizing Salesforce, regularly configuring the platform, \nmanaging users, and looking for ways to get even more out of its features and \ncapabilities. \n\n  JAVA    Certified in Core Java, J2EE technologies & Advanced Java from Cognizant Academy, \nChennai \n\n \n \n\n ","annotation":[{"label":["Skills"],"points":[{"start":7578,"end":7582,"text":"JAVA "}]},{"label":["Certifications"],"points":[{"start":7363,"end":7372,"text":"SALESFORCE"}]},{"label":["Certifications"],"points":[{"start":7202,"end":7211,"text":"SALESFORCE"}]},{"label":["Skills"],"points":[{"start":4906,"end":4909,"text":"SFDX"}]},{"label":["Skills"],"points":[{"start":4680,"end":4683,"text":"SFDX"}]},{"label":["Skills"],"points":[{"start":3833,"end":3835,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":2769,"end":2772,"text":"SFDX"}]},{"label":["Skills"],"points":[{"start":1993,"end":1996,"text":"SOQL"}]},{"label":["Certifications"],"points":[{"start":1221,"end":1230,"text":"SALESFORCE"}]},{"label":["Certifications"],"points":[{"start":1170,"end":1179,"text":"SALESFORCE"}]},{"label":["Skills"],"points":[{"start":1030,"end":1033,"text":"ANT "}]},{"label":["Skills"],"points":[{"start":1022,"end":1024,"text":"GIT"}]},{"label":["Skills"],"points":[{"start":1006,"end":1017,"text":"ANGULAR  JS "}]},{"label":["Skills"],"points":[{"start":998,"end":1000,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":989,"end":992,"text":"SOQL"}]},{"label":["Skills"],"points":[{"start":981,"end":983,"text":"CSS"}]},{"label":["Skills"],"points":[{"start":972,"end":975,"text":"HTML"}]},{"label":["Skills"],"points":[{"start":961,"end":966,"text":"JQUERY"}]},{"label":["Skills"],"points":[{"start":946,"end":955,"text":"JAVASCRIPT"}]},{"label":["Skills"],"points":[{"start":937,"end":941,"text":"JAVA "}]},{"label":["Skills"],"points":[{"start":924,"end":931,"text":"EINSTEIN"}]},{"label":["Skills"],"points":[{"start":912,"end":921,"text":"DELL BOOMI"}]},{"label":["Skills"],"points":[{"start":893,"end":906,"text":"CONGA COMPOSER"}]},{"label":["Skills"],"points":[{"start":872,"end":887,"text":"APEX DATA LOADER"}]},{"label":["Skills"],"points":[{"start":856,"end":866,"text":"VISUALFORCE"}]},{"label":["Skills"],"points":[{"start":848,"end":851,"text":"SFDX"}]},{"label":["Skills"],"points":[{"start":834,"end":842,"text":"LIGHTNING"}]},{"label":["Skills"],"points":[{"start":825,"end":828,"text":"APEX"}]},{"label":["Email_Address"],"points":[{"start":749,"end":773,"text":"vartika.saran28@gmail.com"}]},{"label":["Mobile_No"],"points":[{"start":729,"end":742,"text":"+91 9079357910"}]},{"label":["Skills"],"points":[{"start":453,"end":456,"text":"SFDX"}]},{"label":["Years_of_Experience"],"points":[{"start":325,"end":331,"text":"5 years"}]},{"label":["Certifications"],"points":[{"start":156,"end":168,"text":"2x Salesforce"}]},{"label":["Name"],"points":[{"start":140,"end":152,"text":"VARTIKA SARAN"}]}],"extras":null,"metadata":{"first_done_at":1564208310000,"last_updated_at":1564208310000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "Vignesh Kumar\n          +91-7760380978             Email : vigneshkumar2903@gmail.com                       \n\n\n\tCRM Experience\n\nVEEVA CRM\nVEEVA VAULT\nSalesforce Marketing Cloud\nSalesforce Service Cloud\nSalesforce Sales Cloud \n\nSalesforce Certifications\nVeeva CRM Administrator\nVeeva Vault Administrator\nSalesforce PD1 developer\nSalesforce Administrator\n\nKey Technical Skills & Knowledge\n· VEEVA CRM\n· VEEVA VAULT\n· Salesforce.com \n· Data migration \n(Apex data loader, IOD and workbench) \n· Deployments \n(Force.com IDE, Change sets and Workbench)\nExcellent Communication & Presentation Skills\n\n\t\n\t                                                   Vignesh Kumar \nProfile Summary\nHaving 6.5 years of experience in the Information Technology. Involved in POC’s, Requirement Analysis, effort estimation, development, Application Maintenance, and Deployment.\n\n· Manage team effectively and takes care of client communications\n· Create data model by analyzing the requirement including Objects, Fields, Lookup, Master Detail relationships among objects.\n· Create Workflows including field updates, sending emails, time dependent workflows, Approval Process to automate the business process.\n· Create Formulas, Validation Rules and Triggers to validate data and calculate values automatically \n· Organize different types of Page layouts and Record Types to support different business process.\n· Creating, manage and assign Profiles and Permission sets with set of permissions which controls what a user can do in the organization.\n· Maintain security at Organization, Object level, Record level and Field level security using Profiles, OWD settings, Sharing Settings, Roles and Public Groups\n· Creating Report Types and create Reports based on report types to analyze the data and create Dashboards to show reports in graphically for quick view and easy understanding.\n· Scheduling Reports to different users and Schedule Dashboards refresh.\n· Create Deployment Connections and Deploy Meta data changes to/from one Sandbox to other and to Production using Change Sets and Eclipse IDE. \n· Create sandboxes and refreshing sandboxes at regular intervals.\n· Perform Unit testing and preparing Test Case documents.\n· Preparing deployment checklist and IQOQ documents including post deployment and pre-deployment steps.\n· Experience with data migration using Apex Data loader and Workbench\n· Debug Apex Classes, Apex Triggers and creating Visual Force Pages.\n· Self-motivated, ready to learn new things, takes challenges, and a good team player.\n· Learning Marketing Cloud and Python.\n\n\n\n\nTechnical Skills: Salesforce, Veeva CRM, Veeva Vault, SOQL, Apex Data loader, Workbench, ANT, IDE etc.,\n\nEducational Qualification: Completed B-Tech in 2012 from SRM Valliammai Engineering College, Anna University, Chennai\nProfessional Experience: Working as Veeva CRM / Salesforce developer with Accenture.\nProject Details:\nPROJECT: TAKEDA SFDC AO &   ENHANCEMENTS\n\nCLIENT: TAKEDA PHARMACEUTICALS LTD\n\nCRM TYPE: VEEVA CRM, SALESFORCE CRM, VEEVA VAULT, SALESFORCE MARKETING CLOUD, SERVICE CLOUD\n\nProject TYPE: SUPPORT, Enhancement, CONFIGURATION, DEPLOYMENT, Data Migration\n\nAbout Client:\n          Takeda Pharmaceuticals is the largest pharmaceutical in Japan and Asia and one of the top 15 pharmaceutical company. The company has over 30,000 employees worldwide and achieved $18.5 billion USD in revenue during the 2017 fiscal year. The company is focused on disorders, gastroenterology, neurology, inflammation, as well as oncology through its independent subsidiary, Millennium: The Takeda Oncology Company. Its headquarters is located in Chuo-ku, Osaka.\n\nResponsibilities:\nTeam Lead:\n· Taking care of multiple applications which include Veeva CRM for all regions (APAC, EUCAN, USA) and Veeva Vault for all regions which includes \nPromoMats (includes Multichannel), ETMF, MedComms, Submissions.\n· Working as Transition Lead for all applications related to Veeva CRM, Veeva Vault & Salesforce where Vendor provides KT to our team for new & existing applications to be added in scope.\n· Currently leading a team of 20+ team members.\n· Responsible for End to End deliverables for all applications which are in scope related to VEEVA & Salesforce\n· Gather requirements and suggest technical solutions to client in time bound manner.\n· Involved in resource management and technical solutioning during bidding process from Accenture side\nSenior Support Analyst: \n\n· Designated as Onshore Lead for EUCAN region for period of 1.5 years based out of Madrid, Spain from May 2016 to November 2017.\n· Currently designated as Offshore Lead leading 15+ members involving multiple regions.\n· Responsible for Client communication, escalation management, coordination of different vendors\n· Lead the team of 7 members from onshore(Madrid).\n· Support lead for the Veeva CRM which is built on salesforce platform and Vault application which underwent entire Europe and Canada countries rollout which is approx. 20 countries\n· Single POC for support in onshore for Veeva CRM and Vault Application.\n· Complete E2E support of the application to the customers\n· Mainly involved in functional categorization of the issue and technical decision making\n· Responsible for handling high priority issues and discussion with local country admins on day to day basis\n· Worked with business in person which involves requirement gathering and providing technical inputs to the queries.\n· Have also worked as offshore team lead, responsible for attending client calls along with onsite counterpart and preparing weekly status report \n· Analyzing client requirements, doing POC and giving Alpha, Beta demos to clients\n· Configuring and managing iRep application (offline iPad app for sales reps)\n· Expertise in Single Sign on Configuration\n·  Designing solutions and deploying to sandboxes for testing.\n· Well-equipped to support all Multichannel platform related issues in Veeva.\n· Configuring workflows, Approval Process to automate business.\n· Creating Validation rules, formula fields and triggers to validate data.\n· Creating Reports and Dashboards and storing them in folders to give access to different groups and schedule them to users at diff frequency\n· Managing security at Organization level, Object level, Record level, field level by using Profiles, roles, sharing rules\n· Data migration using Apex data loader and workbench\n· Working with different VEEVA modules like Accounts- Territories, Products Catalog, Sample Management, Multichannel modules, Call Reporting especially with iPad Application\n· Also, have exposure to Veeva Network and Veeva Align\n· Preparing system test scripts, UAT scripts and IQOQ\n· Working on production support issues and resolving in time.\n· Experience in working with Salesforce.com Sandbox, Production Environments and periodically inform client related to environment management.\n· Creating workflows and approval process and validation rules, formulas.\n· Creating report types and reports and stored in different folders and scheduling to different users.\n· Creating dashboards and define running user settings and schedule refreshes.\n· Maintaining security setup at Org level, object level, record level and field level using profiles, roles and sharing rules.\n· Deploy metadata from one sandbox to other sandboxes and production using change sets, Workbench, ANT etc.,\n· Data Management - Data Loader, and Import Wizard, Mass Data Transfers, etc\n· Recently underwent training for Salesforce Marketing cloud and aware of basics\n· Have good understanding in Salesforce service cloud application.\n· Primary POC for all vault related issues\n· Vault have review, approval and Multichannel digital content platform which is specifically designed to manage content as per Pharma Industry standards\n· Have very good domain specific knowledge which allows to resolve issue understanding business process related to Life Sciences Industry.\n\nSupport Analyst:\n\n· Handled a team of 5 members with all success by getting deliverable done within deadlines which includes both USA and Europe applications\n· Complete E2E support of the application to the customers.\n· Expertise in Veeva support which involves resolving complex issues\n· POC for US application offshore team during its initial rollout and support\n· Received multiple appreciations from client and Accenture Management for smooth delivery\n· Responsible for the Veeva release deployments in US applications\n· Expertise in configuration of Page Layouts, Field Level Security, Profile Settings, Validation rules, OWD, VMOC and all Veeva modules.\n· Basic Knowledge exposure to Apex Classes, Apex Triggers and Visualforce pages\n· Worked on data loader, workbench for various data loads \n· Worked on Informatica cloud to automate various data load processes.\n· Designed the System and Unit Test cases. Performed the System and Unit Test case executions and corrected the system as per the issues.\n· Developed solutions for couple of minor releases in US application.\n\nAssociate:\n\n· Help senior team members for completing day to day activities\n· Develop Unit test cases and ensure testing is successful for new enhancements\n· Work on simple issues and ensure there are no SLA miss \n· Involve in realignment activities and help in data loading for objects\n· Responsible for refresh of sandboxes and ensure loading of required data for testing\n· Have minimal knowledge on Siebel Administrator activities\n· Part of Siebel to Salesforce Migration team which also includes avoiding loss of data in the process of migration\n· Underwent training for Salesforce and completed Dev 401 certification\n· Gained team and client confidence for support activities to project team during rollouts and enhancements phase","annotation":[{"label":["Skills"],"points":[{"start":9580,"end":9589,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":9459,"end":9468,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":8691,"end":8699,"text":"workbench"}]},{"label":["Skills"],"points":[{"start":7551,"end":7560,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":7475,"end":7484,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":7354,"end":7356,"text":"ANT"}]},{"label":["Skills"],"points":[{"start":7343,"end":7351,"text":"Workbench"}]},{"label":["Skills"],"points":[{"start":6758,"end":6771,"text":"Salesforce.com"}]},{"label":["Skills"],"points":[{"start":6374,"end":6382,"text":"workbench"}]},{"label":["Skills"],"points":[{"start":6353,"end":6368,"text":"Apex data loader"}]},{"label":["Skills"],"points":[{"start":6332,"end":6345,"text":"Data migration"}]},{"label":["Skills"],"points":[{"start":5025,"end":5033,"text":"Veeva CRM"}]},{"label":["Skills"],"points":[{"start":4826,"end":4834,"text":"Veeva CRM"}]},{"label":["Skills"],"points":[{"start":4212,"end":4221,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":3961,"end":3970,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":3947,"end":3957,"text":"Veeva Vault"}]},{"label":["Skills"],"points":[{"start":3936,"end":3944,"text":"Veeva CRM"}]},{"label":["Skills"],"points":[{"start":3767,"end":3777,"text":"Veeva Vault"}]},{"label":["Skills"],"points":[{"start":3718,"end":3726,"text":"Veeva CRM"}]},{"label":["Skills"],"points":[{"start":3016,"end":3026,"text":"VEEVA VAULT"}]},{"label":["Skills"],"points":[{"start":2989,"end":2997,"text":"VEEVA CRM"}]},{"label":["Skills"],"points":[{"start":2847,"end":2856,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":2835,"end":2843,"text":"Veeva CRM"}]},{"label":["Degree"],"points":[{"start":2718,"end":2723,"text":"B-Tech"}]},{"label":["Skills"],"points":[{"start":2665,"end":2667,"text":"ANT"}]},{"label":["Skills"],"points":[{"start":2654,"end":2662,"text":"Workbench"}]},{"label":["Skills"],"points":[{"start":2636,"end":2651,"text":"Apex Data loader"}]},{"label":["Skills"],"points":[{"start":2630,"end":2633,"text":"SOQL"}]},{"label":["Skills"],"points":[{"start":2617,"end":2627,"text":"Veeva Vault"}]},{"label":["Skills"],"points":[{"start":2606,"end":2614,"text":"Veeva CRM"}]},{"label":["Skills"],"points":[{"start":2594,"end":2603,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":2367,"end":2375,"text":"Workbench"}]},{"label":["Skills"],"points":[{"start":2346,"end":2361,"text":"Apex Data loader"}]},{"label":["Years_of_Experience"],"points":[{"start":685,"end":693,"text":"6.5 years"}]},{"label":["Name"],"points":[{"start":647,"end":659,"text":"Vignesh Kumar"}]},{"label":["Skills"],"points":[{"start":535,"end":543,"text":"Workbench"}]},{"label":["Skills"],"points":[{"start":519,"end":529,"text":"Change sets"}]},{"label":["Skills"],"points":[{"start":504,"end":516,"text":"Force.com IDE"}]},{"label":["Skills"],"points":[{"start":490,"end":500,"text":"Deployments"}]},{"label":["Skills"],"points":[{"start":476,"end":484,"text":"workbench"}]},{"label":["Skills"],"points":[{"start":468,"end":470,"text":"IOD"}]},{"label":["Skills"],"points":[{"start":450,"end":465,"text":"Apex data loader"}]},{"label":["Skills"],"points":[{"start":433,"end":446,"text":"Data migration"}]},{"label":["Skills"],"points":[{"start":415,"end":428,"text":"Salesforce.com"}]},{"label":["Skills"],"points":[{"start":401,"end":411,"text":"VEEVA VAULT"}]},{"label":["Skills"],"points":[{"start":389,"end":397,"text":"VEEVA CRM"}]},{"label":["Skills"],"points":[{"start":328,"end":337,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":303,"end":312,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":277,"end":287,"text":"Veeva Vault"}]},{"label":["Skills"],"points":[{"start":253,"end":261,"text":"Veeva CRM"}]},{"label":["Skills"],"points":[{"start":227,"end":236,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":202,"end":211,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":177,"end":186,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":150,"end":159,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":138,"end":148,"text":"VEEVA VAULT"}]},{"label":["Skills"],"points":[{"start":128,"end":136,"text":"VEEVA CRM"}]},{"label":["Email_Address"],"points":[{"start":59,"end":84,"text":"vigneshkumar2903@gmail.com"}]},{"label":["Mobile_No"],"points":[{"start":23,"end":37,"text":" +91-7760380978"}]},{"label":["Name"],"points":[{"start":0,"end":12,"text":"Vignesh Kumar"}]}],"extras":null,"metadata":{"first_done_at":1564208533000,"last_updated_at":1564208533000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "Vijay Shandilya\n\nPhone : +91-9900110798\nEmail : svijayshandilya@gmail.com\nAddress: 312 LIG 707 4th Phase \n\n4th Cross Yelahanka Newtwon Bangalore – 560064\n\nDOB: 25th Feb 1988\n\nSUMMARY:\nCertified Data Scientist with dynamic career of 11 years in application support & ETL and year on year success in Data analytics, Business Intelligence. Proven track record in Oracle database, Informatica and ITIL framework. Performed various data analysis of business data which impacted the sales and revenue for the BU. Forecasting revenue using time series algorithms was one of the most contributing factors for FY18. Further deep dived into the data warehouse to perform the predictions of the sales to the product level. Strong knowledge on Data preparation, data modelling and evaluation of the output from the data modelling. Also used Tableau for visualizing different datasets like finance and sales connecting to the data warehouse and MS Analysis servers. Extensively used SQL to drill down the data to find the particular sale impacting the warehouse business. Hands on experience on Salesforce to perform data quality checks on total pipeline of business opportunities pertaining to the BU. Worked on setting up a new BI&A project infrastructure starting from requirement analysis of hardware, application and support system framework, Oracle DB installation, ASM configuration analysing the business requirements and designing the ITIL framework as per the business understanding.My GitHub link gives the complete information on my work on different projects on analytics performed on different datasets. The data points gave different insights and tried best in python for visualization of the KPI's too. \n\nA forward-thinking person with strong Multi-lingual communication, analytical & organizational skills; well organized with a track record that demonstrates self-motivation & creativity to achieve corporate goal.\nSKILLS\n· Predective analatics data modeling: Worked on various data mining POC e.g. Predictive Analytics, Aspect based sentiment analysis using IBM Watson.\n· Tableau: Creating dashoard using different data sources.\n\n· Informatica Powercenter ETL: Created mappings for different datasets in Retail and supply chain.\n\n· Python: Used for different regression tests like linear regression and time series.\n· Oracle DBA, Oracle EM: Installed Oracle ASM 12c, Database and configured Database 11g on ASM. Attended one week training from oracle certified trainer\n· SQL.\n· UNIX: Installed oracle Linux for database environment and created cron jobs to run weekly and monthly backups.\n\n· Salesforce: Extracting opp data and performing reconciliation of all the opp with PDW on daily and monthly basis. \n\n· Tidal Enterprise scheduler: Configured Tidal agent, master and scheduler. Created and managing workgroups, jobs, scheduler and users adminstration.\n\n· HP Incident management process.\n\n· HPE SDLCM: Created RFC's to modify the tidal jobs according to the requirement.\n\n· HPE Service Management (HPSM)\n\n· BigBrother: BigBrother is a monitoring tool basically used to monitor different retail systems in TESCO starting from billing devices to network ports.\n\n· Autosys: Monitoring tool for job monitoring\n\n· Sitescope: Sitescope is a monitoring tool basically used to monitor the status of servers connected with the agent.\n\n· Microsoft Windows System Administration: AD Management, NTBackup and User administration\n· Requirement Analysis: Worked on Smart Parking IOT Project for Madhya Pradesh State Govt.\nWORK EXPERIENCE:\n\nMindteck India ltd\nETL specialist\n\n11/2010 - Present\n\n· Experienced in UNIX, writing SQL queries and optimizing the queries in Oracle.\n\n· Excellent knowledge in Data Analysis, Data Validation, Data Cleansing, Data Verification and identifying data mismatch.\n\n· Performed data analysis and data profiling using complex SQL on sources systems like Oracle and SAP Business object.\n\n· Strong experience in using Excel and MS Access to dump the data and analyse based on business needs.\n\n· Experience in automating and scheduling the Tidal jobs using UNIX shell scripting configuring corn-jobs for automation of sessions.\n\n· Excellent knowledge on creating reports on SAP Business Objects, Orion reports for multiple data providers.\n\n· Excellent knowledge in preparing required project documentation, tracking and reporting regularly on the status of projects to all project stakeholders\n\n· Extensive knowledge and experience in producing tables, reports, graphs and listings using various procedures and handling large databases to perform complex data manipulations.\n\n· Excellent experience in Data mining with querying and mining large datasets to discover transition patterns and examine financial data.\n\n· Excellent knowledge on creating DML statements for underlying statements.\n\n· Have good exposure on working in offshore/onsite model with ability to understand and/or create functional requirements working with client and also have Good experience in requirement analysis and generating test artifacts from requirements docs.\n\n· Knowledge in BigData and Hadoop\n\n· Software Development Life Cycle (SDLCM) with good working knowledge of change management.\n\n· Worked in disaster recovery in regards to ensuring that all documents are current and correct. Made suggestions on how to improve based on previous issues.\n\n· Exposure to ITIL framework, Handel client contacts and engage as and when required. Work within accounts to help between establish processes.\n\n· Manage issues as they occur, providing the appropriate vetting of the severity of the issue and performing the required management of the high impacting severity\n\n· An excellent team player & technically strong person who has capability to work with business users, project managers, team leads, architects and peers, thus maintaining healthy environment in the project.\n\nTESCO HSC\n\nApplication Support\n09/2009 – 10/2010\n· Understanding business needs.\n\n· This was a First Remote Transitions which was done due to budget constraints with very minimal onsite travel & was completed within agreed timelines with Stakeholders\n\n· SPOC for any escalations after my Team Lead & handled the team in his absence\n\n· Created Various Project Documents & also planned the Knowledge acquisition Plan for the transition\n\n· Attended conference call with stakeholders to take their feedback on the operations.\n\n· Batch Monitoring using CA applications like Autosys\n\n· Performing Flash Copy Backups on Databases, taking tape backups using backup tool\n\n· Performing various other activities related to Retail.\n\n· Created various documents on projects & completed the project as per the agreed time lines with the stakeholders\nIBM India (contract)\n\nServer support\n01/2007 – 09/2009\n· Windows 2000/2003 administration\n\n· Symantec antivirus server administration\n\n· Windows Backup and Storage\n\n· Incident Management\n\n· Primary System Administrator for Windows 2000 & 2003.\n\n· Installing, Configuring & Maintaining Windows 2003 Standard & Enterprise Edition.\n\n· User Account Administrator.\n\n· Active Directory Services & Administering user Accounts in ADS\n\n· Designing windows 2003 Active Directory Services Infrastructure\n\n· Managing & Maintaining Group Policy, Active Directory Infrastructure\n\n· Symantec Antivirus server administration\n\n· Install software releases, system upgrades and patches to all OS and applications for the production environment routinely troubleshooting advanced technical issues with Microsoft systems including but not limited to Active Directory\n\n· Customer issue resolution and assisting other system administrators.\n\n· Maintaining backup/recovery procedures daily, weekly, Monthly Backups using NTBACKUP\n\n· Tape Management\n\nEducation:\n\n· MBA in Business analytics Reva University- present\n\n· Bachelors of Science in Information Technology, Sikkim Manipal University \n· Diploma in Electronics and Communication engineering, MN Technical Institute, Bangalore \nCERTIFICATES:\n· ITIL V3 Foundation – EXIN\n\n· Microsoft Certified System Administrator Messaging\n\n· IBM Certified Data Scientist Mastery Award\n\n· IBM Certified Predictive Analytics Modeler Mastery Award\n· UiPath Certified Robot Methodologist\n\n· UiPath Certified Robot Analyst\nExternal Activities:\n· Creating Tableau dashboards for Rail Wheel Factory (Indian Railways).\n· Volenteer and create dashboards showing the benefits of business intelligence to local retailers.\n\n· Volenteer for Rotary India, working with different datasets to get insights.\n\nGithub link: https://github.com/svijayshandilya/","annotation":[{"label":["Skills"],"points":[{"start":8278,"end":8284,"text":"Tableau"}]},{"label":["Certifications"],"points":[{"start":8175,"end":8211,"text":"UiPath Certified Robot Methodologist\n"}]},{"label":["Certifications"],"points":[{"start":8116,"end":8171,"text":"IBM Certified Predictive Analytics Modeler Mastery Award"}]},{"label":["Certifications"],"points":[{"start":8070,"end":8112,"text":"IBM Certified Data Scientist Mastery Award\n"}]},{"label":["Certifications"],"points":[{"start":8016,"end":8066,"text":"Microsoft Certified System Administrator Messaging\n"}]},{"label":["Certifications"],"points":[{"start":7986,"end":8004,"text":" ITIL V3 Foundation"}]},{"label":["Degree"],"points":[{"start":7751,"end":7753,"text":"MBA"}]},{"label":["Tools"],"points":[{"start":6444,"end":6450,"text":"Autosys"}]},{"label":["Skills"],"points":[{"start":4083,"end":4086,"text":"UNIX"}]},{"label":["Skills"],"points":[{"start":3855,"end":3857,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":3622,"end":3624,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":3608,"end":3611,"text":"UNIX"}]},{"label":["Tools"],"points":[{"start":3231,"end":3239,"text":"Sitescope"}]},{"label":["Tools"],"points":[{"start":3220,"end":3228,"text":"Sitescope"}]},{"label":["Tools"],"points":[{"start":3173,"end":3179,"text":"Autosys"}]},{"label":["Tools"],"points":[{"start":3030,"end":3039,"text":"BigBrother"}]},{"label":["Tools"],"points":[{"start":3018,"end":3027,"text":"BigBrother"}]},{"label":["Skills"],"points":[{"start":2985,"end":3013,"text":"HPE Service Management (HPSM)"}]},{"label":["Skills"],"points":[{"start":2902,"end":2910,"text":"HPE SDLCM"}]},{"label":["Skills"],"points":[{"start":2716,"end":2741,"text":"Tidal Enterprise scheduler"}]},{"label":["Skills"],"points":[{"start":2598,"end":2607,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":2484,"end":2487,"text":"UNIX"}]},{"label":["Skills"],"points":[{"start":2477,"end":2479,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":2336,"end":2344,"text":"Oracle EM"}]},{"label":["Skills"],"points":[{"start":2324,"end":2333,"text":"Oracle DBA"}]},{"label":["Skills"],"points":[{"start":2238,"end":2243,"text":"Python"}]},{"label":["Skills"],"points":[{"start":2138,"end":2164,"text":"Informatica Powercenter ETL"}]},{"label":["Skills"],"points":[{"start":2078,"end":2084,"text":"Tableau"}]},{"label":["Skills"],"points":[{"start":2064,"end":2073,"text":"IBM Watson"}]},{"label":["Skills"],"points":[{"start":2039,"end":2056,"text":"sentiment analysis"}]},{"label":["Skills"],"points":[{"start":1929,"end":1962,"text":"Predective analatics data modeling"}]},{"label":["Skills"],"points":[{"start":1082,"end":1091,"text":"Salesforce"}]},{"label":["Skills"],"points":[{"start":970,"end":972,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":829,"end":835,"text":"Tableau"}]},{"label":["Years_of_Experience"],"points":[{"start":232,"end":239,"text":"11 years"}]},{"label":["Email_Address"],"points":[{"start":48,"end":72,"text":"svijayshandilya@gmail.com"}]},{"label":["Mobile_No"],"points":[{"start":25,"end":38,"text":"+91-9900110798"}]},{"label":["Name"],"points":[{"start":0,"end":14,"text":"Vijay Shandilya"}]}],"extras":null,"metadata":{"first_done_at":1564056212000,"last_updated_at":1564056212000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "Vijay Shandilya\n\nPhone : +91-9900110798\nEmail : svijayshandilya@gmail.com\nAddress: 312 LIG 707 4th Phase \n\n4th Cross Yelahanka Newtwon Bangalore – 560064\n\nDOB: 25th Feb 1988\n\nSUMMARY:\nCertified Data Scientist with dynamic career of 11 years in application support & ETL and year on year success in Data analytics, Business Intelligence. Proven track record in Oracle database, Informatica and ITIL framework. Performed various data analysis of business data which impacted the sales and revenue for the BU. Forecasting revenue using time series algorithms was one of the most contributing factors for FY18. Further deep dived into the data warehouse to perform the predictions of the sales to the product level. Strong knowledge on Data preparation, data modelling and evaluation of the output from the data modelling. Also used Tableau for visualizing different datasets like finance and sales connecting to the data warehouse and MS Analysis servers. Extensively used SQL to drill down the data to find the particular sale impacting the warehouse business. Hands on experience on Salesforce to perform data quality checks on total pipeline of business opportunities pertaining to the BU. Worked on setting up a new BI&A project infrastructure starting from requirement analysis of hardware, application and support system framework, Oracle DB installation, ASM configuration analysing the business requirements and designing the ITIL framework as per the business understanding.My GitHub link gives the complete information on my work on different projects on analytics performed on different datasets. The data points gave different insights and tried best in python for visualization of the KPI's too. \n\nA forward-thinking person with strong Multi-lingual communication, analytical & organizational skills; well organized with a track record that demonstrates self-motivation & creativity to achieve corporate goal.\nSKILLS\n· Predective analatics data modeling: Worked on various data mining POC e.g. Predictive Analytics, Aspect based sentiment analysis using IBM Watson.\n· Tableau: Creating dashoard using different data sources.\n\n· Informatica Powercenter ETL: Created mappings for different datasets in Retail and supply chain.\n\n· Python: Used for different regression tests like linear regression and time series.\n· Oracle DBA, Oracle EM: Installed Oracle ASM 12c, Database and configured Database 11g on ASM. Attended one week training from oracle certified trainer\n· SQL.\n· UNIX: Installed oracle Linux for database environment and created cron jobs to run weekly and monthly backups.\n\n· Salesforce: Extracting opp data and performing reconciliation of all the opp with PDW on daily and monthly basis. \n\n· Tidal Enterprise scheduler: Configured Tidal agent, master and scheduler. Created and managing workgroups, jobs, scheduler and users adminstration.\n\n· HP Incident management process.\n\n· HPE SDLCM: Created RFC's to modify the tidal jobs according to the requirement.\n\n· HPE Service Management (HPSM)\n\n· BigBrother: BigBrother is a monitoring tool basically used to monitor different retail systems in TESCO starting from billing devices to network ports.\n\n· Autosys: Monitoring tool for job monitoring\n\n· Sitescope: Sitescope is a monitoring tool basically used to monitor the status of servers connected with the agent.\n\n· Microsoft Windows System Administration: AD Management, NTBackup and User administration\n· Requirement Analysis: Worked on Smart Parking IOT Project for Madhya Pradesh State Govt.\nWORK EXPERIENCE:\n\nMindteck India ltd\nETL specialist\n\n11/2010 - Present\n\n· Experienced in UNIX, writing SQL queries and optimizing the queries in Oracle.\n\n· Excellent knowledge in Data Analysis, Data Validation, Data Cleansing, Data Verification and identifying data mismatch.\n\n· Performed data analysis and data profiling using complex SQL on sources systems like Oracle and SAP Business object.\n\n· Strong experience in using Excel and MS Access to dump the data and analyse based on business needs.\n\n· Experience in automating and scheduling the Tidal jobs using UNIX shell scripting configuring corn-jobs for automation of sessions.\n\n· Excellent knowledge on creating reports on SAP Business Objects, Orion reports for multiple data providers.\n\n· Excellent knowledge in preparing required project documentation, tracking and reporting regularly on the status of projects to all project stakeholders\n\n· Extensive knowledge and experience in producing tables, reports, graphs and listings using various procedures and handling large databases to perform complex data manipulations.\n\n· Excellent experience in Data mining with querying and mining large datasets to discover transition patterns and examine financial data.\n\n· Excellent knowledge on creating DML statements for underlying statements.\n\n· Have good exposure on working in offshore/onsite model with ability to understand and/or create functional requirements working with client and also have Good experience in requirement analysis and generating test artifacts from requirements docs.\n\n· Knowledge in BigData and Hadoop\n\n· Software Development Life Cycle (SDLCM) with good working knowledge of change management.\n\n· Worked in disaster recovery in regards to ensuring that all documents are current and correct. Made suggestions on how to improve based on previous issues.\n\n· Exposure to ITIL framework, Handel client contacts and engage as and when required. Work within accounts to help between establish processes.\n\n· Manage issues as they occur, providing the appropriate vetting of the severity of the issue and performing the required management of the high impacting severity\n\n· An excellent team player & technically strong person who has capability to work with business users, project managers, team leads, architects and peers, thus maintaining healthy environment in the project.\n\nTESCO HSC\n\nApplication Support\n09/2009 – 10/2010\n· Understanding business needs.\n\n· This was a First Remote Transitions which was done due to budget constraints with very minimal onsite travel & was completed within agreed timelines with Stakeholders\n\n· SPOC for any escalations after my Team Lead & handled the team in his absence\n\n· Created Various Project Documents & also planned the Knowledge acquisition Plan for the transition\n\n· Attended conference call with stakeholders to take their feedback on the operations.\n\n· Batch Monitoring using CA applications like Autosys\n\n· Performing Flash Copy Backups on Databases, taking tape backups using backup tool\n\n· Performing various other activities related to Retail.\n\n· Created various documents on projects & completed the project as per the agreed time lines with the stakeholders\nIBM India (contract)\n\nServer support\n01/2007 – 09/2009\n· Windows 2000/2003 administration\n\n· Symantec antivirus server administration\n\n· Windows Backup and Storage\n\n· Incident Management\n\n· Primary System Administrator for Windows 2000 & 2003.\n\n· Installing, Configuring & Maintaining Windows 2003 Standard & Enterprise Edition.\n\n· User Account Administrator.\n\n· Active Directory Services & Administering user Accounts in ADS\n\n· Designing windows 2003 Active Directory Services Infrastructure\n\n· Managing & Maintaining Group Policy, Active Directory Infrastructure\n\n· Symantec Antivirus server administration\n\n· Install software releases, system upgrades and patches to all OS and applications for the production environment routinely troubleshooting advanced technical issues with Microsoft systems including but not limited to Active Directory\n\n· Customer issue resolution and assisting other system administrators.\n\n· Maintaining backup/recovery procedures daily, weekly, Monthly Backups using NTBACKUP\n\n· Tape Management\n\nEducation:\n\n· MBA in Business analytics Reva University- present\n\n· Bachelors of Science in Information Technology, Sikkim Manipal University \n· Diploma in Electronics and Communication engineering, MN Technical Institute, Bangalore \nCERTIFICATES:\n· ITIL V3 Foundation – EXIN\n\n· Microsoft Certified System Administrator Messaging\n\n· IBM Certified Data Scientist Mastery Award\n\n· IBM Certified Predictive Analytics Modeler Mastery Award\n· UiPath Certified Robot Methodologist\n\n· UiPath Certified Robot Analyst\nExternal Activities:\n· Creating Tableau dashboards for Rail Wheel Factory (Indian Railways).\n· Volenteer and create dashboards showing the benefits of business intelligence to local retailers.\n\n· Volenteer for Rotary India, working with different datasets to get insights.\n\nGithub link: https://github.com/svijayshandilya/","annotation":null,"extras":null,"metadata":{"first_done_at":1564229628000,"last_updated_at":1564229628000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "V S PRAVEEN\nEmail: vspraveen1132@gmail.com\t\t\t\t\t \n\nPhone:  +91-8639524955\n\n\n\nPROFILE SUMMARY:\t\n\n· Having 6 years of experience in Salesforce.com CRM, Force.com platform as Developer and Administrator.\n· Experience in Requirement Capturing from client, Designing, Development, Testing and Deploying Sales force applications.\n· Experience in working on Sales Cloud and Service Cloud\n· Expertise in Salesforce.com CRM, Force.com, Apex Classes, and Apex triggers, Visual force, Force.com API, SOQL and SOSL.\n· Experience in customizing standard objects, creating roles and Profiles and configured the permissions based on the organizational hierarchy.\n· Designed custom Formula Fields, Validation Rules, Field Dependencies, Workflows, Approval Processes, Page layouts and search layout.\n· Implemented security and sharing rules at object, field, and record level for different users at different levels of organization, also created various profiles and configured the permissions based on the organizational hierarchy.\n· Experience on Eclipse IDE with Force.com Plug-in for writing business logic in Apex programming language, Visual Force.\n· Experience in Batch Processing (Batch Apex) with scheduling (Manual & Automated).\n· Extensive experience in writing Test classes and Test Coverage for all the apex classes designed.\n· Involved in Code review, to improve performance tuning and code optimization.\n· Experience with Agile Methodology Development (Scrum Methodology).\n· Developed SFDC Customized Reports and Dashboards.\n· Experience working with Force.com IDE, Data Loader, Import Wizard and salesforce.com Sandbox Environments.\n· Having good communication and interpersonal skills.\nPROFESSIONAL EXPERIENCE:\n\n· Worked as a Software Developer in Red Berry Software Technologies Pvt. Ltd. from November 2012 to May 2016.\n· Worked as a Senior Enterprise Solution Developer in UST Global from June 2016 to July 2017.\n· Working as Senior Software Engineer in YBR Infotechs Pvt. Ltd. from August 2017 to till date.\nTECHNICAL SKILLS:\n\n\tProgramming Languages\n\tApex, Visual Force\n\n\tWeb Technologies\n\tHTML, Java Script, CSS \n\n\tPlatforms                            \n\tForce.com \n\n\tDevelopment Tools              \n\tForce IDE, Salesforce.com console, Eclipse\n\n\tData Migration Tool            \n\tApex Data Loader (GUI & Command Line), Workbench  \n\n\tMigration/Deployment Tools\n\tChange Set, Force.com IDE, Eclipse, ANT, Perforce\n\n\tIntegration knowledge\n\tSOAP, REST\n\n\n\n\nCERTIFICATIONS:\n\n· Salesforce.com Certified Platform Developer I\nACADEMIC PROFILE:\n\n· Completed M. Tech. (E.C.E) from Koneru Lakshmaiah University, 2012 with 8.1 CGPA.\n\nPROJECTS PROFILE:\n\nProject 1\nTitle \t\t\t\t: \tRFO\nClient \t\t\t\t: \tDell Solutions\nRole \t\t\t\t: \tDesign & Implementation\nArchitecture             \t\t: \tMVC\nEnvironment \t\t\t: \tSalesforce, apex and Visual force, HTML, JS, CSS etc.,\nSolution Components \t\t:  \tForce.com (SFDC), Order Tracking System (DELL), \nTeam Size \t\t\t: \t8\nProject Description:  \nDELL is one of the big products based Million $ business company in US. DELL is building a new salesforce application to sell their products to their customers based on Order with specific line of business. In this project an EMI process was built for client to generate the billing schedules. A standard salesforce approval process has been implemented for DELL Product availability and status updates. \nResponsibilities:\n· Daily client calls and task updates.\n· Requirement Analysis and estimations.\n· Design Approach.\n· User setup, security Model and Salesforce Customizations.\n· Report Building and Dashboards \n· Visual force Pages and Apex class functionality\n· Unit Testing\n· Deployment and Task status Update.\n\nProject 2\nTitle \t\t\t\t: \tProduct Management System \nClient \t\t\t\t: \tBTU Solutions\nRole \t\t\t\t: \tAdministrator and Developer\nArchitecture             \t\t:            \tMVC\nEnvironment \t\t\t: \tSalesforce, apex and Visual force, HTML, JS etc.,\nSolution Components        \t:  \tForce.com (SFDC) \nTeam Size \t\t\t: \t4\nProject Description:\nIn this Project, all the products in the organization are to be displayed in their website. Each product has a primary photo, description, price book associated with it. We need to display them in their website. Developed a site in salesforce.com to display their products with pictures and embedded it in their style sheet. Also implemented search criteria on category based and user search.\n\nResponsibilities:\n· Requirement analysis and estimations\n· Design Approach\n· Salesforce customization (Force.com Sites)\n· Apex coding & Unit Testing\n· Visual force Development\nProject 3\nTitle\t\t\t\t:\tNetApp GTM  \nClient\t\t\t\t:\tNetApp\nRole\t\t\t\t:\tDeveloper and Administrator\nArchitecture\t\t\t:\tMVC\nEnvironment\t\t\t:\tSalesforce, Apex and Visual force Programming\nSolution Components\t\t:\tForce.com (SFDC)\nTeam Size\t\t\t:\t15\n\nDescription:  \nNetApp is an American multinational storage and data management company. NetApp offers software, systems and services to manage and store data, including its proprietary Data ONTAP operating system.\nOver View:\n· NetApp Salesforce CRM is a sales cloud application which takes care of sales cycle.\n· Salesforce application is mainly used by NetApp internal employees, their partners and distributors. Salesforce is widely used by sales Rep, system engineers, hire management users and marketing users.\n· Main flow is Lead - >Contact->Account->Opportunity.\nResponsibilities:  \n· Performed the roles of Salesforce.com Developer and Administrator in the organization\n· Worked with standard Salesforce.com objects like Accounts, Contacts, Leads and Opportunities\n· Created workflow rules and related them to tasks, email alerts and field updates\n· Implemented pick lists, dependent pick lists, lookups, master detail relationships, validation and formula fields to the custom objects\n· Used Data Loader for insert, update, and bulk import or export of data from Salesforce.com objects. Used it to extract, and load data from comma separated values (CSV) files.\n· Involved in the Deploy the components by using package.xml through Eclipse, Perforce and Ant tools.\nProject 4\n\nProject Name       \t\t: \tHMS (Hospital Management System)\nClient                     \t\t: \tDominion Medical Centers, Canada \nRole                         \t\t: \tSales force Developer                                                                                        Environment         \t\t:\tForce.com, Changesets, and Internet Cloud                                                                              Environment, Data Loader, Eclipse\n\nProject Description:\n\nThis project will automate the daily operations of Dominion Medical Centers. The project keeps track of the staff and patient (in-patient, out-patient) details. It also takes care of the ward, medical, invoice and the doctor's appointment details. It provides easy access to critical information thus enabling the management to take better decisions on time. This project deals with processing of each and every department in the hospital. The details of Doctor and staff help the hospital to maintain the record of every person. The billing system provides an efficient way for calculating bill details of the patients. \nResponsibilities:\n· Source analyzing and validation.\n· Interaction with primary requester (Client) for Requirements gathering, analysis, design, creating packages, test and Implementation.\n· Worked with the user group for requirement gathering throughout the planning and implementation.\n· Worked with various salesforce.com Standard objects like Accounts, Contacts, Leads, Cases, Campaigns, Reports, and Dashboards.\n· Defined Org wide default to restrict access from users. \n· Created Workflow Rules to automate Tasks, Email Alerts, Field Updates, time-dependent actions.\n· Designed & Developed Apex Classes, Controller Classes, extensions and Apex Triggers for various functional needs in the application.\n· Used SOQL & SOSL with consideration to Governor Limits for data manipulation needs of the application using platform database objects.\n· Strictly maintained Model-View-Controller (MVC) architecture throughout the application.\n· Responsible for all the activities related to configuring Data Loader, uploading data in CSV files into salesforce.com, checking for the correctness of the data.\n· Packaged and Deployed customizations from Sandbox to other environments using Force.com IDES.  \nProject 5\n\nProject Name       \t\t: \tCRM (Customer Relationship Management)\nClient                      \t\t: \tRoss Medical Group Denver, USA\nRole                         \t\t: \tSales force Programmer                                                              Environment         \t\t:\tForce.com, Change sets, and Internet Cloud Environment, Data Loader\n\nProject Description:                                                  \nCRM (Customer Relationship Management) is to assist or guide a sales person to contact customers on a scheduled date & record the output of the particular follow up which is assigned to particular sales person, it also deals with order got by a sales person, orders lost to competitors. It also handles data about customers and competitors. It helps us to calculate the market share of the products in the particular region; it helps us to know the performance of sales person. It is of utmost importance because it also conveys to the user that the order lost at the order stage, quotation stage or at enquiry stage. It also maintains the records of Enquiry, Quotations, Order, Invoice and Payment Tracking. \nRoles and Responsibilities: \n· Source analyzing and validation.\n· Preparing Objects, Profiles and Fields for the structure needs.\n· Analyzed and Implemented the Security model (Object level, Field level and Record level) using Profiles, Roles and Sharing Model (Organizational-wide defaults & Sharing rules) settings.\n· Implemented Case Management like Email-To-Case, Web-To-Case etc.\n· Designed and developed Workflow rules for generating mails and Tasks & Approval Process for approvals.\n· Developed Apex Classes, Triggers, Components and Visual force pages for applying the business logic on Database events.\n· Strictly maintained Model-View-Controller (MVC) architecture throughout the application.\n· Developed Deployment Strategy and deployed the application to SIT, Training, UAT and Production environments smoothly using Force.com IDE, Change Sets.\n· Interacting with Primary Requester (client) for requirements gathering, analysis, design, development, test and Implementation.\n· Tested Documents, which include doing a Quality check on the created Objects. In the process the Objects are tested for the compliance with CLIENT policies and Procedures & CLIENT standards.","annotation":[{"label":["Tools"],"points":[{"start":10339,"end":10348,"text":"Change Set"}]},{"label":["Tools"],"points":[{"start":10324,"end":10336,"text":"Force.com IDE"}]},{"label":["Skills"],"points":[{"start":9997,"end":10000,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":8645,"end":8653,"text":"Force.com"}]},{"label":["Tools"],"points":[{"start":8347,"end":8359,"text":"Force.com IDE"}]},{"label":["Skills"],"points":[{"start":7812,"end":7815,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":7763,"end":7766,"text":"Apex"}]},{"label":["Tools"],"points":[{"start":6514,"end":6520,"text":"Eclipse"}]},{"label":["Skills"],"points":[{"start":6369,"end":6377,"text":"Force.com"}]},{"label":["Tools"],"points":[{"start":6044,"end":6051,"text":"Perforce"}]},{"label":["Tools"],"points":[{"start":6035,"end":6041,"text":"Eclipse"}]},{"label":["Skills"],"points":[{"start":4761,"end":4769,"text":"Force.com"}]},{"label":["Skills"],"points":[{"start":4704,"end":4707,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":4510,"end":4513,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":4491,"end":4499,"text":"Force.com"}]},{"label":["Skills"],"points":[{"start":3937,"end":3945,"text":"Force.com"}]},{"label":["Skills"],"points":[{"start":3890,"end":3893,"text":"HTML"}]},{"label":["Skills"],"points":[{"start":3596,"end":3599,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":2866,"end":2874,"text":"Force.com"}]},{"label":["Skills"],"points":[{"start":2830,"end":2832,"text":"CSS"}]},{"label":["Skills"],"points":[{"start":2820,"end":2823,"text":"HTML"}]},{"label":["Degree"],"points":[{"start":2549,"end":2564,"text":"M. Tech. (E.C.E)"}]},{"label":["Certifications"],"points":[{"start":2472,"end":2516,"text":"Salesforce.com Certified Platform Developer I"}]},{"label":["Tools"],"points":[{"start":2404,"end":2411,"text":"Perforce"}]},{"label":["Tools"],"points":[{"start":2399,"end":2401,"text":"ANT"}]},{"label":["Tools"],"points":[{"start":2390,"end":2396,"text":"Eclipse"}]},{"label":["Tools"],"points":[{"start":2375,"end":2387,"text":"Force.com IDE"}]},{"label":["Tools"],"points":[{"start":2363,"end":2372,"text":"Change Set"}]},{"label":["Tools"],"points":[{"start":2321,"end":2329,"text":"Workbench"}]},{"label":["Tools"],"points":[{"start":2282,"end":2318,"text":"Apex Data Loader (GUI & Command Line)"}]},{"label":["Tools"],"points":[{"start":2239,"end":2245,"text":"Eclipse"}]},{"label":["Tools"],"points":[{"start":2215,"end":2236,"text":"Salesforce.com console"}]},{"label":["Tools"],"points":[{"start":2204,"end":2212,"text":"Force IDE"}]},{"label":["Skills"],"points":[{"start":2158,"end":2166,"text":"Force.com"}]},{"label":["Skills"],"points":[{"start":2112,"end":2114,"text":"CSS"}]},{"label":["Skills"],"points":[{"start":2099,"end":2109,"text":"Java Script"}]},{"label":["Skills"],"points":[{"start":2093,"end":2096,"text":"HTML"}]},{"label":["Skills"],"points":[{"start":2060,"end":2071,"text":"Visual Force"}]},{"label":["Skills"],"points":[{"start":2054,"end":2057,"text":"Apex"}]},{"label":["Tools"],"points":[{"start":1548,"end":1560,"text":"Force.com IDE"}]},{"label":["Skills"],"points":[{"start":1177,"end":1180,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":1123,"end":1134,"text":"Visual Force"}]},{"label":["Skills"],"points":[{"start":1096,"end":1099,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":1048,"end":1056,"text":"Force.com"}]},{"label":["Tools"],"points":[{"start":1031,"end":1037,"text":"Eclipse"}]},{"label":["Skills"],"points":[{"start":473,"end":481,"text":"Force.com"}]},{"label":["Skills"],"points":[{"start":444,"end":447,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":426,"end":429,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":415,"end":423,"text":"Force.com"}]},{"label":["Skills"],"points":[{"start":149,"end":157,"text":"Force.com"}]},{"label":["Years_of_Experience"],"points":[{"start":104,"end":110,"text":"6 years"}]},{"label":["Mobile_No"],"points":[{"start":58,"end":71,"text":"+91-8639524955"}]},{"label":["Email_Address"],"points":[{"start":19,"end":41,"text":"vspraveen1132@gmail.com"}]},{"label":["Name"],"points":[{"start":0,"end":10,"text":"V S PRAVEEN"}]}],"extras":null,"metadata":{"first_done_at":1564122784000,"last_updated_at":1564122784000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "Name Yogesh Sowndararaj\t\n\t Front End Data Engineer\nPh : 9790698426\t\n\t\t\n\t\t\n\tEmail:\t yogutechwizard@gmail.com\n\nBackground\nSeeking a challenging position in the area of Software Developing in a Professional Organization, where my skills can add value to the growth of the organization and allow development of my skills to organization potential in the field of Developing.\n•\tB.Tech Information Technology (2012) studied in Institute of Road and Transport Technology - secured  6.68 CGPA in academic performance\n•\tHaving 6+ years of total experience and 2+ years of experience in Hadoop, PySpark, Hive and Sqoop\n•\tHands on experience in performing requirement gathering, analysis and designing including developing technical specifications\n•\tDevelop Spark jobs in Python to transform and aggregate the data in HDFS on the fly and persist to data source like HDFS  \n•\tInvolved in loading data from UNIX file system to HDFS\n•\tWorked with Hadoop clusters using Hortonworks distributions to store data on HDFS\n\nProfessional experience\nROYAL SPADE TECHNOLOGIES - 10/16 to Present\nBIG DATA EXPERIENCE:\nProject\t              : Income Tax Forms Data Migration\nRole\t\t: Developer\nTeam Size\t: 5\nEnvironment\t: Spark, Hive, Sqoop\n•\tPerformed transformations on the data using Hive\n•\tAnalyzed the data by performing Hive Queries(Hive QL) to study customer behavior\n•\tCreating and managing tables in Hive with large amount of data which includes\nBucketing and partitioning\n•\tDevelop Spark jobs in Python to transform and aggregate the data in HDFS on the fly and persist to data source like HDFS \nProject\t              : Health Care Data Migration\nRole\t\t: Developer, Support\nTeam Size\t: 3\nEnvironment\t: Hive, Sqoop, HDFS\nRoles and Responsibilities \n•\tAnalyzing Json Data with Hive\n•\tWorked on Partitioning, Bucketing, Join optimizations and Query optimizations in Hive\n•\tWorked on Data Processing with PySpark(Python)\n•\tHandled importing other enterprise data from different sources into HDFS using Sqoop\n•\tExported the analyzed data to relational databases using Sqoop\n•\tWorked with Hadoop clusters using Hortonworks distributions to store data on HDFS\n\n\n\nSkills\n•\tProgramming Language :  C#, Python\n•\tBig Data Ecosystems : Hadoop, PySpark, Hive, Sqoop\n•\tDatabase : MS  SQL Server\n•\tTechnologies : WIN Form\n•\tSource Control : TFS\n•\tTools : Visual Studio , iTextSharp\n\n\n\nVIT Consultancy private Limited [Aara Tech]- 01/16 to 09/16\nProject  \t: Ezy-Trak Application\nRole\t\t: Developer\nTeam Size\t: 25\nEnvironment\t: C#.Net , SQL Server 2008 , iTextSharp\nRoles and Responsibilities  \n•\tAssisted in front-end development and documentation, UI development and assist other developers in methodology documentation when needed\n•\tTrouble shoot and fixed problems through customer request\t\n•\tManaged work items in Microsoft Team Foundation Server\nROYAL SPADE TECHNOLOGIES - 12/12 to 12/15\nProject\t            : Samson Gym\nRole\t\t: Developer\nTeam Size\t: 1\nEnvironment\t: C#.Net , SQL Server 2008, Crystal Report\nRoles and Responsibilities \n•\tPerformed analysis, design and development of the application according to client requirements\n•\tInvolved in Data Access Layer with ADO.Net\n•\tInvolved in database design on SQL Server 2008 and development of Stored Procedures\nProject\t              : Matrimony\nRole\t\t: Developer\nTeam Size\t: 1\nEnvironment\t: C#.Net , SQL Server 2008 , Crystal Report\nProject  \t: POS for Textile Shop\nRole\t\t: Developer\nTeam Size\t: 1\nEnvironment\t: C#.Net, SQL Server 2008, iTextSharp\n\n\n\n‹#›","annotation":null,"extras":null,"metadata":{"first_done_at":1564243519000,"last_updated_at":1564293404000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "Backend Primary Skills\n\tExperience(In Years)\n\tScreening Criteria for Skill Sets(Minimal Years required)\n\tCandidates Rating(For 10)\n\tCandidate Description(Optional Notes)\n\n\tJAVA\n\t3.0 Years \n\t3.0 Years\n\t9\n\t \n\n\tMICROSERVICES\n\t1.0 Year\n\t1.0 Year\n\t8\n\t \n\n\tSPRING BOOT\n\t1.0 Year\n\t1.0 Year\n\t8\n\t \n\n\tAngular\n\t1.0 Year\n\t1.0 Year\n\t8\n\t \n\n\n\nAbhishek Kumar  \nMobile:   +91 9980977738\nE-mail:    abhishekyouy@gmail.com\n\n\nObjectives\n\nAccomplished IT Engineer seeking a challenging career opportunity where IT initiatives through strategic planning, proactive project management, decisive leadership, and dynamic client relations can be effectively applied to achieve company objectives.\n\n\nProfessional Summary\n\n· Having 3 years of experience in development of Server/Client side Programming and Front-End development by using Java API and related technologies.\n· Hands on experience in application development using Hibernate, Spring, Spring Boot, JavaScript, Angular, JQuery, frameworks.\n· Hands on experience in application development using Microservices and Spring REST services.\n· Expertise in using Hibernate concepts for Database interactions in persistence layers.\n· Worked on Spring AOP based transaction support at business layer.\n· Extensive Experience in using MVC (Model View Controller) architecture for developing applications using JSP, JSTL, Java Beans and Servlets.\n· Experience in web application development using open source MVC implementations like Spring Frameworks.\n· Expertise in Object-Oriented Programming/ Development and Design skill.\n· Knowledge of Java and J2EE Design Patterns.\n· Strong experience of developing data models using Hibernate POJO's and configuring Hibernate persistence layer.\n· Hands on experience in using Application/Web servers like Web Sphere and Tomcat servers.\n· Worked on databases like Oracle & MYSQL.\n· Good understanding of Software Development Life Cycle (SDLC) process in the areas of Analysis, Design, Implementation and Testing of Software Applications using Java/J2EE technologies.\n· Worked in Agile Development for smoothness of Project\n\nSkill Set\n\n\tJava Technologies\n\tJavaSE, JavaEE.\n\n\tWeb Technologies\n\tHTML, XML, Servlets, JSP and JSTL.\n\n\tDevelopment Model\n\tAgile\n\n\tFrameworks\n\tHibernate, Spring, Spring Boot, Microservices, AngularJS, javaScript, jQuery.\n\n\tDatabase\n\tOracle.\n\n\tApplication Servers\t\n\tWebSphere.\n\n\tWeb Servers\t\n\tTomcat.\n\n\tTools\n\tLog4j, Maven, SVN, Bitbucket .\n\n\tUnit Testing\n\tJunit.\n\n\tIDE\n\tEclipse, STS, SQL Developer,\n\n\n\n\nWork Experience\n\t\n· Working as a Senior Software Engineer in HCL from May 2016 to till date.\n\nEducational Qualification\n\n· Master in Computer Applications (M.C.A) from GBTU Lucknow.\n              \n\n\n\nProject Summary:\n\nProject 2           : Employee Due Diligence (EmDD)\t\t\t                 \nClient                : Citi Banks.\nDuration           : March-2018 – Till Present.\nSkills used        : Spring Boot, Hibernate 4.0, JAVA 8, Angular 2. REST and Microservices.\nServers\t  : Tomcat 7 Web sphere, Oracle 11g.\nTools\t\t  :  Maven, Log4j, GIT, Junit.\nRole                  :  Senior Developer\n\n\nDescription: \t\n\nEmployee Due Diligence (EmDD) is a global Compliance initiative to unify the functionalities of regional applications within a common global platform, in support of Employee Due Diligence activities. This endeavor will consolidate and standardize (where applicable) the Employee Due Diligence functions.  These functions include (but are not limited to):  disclosure of Covered & Passive Accounts, Outside Directorships, Outside Business Affiliations, Outside Private Investments; Employee On-boarding module; interim requests/disclosures; disclosures tracking and storage; employee statements; enforcement of personal trading policies; facilitation of trade pre-clearance; annual certification activities; disclosure and tracking of political activities and contributions.\n·    Employee On-boarding\nDisclosure: This page provides employee an access to all his/her disclosures submitted till date and allows creating new disclosures.\n· Covered & Passive Accounts\n· Outside Directorships\n· Outside Business Affiliations\n· Private Investments\n· Independent Related Person’s Accounts (applicable to APAC region only)\n· Initial Holding Reports (applicable to Access Persons and/or Program Level Access Persons only)\n· RIA Survey (applicable to Access Persons and/or Program Level Access Persons only)\nTrade Preclearance\nStatement Review\nAnnual Certification\nPAC (Political Activity Contribution)\n\n\n\n\n\n\n\nRoles & Responsibilities:\n\n· Coded the modules allocated to me following coding standards and assuring quality delivery and timeliness. \n· Involved in discussion with other team members and mentor to discuss the design of the modules, pages and database structure. \n· Testing the whole application and logging the defects. Resolved the issues aroused in testing. \n· Application developed on Eclipse IDE. Testing and deployment of application code is achieved by Web Sphere Application Server\n· Provide client side and server side validation as per client specification.\n\n\nProject 1           : IAMS (Insurance Annuity Management System)\t\t\t                 \nClient                : Citi Banks.\nDuration           : May-2016 – March-2018\nSkills used        : Spring 4.0, Hibernate 4.0, java 7, JSP. REST Java Script, JQuery & Ajax.\nServers\t  : Tomcat 7, Oracle 11g.\nTools\t\t  :  Maven, Log4j, GIT, Junit.\nRole                  :  Senior Developer\n\nDescription:\n\nThe Insurance and Licensing application is registry of Citigroup’s insurance licensed individuals (Financial Advisor, Consultants, private bankers, etc.) who are authorized to sell insurance and annuities. The application caters to the following functionalities on broad level: Maintain Corporate Licenses, Maintain Branch Information, Maintain License for Individuals,  Establishes and maintains individual appointments with insurance carriers  in order to sell their products, Maintain corporate affiliation for individuals, Maintain carrier profile, Maintain State Rules. This application monitors the credentials of financial advisors licensed to sell insurance in annuities. In order to perform this function, the insurance department creates and manages the portfolio of financial advisors.\n\nRoles & Responsibilities:\n\n· Coded the modules allocated to me following coding standards and assuring quality delivery and   timeliness. \n· Involved in discussion with other team members and mentor to discuss the design of the modules, pages and database structure. \n· Testing the whole application and logging the defects. Resolved the issues aroused in testing. \n·  Coordinated with clients regarding requirements\n\n\nDECLARATION:\n\nI hereby declare that all the statements made above are correct to the best of my knowledge \nand belief.\n\nPlace: Bangalore                                                                                                                             Abhishek Kumar","annotation":[{"label":["Name"],"points":[{"start":6933,"end":6946,"text":"Abhishek Kumar"}]},{"label":["Tools"],"points":[{"start":5391,"end":5395,"text":"Junit"}]},{"label":["Tools"],"points":[{"start":5379,"end":5383,"text":"Log4j"}]},{"label":["Tools"],"points":[{"start":5372,"end":5376,"text":"Maven"}]},{"label":["Skills"],"points":[{"start":5348,"end":5353,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":5338,"end":5343,"text":"Tomcat"}]},{"label":["Skills"],"points":[{"start":5288,"end":5290,"text":"JSP"}]},{"label":["Skills"],"points":[{"start":5265,"end":5273,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":5253,"end":5258,"text":"Spring"}]},{"label":["Tools"],"points":[{"start":3030,"end":3034,"text":"Junit"}]},{"label":["Tools"],"points":[{"start":3018,"end":3022,"text":"Log4j"}]},{"label":["Tools"],"points":[{"start":3011,"end":3015,"text":"Maven"}]},{"label":["Skills"],"points":[{"start":2987,"end":2992,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":2966,"end":2971,"text":"Tomcat"}]},{"label":["Skills"],"points":[{"start":2939,"end":2951,"text":"Microservices"}]},{"label":["Skills"],"points":[{"start":2896,"end":2904,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":2883,"end":2893,"text":"Spring Boot"}]},{"label":["Degree"],"points":[{"start":2611,"end":2649,"text":"Master in Computer Applications (M.C.A)"}]},{"label":["Tools"],"points":[{"start":2441,"end":2445,"text":"Junit"}]},{"label":["Tools"],"points":[{"start":2413,"end":2421,"text":"Bitbucket"}]},{"label":["Tools"],"points":[{"start":2408,"end":2410,"text":"SVN"}]},{"label":["Tools"],"points":[{"start":2401,"end":2405,"text":"Maven"}]},{"label":["Tools"],"points":[{"start":2394,"end":2398,"text":"Log4j"}]},{"label":["Skills"],"points":[{"start":2377,"end":2382,"text":"Tomcat"}]},{"label":["Skills"],"points":[{"start":2350,"end":2358,"text":"WebSphere"}]},{"label":["Skills"],"points":[{"start":2318,"end":2323,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":2298,"end":2303,"text":"jQuery"}]},{"label":["Skills"],"points":[{"start":2286,"end":2295,"text":"javaScript"}]},{"label":["Skills"],"points":[{"start":2275,"end":2283,"text":"AngularJS"}]},{"label":["Skills"],"points":[{"start":2260,"end":2272,"text":"Microservices"}]},{"label":["Skills"],"points":[{"start":2247,"end":2257,"text":"Spring Boot"}]},{"label":["Skills"],"points":[{"start":2239,"end":2244,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":2228,"end":2236,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":2181,"end":2184,"text":"JSTL"}]},{"label":["Skills"],"points":[{"start":2173,"end":2175,"text":"JSP"}]},{"label":["Skills"],"points":[{"start":2163,"end":2170,"text":"Servlets"}]},{"label":["Skills"],"points":[{"start":2158,"end":2160,"text":"XML"}]},{"label":["Skills"],"points":[{"start":2152,"end":2155,"text":"HTML"}]},{"label":["Skills"],"points":[{"start":2124,"end":2129,"text":"JavaEE"}]},{"label":["Skills"],"points":[{"start":2116,"end":2121,"text":"JavaSE"}]},{"label":["Skills"],"points":[{"start":1825,"end":1830,"text":"Oracle"}]},{"label":["Skills"],"points":[{"start":1782,"end":1787,"text":"Tomcat"}]},{"label":["Skills"],"points":[{"start":1678,"end":1686,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":1645,"end":1653,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":1454,"end":1459,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":1357,"end":1364,"text":"Servlets"}]},{"label":["Skills"],"points":[{"start":1336,"end":1339,"text":"JSTL"}]},{"label":["Skills"],"points":[{"start":1331,"end":1333,"text":"JSP"}]},{"label":["Skills"],"points":[{"start":1168,"end":1173,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":1088,"end":1096,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":1045,"end":1050,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":1027,"end":1039,"text":"Microservices"}]},{"label":["Skills"],"points":[{"start":918,"end":928,"text":"Spring Boot"}]},{"label":["Skills"],"points":[{"start":910,"end":915,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":899,"end":907,"text":"Hibernate"}]},{"label":["Years_of_Experience"],"points":[{"start":703,"end":709,"text":"3 years"}]},{"label":["Email_Address"],"points":[{"start":380,"end":401,"text":"abhishekyouy@gmail.com"}]},{"label":["Mobile_No"],"points":[{"start":353,"end":367,"text":" +91 9980977738"}]},{"label":["Name"],"points":[{"start":327,"end":340,"text":"Abhishek Kumar"}]}],"extras":null,"metadata":{"first_done_at":1564047901000,"last_updated_at":1564047901000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "ARNAB CHAKRABORTY                        DATA ANALYST(3 years Exp.)\n Contact Number : 8861683595                                          Key Skills: Experience in SQL, Tableau, \n E-mail : arnab2592@gmail.com                                            R,Excel & Machine learning\n DOB : 25/10/1992\n \nCareer Objective\n To work in a challenging environment which can extract the best out of me, is conducive to learning and growth at a professional level, directing all my future endeavors to be an asset to the organization. \n\nSoftware Skills\n\n· Working on Text Analytics using R programming language.\n· Experienced in prediction model, descriptive analysis, exploratory data analysis, cluster analysis.\n\n· Experienced in writing SQL query.\n· Experienced in creating complex data visualization in Tableau on different data sources (RDBMS and flat file).\n· Experienced in EXCEL.\n\n· Knowledge in Statistics. \n· Knowledge on Hadoop cluster.\n\n· Knowledge in HDFS, PIG, HIVE.\n\nWork Experience\n\tOrganization\n\tMyntra-Jabong India Private Limited\n\n\tDesignation\n\tAssociate Data Analyst\n\n\tDuration\n\tFrom 10th Dec 2018 to Till Date\n\n\tAbout Area of works / Projects worked in \n\tRole – Data Analyst\nDescription -  \n\n· Working on supply chain management system. \n· Man Power Movement planning in different departments.\n· Finding Root cause analysis for each non complaint steps.\n· Data extraction and manipulation of huge volume of data and simplifying it R for better understanding.\n· Maintaining and tracking product flow from inventory to logistics and inward to inventory.\n·  Connecting multiple mysql servers, consolidating data and automating manual report using R language.\n\n· Implementing tableau to visualize and understand data more clearly.\n· Working with different stake holder, identify and analyse their report.\n\n\n\tOrganization\n\tCERNER HEALTHCARE SOLUTIONS INDIA PRIVATE LIMITED\n\n\tDesignation\n\tSystem Engineer\n\n\tDuration\n\tFrom 28th AUGUST  2017 to 7th Dec 2018\n\n\tAbout Area of works / Projects worked in \n\tRole – Data Analyst\n Description -  \n\n· Working on different machine learning project (text analytics/text classification)\n· Text classification model (semi supervised) to predict change request for becoming a obligated Change request or not depending on two text based columns.\n\n· Cluster analysis(unsupervised) on incident, change request data to identify the most common issue type in the domain.\n\n· Utilizing regression model (supervised) on various dataset to identify the missing value.\n· Creating data visualization on live data through R programming from different RDBMS sources.\n· Writing complex SQL query to fetch the data through Tableau.\n· Visualizing data using Tableau for better understanding and to make it interactive. Create dashboards using Tableau Dashboard and representing the proper cleaned data from the database.\n                       -      Develop Scorecard from different perspective like performance                      \nanalysis of associate (Monthly, quarterly), analyse on types of issues with respect to different domain in Organization\n· Derive the performance scales of employees on issues and help in finding most often issues.\n\n· Prepare and present the reports with charts to highlight the important analysis point.\n· Connect to JIRA database from Tableau and fetch the data to create time duration chart for each JIRAs, assigned to respective associate.\n· Creating interesting informative chart on Tableau like, Bump chart, Donut chart, Pareto chart ,water fall chart and so on.\n\n· Automating of repetitive tasks.\n\n\n\tOrganization\n\tIBM INDIA PRIVATE LIMITED\n\n\tDesignation\n\tTechnical Specialist\n\n\tDuration\n\tFrom 18th Feb 2016 to 21st July 2017\n\n\tAbout Area of works / Projects worked in \n\t Role – Data Analyst\nDescription -  \n\n· Utilize and analyse on Extracted data from different data source and different team.\n· Extracted, compiled and tracked data, and analysed data to generate reports.\n\n· Using histograms, running records, process behaviour charts and different chart to analyse business \n\n· Data Coordination with client for updates and troubleshooting.\n\n· Coordination with team for better efficiency.\n\n· Analysing System log through pig script.\n\n· Analyse the efficiency of process and depending on that automated the manual process.\n\n· Identify and analysis the reports and help manager to identify to most often issues.\n\n\n\nBASIC ACADEMIC CREDENTIALS \n\n\tQualification\n\tInstitution\n\tBoard/University\n\tYear\n\tPercentage\n\n\tB.Tech \n\tNeotia Institute of Technology, Management and Science\n\tWest Bengal University of Technology(WBUT)\n\t2014\n\n\n\t    74.4%\n\n\tXII\n\n\tMidnapore Collegiate School\n\tWest Bengal Council of Higher Secondary Education \n \n\t2010\n\n\n\t81.6%\n\n\tX\n\tMidnapore Collegiate School\n\tWest Bengal Board of Secondary Education\n\t2008\n\n\n\t84.75%  \n\n\n\n\nPERSONAL DETAILS\n· Father’s Name\n\n\n:\nLakshmi Kanta Chakraborty\n· Present Address\n\n            :\n5th Cross, 4th Block ,HBR Layout,Bangalore-560043\n· Language Known\n\n:\nEnglish, Bengali, Hindi\n· Interest & Hobbies\n\n:\nOutdoor  & indoor  games(football,table tennis), \n                                                                   \nGuitar playing & passionate about my singing.\n\nI do hereby declare that the above information is true to the best of my knowledge.\nDate: ____________________________\n\n \n_________________________________\n\n\n\n\n\n\n\n                   Arnab Chakraborty\nPlace: ___________________________","annotation":[{"label":["Degree"],"points":[{"start":4475,"end":4480,"text":"B.Tech"}]},{"label":["Skills"],"points":[{"start":3444,"end":3450,"text":"Tableau"}]},{"label":["Skills"],"points":[{"start":3293,"end":3299,"text":"Tableau"}]},{"label":["Skills"],"points":[{"start":2765,"end":2771,"text":"Tableau"}]},{"label":["Skills"],"points":[{"start":2680,"end":2686,"text":"Tableau"}]},{"label":["Skills"],"points":[{"start":2646,"end":2652,"text":"Tableau"}]},{"label":["Skills"],"points":[{"start":2610,"end":2612,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":1653,"end":1653,"text":"R"}]},{"label":["Skills"],"points":[{"start":1440,"end":1440,"text":"R"}]},{"label":["Skills"],"points":[{"start":963,"end":966,"text":"HIVE"}]},{"label":["Skills"],"points":[{"start":958,"end":960,"text":"PIG"}]},{"label":["Skills"],"points":[{"start":952,"end":955,"text":"HDFS"}]},{"label":["Skills"],"points":[{"start":920,"end":933,"text":"Hadoop cluster"}]},{"label":["Skills"],"points":[{"start":892,"end":901,"text":"Statistics"}]},{"label":["Skills"],"points":[{"start":869,"end":873,"text":"EXCEL"}]},{"label":["Skills"],"points":[{"start":795,"end":801,"text":"Tableau"}]},{"label":["Skills"],"points":[{"start":728,"end":730,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":684,"end":699,"text":"cluster analysis"}]},{"label":["Skills"],"points":[{"start":657,"end":681,"text":"exploratory data analysis"}]},{"label":["Skills"],"points":[{"start":635,"end":654,"text":"descriptive analysis"}]},{"label":["Skills"],"points":[{"start":617,"end":632,"text":"prediction model"}]},{"label":["Skills"],"points":[{"start":576,"end":576,"text":"R"}]},{"label":["Skills"],"points":[{"start":555,"end":568,"text":"Text Analytics"}]},{"label":["Skills"],"points":[{"start":262,"end":277,"text":"Machine learning"}]},{"label":["Skills"],"points":[{"start":254,"end":258,"text":"Excel"}]},{"label":["Skills"],"points":[{"start":252,"end":252,"text":"R"}]},{"label":["Email_Address"],"points":[{"start":189,"end":207,"text":"arnab2592@gmail.com"}]},{"label":["Skills"],"points":[{"start":169,"end":175,"text":"Tableau"}]},{"label":["Skills"],"points":[{"start":164,"end":166,"text":"SQL"}]},{"label":["Mobile_No"],"points":[{"start":86,"end":95,"text":"8861683595"}]},{"label":["Years_of_Experience"],"points":[{"start":54,"end":60,"text":"3 years"}]},{"label":["Name"],"points":[{"start":0,"end":16,"text":"ARNAB CHAKRABORTY"}]}],"extras":null,"metadata":{"first_done_at":1564050957000,"last_updated_at":1564050957000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "ASHISH PAL \nContact: +91-9251644048                                                                     \nEmail id: ashishpal2702@gmail.com                                                     \n\nCareer Objective: \nTo work hard with full determination and dedication in order to obtain a position that will enable\nme to use my skills, and ability to work well with people for achieving professional and personal\nobjective.\n\nProfessional Abridgement:\n Working in Siemens Healthineers Pvt. Ltd as Quality and Data Analyst.\n A Result oriented professional with experience in Python, R, Statistical & Predictive\n\nModelling, Machine Learning, NLP, Deep Neural Networks and Oracle RPAS (Retail\nPredictive Application Server) & BIG Data Analytics\n\n Worked on Automation using VBA, Python and RPA (Robotic Process Automation).\n Elementary exposure to IBM Watson, Microsoft AZURE, SPLUNK and Google cloud.\n A self-motivated team player with strong communication skills and an analytical mind with\n\nability to think clearly, logically and innovatively. \nEducational Qualification: \n\nExamination Discipline Institution Board/Unive\nrsity \n\nYear of \nPassing \n\nPercentage\n(%) \n\nPGPBDA (Post\nGraduate Program in\nBig Data Analytics)\n\nBIG Data\nAnalytics\nand ML  \n\nGreat Lakes Institute  \nof Management\n\nIllinois\nInstitute of\nTechnology\n\n2018 80%\n\nB.Tech ECE JECRC-UDML \nCollege of \n\nEngineering \n\nRTU, Kota 2015 76% \n\nXII All subjects St. Xavier Sen. Sec\nSchool, Jaipur\n\nCBSE 2011 86.2% \n\nX All subjects St. Xavier Sen. Sec\nSchool, Jaipur\n\nCBSE 2009 87.6% \n\n  Technical Skill sets: \n Python , R\n Excel VBA\n Machine learning(Supervised & Unsupervised) \n RPA\n SQL , Pl/SQL \n Unix\n Basic JAVA\n Big Data (Hadoop and Spark)\n Embedded & Robotics\n\n\n\nWork Experience Summary: \n Siemens Healthineers \n\n Working as Quality engineer in enhancing Software Quality and Automating Dashboards creations.\n Created ML models for Defect Density Prediction and Defect Prone Units prediction which improved\n\nturnaround time in Software Development.\n Created Model which used Defect Comment using NLP and ML techniques in automatic categorization\n\nand RCA analysis.\n Using Advance Analytics and Machine Learning concepts for Resource and time Management and\n\ndefects prediction. \n Worked on POC’s related to Image classification and Object detection using Deep learning.\n\nAccenture (March 2016 – May 2018) \n Created Solutions for Retail clients related to Planning and Supply Chain Management. \n Worked on Development of MFP (Merchandise Financial Planning) and Assortment\n\nPlanning Solution for a Client using Oracle RPAS. \n Worked under Testing Team for Functional, Integration and Analytics Verification/Testing\n\nof the Solution using Excel.\n\n Worked in development of Optimizing Replenishment and Promotional Solution using\nPython and Machine Learning Algorithms. \n\n Worked on Accenture RPA tool for Automating various use-cases related to automating\nIncident closure, CR closure, Creating Dashboards etc. which reduced 90% of manual\nefforts.\n\n Automated Outlook, Excel, Ppt etc. which saved lots of manual efforts of Senior\nManagement. \n\n Worked on various libraries like numpy, pandas and sklearn and different Machine Learning\nAlgorithms like Decision Tree, KNN, and Naïve Bayes etc. used for Data Science using\nPython.\n\n Created Reports and BATCH ANALYTICS Dashboard using SPLUNK.\n\nCapstone Project –\n\n News Summarization, Classification and Recommendation Engine created using advance\nNLP techniques and machine learning models. \n\n\n\n    Achievements & Certifications: \n Participated in AI Hackathon and build Online Predictive Solution for News Popularity\n\nusing Python and Decision Tree and Ensemble methods. \n ATA 2.0 (Accenture Technology Academy) Python Developer certified by MIT. \n Stood 48th in MMT Data Science Hackathon.\n NPTEL Quality Management Certification – \nhttp://nptel.ac.in/noc/social_cert/noc18-mg39/NPTEL18MG39S211204061810008126.jpg\n\n PGP_BDML Certification – \nhttps://olympus1.greatlearning.in/certificate/ECTSFNLD\n\nPersonal Dossier:  \nDate of Birth              : 27th February, 1993 \nGender                       :  Male \nNationality                 :  Indian \nLanguage Proficiency:  English, Hindi \nAddress            :  Plot no 2, Vishnu colony, Sodala, Jaipur-302006\nGitHub link                : github.com/ashishpal2702\nLinkedIn                     : linkedin.com/in/ashish-pal-66314b75/\n\nAdditional Information: \n Participated and Won various National Level ROBOTICS competitions.\n Have done summer internships in Various Organizations like MTS, BSNL and JAIPUR\n\nMETRO.\n Core team member of college TechFest SARVATRA and coordinated various events and\n\nworkshops.\n Enthusiast and like to explore and work on new technologies.\n\n                                                                                                                                ASHISH PAL\n\nhttps://olympus1.greatlearning.in/certificate/ECTSFNLD\n\n\nTextControl1","annotation":[{"label":["Name"],"points":[{"start":4889,"end":4898,"text":"ASHISH PAL"}]},{"label":["Certifications"],"points":[{"start":3957,"end":3964,"text":"PGP_BDML"}]},{"label":["Certifications"],"points":[{"start":3919,"end":3923,"text":"NPTEL"}]},{"label":["Certifications"],"points":[{"start":3831,"end":3835,"text":"NPTEL"}]},{"label":["Certifications"],"points":[{"start":3801,"end":3816,"text":"MMT Data Science"}]},{"label":["Certifications"],"points":[{"start":3749,"end":3764,"text":"Python Developer"}]},{"label":["Skills"],"points":[{"start":3660,"end":3665,"text":"Python"}]},{"label":["Skills"],"points":[{"start":3304,"end":3309,"text":"Python"}]},{"label":["Skills"],"points":[{"start":2874,"end":2876,"text":"RPA"}]},{"label":["Skills"],"points":[{"start":2810,"end":2815,"text":"Python"}]},{"label":["Skills"],"points":[{"start":2598,"end":2600,"text":"RPA"}]},{"label":["Skills"],"points":[{"start":1714,"end":1732,"text":"Embedded & Robotics"}]},{"label":["Skills"],"points":[{"start":1705,"end":1709,"text":"Spark"}]},{"label":["Skills"],"points":[{"start":1694,"end":1699,"text":"Hadoop"}]},{"label":["Skills"],"points":[{"start":1684,"end":1691,"text":"Big Data"}]},{"label":["Skills"],"points":[{"start":1671,"end":1680,"text":"Basic JAVA"}]},{"label":["Skills"],"points":[{"start":1664,"end":1667,"text":"Unix"}]},{"label":["Skills"],"points":[{"start":1654,"end":1659,"text":"Pl/SQL"}]},{"label":["Skills"],"points":[{"start":1648,"end":1650,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":1642,"end":1644,"text":"RPA"}]},{"label":["Skills"],"points":[{"start":1612,"end":1636,"text":"Supervised & Unsupervised"}]},{"label":["Skills"],"points":[{"start":1595,"end":1610,"text":"Machine learning"}]},{"label":["Skills"],"points":[{"start":1583,"end":1591,"text":"Excel VBA"}]},{"label":["Skills"],"points":[{"start":1579,"end":1579,"text":"R"}]},{"label":["Skills"],"points":[{"start":1570,"end":1575,"text":"Python"}]},{"label":["Skills"],"points":[{"start":1199,"end":1206,"text":"Big Data"}]},{"label":["Degree"],"points":[{"start":1166,"end":1171,"text":"PGPBDA"}]},{"label":["Skills"],"points":[{"start":785,"end":787,"text":"RPA"}]},{"label":["Skills"],"points":[{"start":774,"end":779,"text":"Python"}]},{"label":["Skills"],"points":[{"start":674,"end":676,"text":"RPA"}]},{"label":["Skills"],"points":[{"start":579,"end":579,"text":"R"}]},{"label":["Skills"],"points":[{"start":571,"end":576,"text":"Python"}]},{"label":["Email_Address"],"points":[{"start":115,"end":137,"text":"ashishpal2702@gmail.com"}]},{"label":["Mobile_No"],"points":[{"start":21,"end":34,"text":"+91-9251644048"}]},{"label":["Name"],"points":[{"start":0,"end":9,"text":"ASHISH PAL"}]}],"extras":null,"metadata":{"first_done_at":1564231722000,"last_updated_at":1564231722000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "CURRICULUM VITAE                                                                                                                  \n\nAshish\t\t\t\t\t                                                                                                 \nE-Mail: ashishvikash969@gmail.com\nContact No. : 9945105862 \n\t\t\t\t                              \nPROFILE\nResult oriented Software Engineer and Developer with considerable training and enterprise experience in Web Development, E-Commerce Platform,Implementation, Integration(ATG-SIEBEL CRM ) and in all stages of design, coding, testing .     Thorough understanding of Computer Science Fundamentals. I also understand the importance of creating highly readable and easily maintainable source code.\n\nPROFESSIONAL SUMMARY\n\n· Strong background in Java/J2EE environments.Worked extensively on J2EE for developing Web and Distributed Applications by using JDBC,springCore,Java, Web Services , Oracle ecommerce ATG Framework and Microservice.\n· Successfully completed SAFe Certification(4.0).  \n· Extensive knowledge of Core Java concepts.\n· Having very good implementation knowledge of OOP concepts.\n· Proficiency in Java, SpringCore, ATG Framework,DataStruture.\n· Good Knowledge of ATG Web Commerce. Understood core concepts of Components, Repository, Droplets, Pricing Engine, Order Management.\n· Good knowledge of Web Services(SOAP and Rest service).\n· Hands on Experience in Using Eclipse IDE for developing software applications.\n· Experience in various servers namely WebLogic Server and Tomcat, Jboss.\n· Developing applications in MVC architecture using Spring framework.\n· Influential character with good Technical & pioneering skills and can handle Software Projects efficiently.\n· Ability to learn new technologies with minimal time period.\n· Flexible and versatile to adopt any new environment and work on any project.\n· A dedicated team player with excellent organizational and Interpersonal skills.\n\n\nEXPERIENCE DETAILS [TOTAL 3. 2Year]\nAccenture Services Pvt. Ltd., Bangalore.\n— Application Development Analyst\nEDUCATION:\nSSCET BHILAI.\n— B.E., Computer Science & Engineering [8.2] 2011-2015\nSANT. JOSEPH PUB SCHOOL, SAMASTIPUR\n—  CBSE(12th) [69.2%]2010\nJ.N.V EAST CHAMPARAN\n—  CBSE (10th) [85%]2008\n\nTECHNICAL KNOWLEDGE\n\nTechnologies: JAVA 8, J2EE, SQL, C, ATG, OOPs, Spring Boot\nWeb Technologies: Servlets, JSP, XML, Spring Framework, JSON\nFront End Technologies: JavaScript, Ajax,Jquery,HTML5\nApplication Servers and Databases: Weblogic , Apache Tomcat , Oracle 11g.\nWeb Application Framework:  Spring , ATG Framework\nBuild Tools: Maven, Eclipse, SQL Developer\n\nPROJECT #1:\nTITLE : Orange Spain ATG\nMarch 2016 – jan 2018\n\nClient  : Orange Spain (http://www.orange.es)    \n\nRole :\n· Involved in system analysis, design, management, development and        Involved in designing Components with various Java core design patterns.\n· Work with Spring MVC framework and hibernate and provide the critical solution to clients .\n· Fixes lots of critical issue in the production and followed all agile methodology while development. \n· Gathered various requirements from business analysts, product technical specifications.\n\n\nLANGUAGES & TOOLS\t :  Java, Spring Framework, SOAP UI, javaScript,Ajax ,Jquery,Web services, SQL Developer,  Weblogic,Hibernate .\n\nPROJECT #2:    \n\nTITLE : Aristos Premier\nFeb 2018–till now\n\nClient  : AT&T (https://www.att.com/wireless/)    \n\n\nRole:   Highly involved in a Oracle framework   called ATG,\nMicroservice, Spring MVC, JMS, Web Services, Spring Boot, Apache Kafka.\nGathered various requirements from business analysts, product technical specifications.\nDeveloped   various services with help of restful webservice.\n\n\n\nPERSONAL   PROFILE\nDate of Birth             \t:\t15th Oct 1992\nGender        \t\t:           \tMale\nNationality                \t:           \tIndian\nLanguages Known  \t:\tEnglish, Hindi \nPersonal Interest\t\t:\tPlaying Cricket , exploring new places\n\nDECLARATION\nI solemnly pledge that the above-furnished information is true to the best of my knowledge and belief.\nYours Sincerely,\n(Ashish)","annotation":[{"label":["Name"],"points":[{"start":4069,"end":4074,"text":"Ashish"}]},{"label":["Skills"],"points":[{"start":3515,"end":3525,"text":"Spring Boot"}]},{"label":["Skills"],"points":[{"start":3484,"end":3489,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":3465,"end":3467,"text":"ATG"}]},{"label":["Skills"],"points":[{"start":3275,"end":3282,"text":"Weblogic"}]},{"label":["Tools"],"points":[{"start":3259,"end":3271,"text":"SQL Developer"}]},{"label":["Skills"],"points":[{"start":3238,"end":3243,"text":"Jquery"}]},{"label":["Skills"],"points":[{"start":3232,"end":3235,"text":"Ajax"}]},{"label":["Skills"],"points":[{"start":3194,"end":3209,"text":"Spring Framework"}]},{"label":["Skills"],"points":[{"start":2888,"end":2893,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":2644,"end":2646,"text":"ATG"}]},{"label":["Tools"],"points":[{"start":2596,"end":2608,"text":"SQL Developer"}]},{"label":["Tools"],"points":[{"start":2587,"end":2593,"text":"Eclipse"}]},{"label":["Tools"],"points":[{"start":2580,"end":2584,"text":"Maven"}]},{"label":["Skills"],"points":[{"start":2553,"end":2565,"text":"ATG Framework"}]},{"label":["Skills"],"points":[{"start":2544,"end":2549,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":2504,"end":2513,"text":"Oracle 11g"}]},{"label":["Skills"],"points":[{"start":2488,"end":2500,"text":"Apache Tomcat"}]},{"label":["Skills"],"points":[{"start":2477,"end":2484,"text":"Weblogic"}]},{"label":["Skills"],"points":[{"start":2436,"end":2440,"text":"HTML5"}]},{"label":["Skills"],"points":[{"start":2429,"end":2434,"text":"Jquery"}]},{"label":["Skills"],"points":[{"start":2424,"end":2427,"text":"Ajax"}]},{"label":["Skills"],"points":[{"start":2412,"end":2421,"text":"JavaScript"}]},{"label":["Skills"],"points":[{"start":2383,"end":2386,"text":"JSON"}]},{"label":["Skills"],"points":[{"start":2365,"end":2380,"text":"Spring Framework"}]},{"label":["Skills"],"points":[{"start":2360,"end":2362,"text":"XML"}]},{"label":["Skills"],"points":[{"start":2355,"end":2357,"text":"JSP"}]},{"label":["Skills"],"points":[{"start":2345,"end":2352,"text":"Servlets"}]},{"label":["Skills"],"points":[{"start":2315,"end":2325,"text":"Spring Boot"}]},{"label":["Skills"],"points":[{"start":2309,"end":2312,"text":"OOPs"}]},{"label":["Skills"],"points":[{"start":2304,"end":2306,"text":"ATG"}]},{"label":["Skills"],"points":[{"start":2300,"end":2301,"text":" C"}]},{"label":["Skills"],"points":[{"start":2296,"end":2298,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":2290,"end":2293,"text":"J2EE"}]},{"label":["Skills"],"points":[{"start":2282,"end":2285,"text":"JAVA"}]},{"label":["Degree"],"points":[{"start":2085,"end":2120,"text":"B.E., Computer Science & Engineering"}]},{"label":["Years_of_Experience"],"points":[{"start":1972,"end":1980,"text":" 3. 2Year"}]},{"label":["Skills"],"points":[{"start":1594,"end":1599,"text":"Spring"}]},{"label":["Tools"],"points":[{"start":1418,"end":1424,"text":"Eclipse"}]},{"label":["Skills"],"points":[{"start":1216,"end":1218,"text":"ATG"}]},{"label":["Skills"],"points":[{"start":1168,"end":1180,"text":"ATG Framework"}]},{"label":["Skills"],"points":[{"start":1156,"end":1161,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":943,"end":955,"text":"ATG Framework"}]},{"label":["Skills"],"points":[{"start":827,"end":830,"text":"J2EE"}]},{"label":["Skills"],"points":[{"start":787,"end":790,"text":"J2EE"}]},{"label":["Skills"],"points":[{"start":513,"end":515,"text":"ATG"}]},{"label":["Mobile_No"],"points":[{"start":289,"end":299,"text":"9945105862 "}]},{"label":["Email_Address"],"points":[{"start":249,"end":273,"text":"ashishvikash969@gmail.com"}]},{"label":["Name"],"points":[{"start":132,"end":137,"text":"Ashish"}]}],"extras":null,"metadata":{"first_done_at":1564206958000,"last_updated_at":1564206958000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "B H A R G A V\nR E D D Y\n\nDATA MIGRATION TEAM LEAD \nACCENTURE\n\nABOUT ME Over 7 Years of experience in analysis,design,test & \nimplementation of Data Warehouse in development, test & \nproduction environments.expert in writing SQL, working on \nETL,Data Cleansing,Data Scrubbing, Data Migration and Data \nvalidation and reconciliation in Oracle,DB2 and Netezza data bases. \nProficient in building end to end data integration and workflow \nsolution from input source extraction, apply various \ntransformations and data loading. Complete understanding of \nSDLC and Agile methodologies.  \n\n+91-8553012789 bhargavreddya@gmail.com\n\nVINAYAKA MISSIONS UNIVERSITY, SALEM \n\nB-TECH IN BIO-INFORMATICS \n\nEDUCATION\n\nINFORMATICA \n\nORACLE PL/SQL \n\nUNIX \n\nDB2 \n\nNETEZZA \n\nMSTR \n\nCOGNOS\n\nAWS \n\nSIEBEL/EIM \n\nETL TESTING \n\nHP ALM \n\nAGILE/JIRA \n\nGOLDEN GATE \n\nWDEP\n\nSKILLS\n\nEXPERIENCE TEAM LEAD | Accenture \nWorking in Marriott Integration | Dec 2017 - Present.\n\n \nSr.Software Engineer | Accenture \nWorked In Starwood Data Analytics | Sep 2015 - Nov 2017 \n\n \nSoftware Engineer | Accenture \nWorked In Exadata migration | Nov 2013 - Nov 2015 \n\n \nAssociate Software Engineer | Accenture \nWorked In Starwood Data warehouse | Sep 2011 - Nov 2013 \n\n \n \n\n\n\nEXPERIENCE ETL TESTING \nRole :- Software Engineer(2014-2018) \nTechnologies used:- Oracle,DB2,Netezza,UNIX,JIRA and Informatica \nProject Overview:- Starwood EDW is central data repository for all the \napplications which was consistently growing with new enhancements and \ndata volume.\n\nRESPONSIBILITIES\n\nManaged a team of 7 people.\nResponsible for validation of ETL processes using informatica 9.1\nAnalyzed and validated ETL and PL/SQL code enhancements\nCreated test approach and test design for releases.\nReviewed the test cases created by the team and provided \nsuggestions to add\nscenarios to cover negative testing.\nCreated test data for different scenarios for maximum coverage.\nExecuted Test cases in JIRA and attached proofs.\nDefect logging and tracking in JIRA\n\nSTARWOOD - MARRIOTT INTEGRATION\nRole :- Team Lead(2017-current) \nTechnologies used:- Oracle,DB2, NETEZZA,Informatiaca,Cagnos,MSTR &\nUNIX \nProject Overview:- The project is about migrating 196Tb of Starwood\nhotel data to Marriott systems in support of the Marriott and Starwood\nhotel merger. \n\nEXPERIENCE\n\nRESPONSIBILITIES\n\nAnalyzed starwood and marriott warehouses and identified the\ncommon points of integration. \nCreated the mapping documents for each of the stages of data\nmigration and got it approved from the Business. \nSet up the conversion environment to felicitate the data migration\nprocess between both the warehouses. \nWorked with clients in gathering business requirements for data\nmigration needs \nDeveloped new mappings to manipulate the data in the format\ncompatible with the target warehouse. \nMitigate the issue well in advance and stopped them flowing to the \nproduction environment. \nDeploy the code to Lower and higher environment. \nSetting up the meetings with stake holders across and integrating the \ninterim requirement changes into the mapping document and code. \nConduct daily and weekly meetings with the team to ensure the timely\ndelivery. \nPerform migration and testing of static data and transaction data from\none core system to another. \nNormalize the De-normalized data from source DWH to move up to\nOLTP system and push it down to target OLAP system.\nSupport deployment of solution, cut over planning and execution\nactivities \n\n\n\nEXPERIENCE\n\nEXPERIENCE\n\nEXADTA MIGRATION\nRole :- Senior Software Engineer(2014-2016) \nTechnologies used:- Oracle Exadata,Informatiaca,MSTR,UNIX and APX \nProject Overview:- project is to upgrade the hardware and software and\nmigrate the date from legacy system and Flat files to Exadat X-6 server\nas a central reporting warehouse.\n\nSTARWOOD ETL & DWH DEVELOPER\nRole :- Software Engineer(2011-2014) \nTechnologies used:- Oracle Exadata,Informatiaca,MSTR,UNIX and APX \nProject Overview:- Starwood EDW is central data repository for all the \napplications which was consistently growing with new enhancements and \ndata volume.\n\nRESPONSIBILITIES\n\nAnalyzing the risks involved in migrating the data centers and server \nupgrades.\nAssist in designing, planning and managing the data migration process \nWork with subject matter experts and project team to identify, define, \ncollate, document and communicate the data migration requirements. \nPerform source system data analysis in order to manage source to \ntarget data mapping. \nPerform migration and testing of static data and transaction data from \none core system to another. \nPerform data migration audit, reconciliation and exception reporting. \nManage cross-program data assurance for physical data items in \nsource and target systems. \nDeveloped APEX application and hosted it on AWS cloud to capture \nthe internal metrics. \n\nRESPONSIBILITIES\n\nResponsible for developing, support and maintenance for the ETL\n(Extract,Transform and Load) processes using Informatica Power\nCenter 9.1. \nWorked on ETL and PL/SQL code Enhancements to provide the better \nfunctionality to the business user with proper coding standards. \nPrepared code migration document to move the mappings and PL/SQL\ncode from development to testing and then to production repositories. \nParsed high-level design specification to simple ETL coding and\nmapping standards. \nAttending daily scrum calls with stake holders.","annotation":null,"extras":null,"metadata":{"first_done_at":1564231901000,"last_updated_at":1564242362000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "Backend Primary Skills\n\tExperience(In Years)\n\tScreening Criteria for Skill Sets(Minimal Years required)\n\tCandidates Rating(For 10)\n\tCandidate Description(Optional Notes)\n\n\tJAVA\n\t3.0 Years \n\t3.0 Years\n\t9\n\t \n\n\tMICROSERVICES\n\t1.6 Years\n\t1.0 Year\n\t8\n\t \n\n\tSPRING BOOT\n\t1.6 Years\n\t1.0 Year\n\t8\n\t \n\n\tAngular\n\t1.0 Years\n\t1.0 Year\n\t8\n\t \n\n\n\nChandra Bhushan Kumar\t\t \nMob. No. :  +91 8105446706\n\t\t\t\t                  \t\t\t            E-mail:-b.chandra10@yahoo.com                                                   \t\t                           \n\t\t\t         \nSYNOPSIS:\n· 3.0 years Training and experience in IT with expertise in design and development and of client server applications using Microservices with Spring Boot, JAVA / J2EE.\n· Good Experience in Developing Applications using JSP, Servlets, JDBC\n·  Implementation Knowledge in J2EE Design Patterns.\n·  Implementation Knowledge in Hibernate Framework.\n·  Implementation Knowledge of Spring Framework and Microservices with Spring Boot.\n·  Knowledge in Java Web Services (Restful).\n·  Knowledge of Data Structure and Algorithms.\n·  Implementation Knowledge of MySQL Workbench 6.3\n·  Implementation knowledge of java script and Angular JS and Angular 4.0\n·  Implementation Knowledge of Debugging\n\t\nExperience:\n\nInfosys Ltd.\n                              Nov 2017 - Till Date\nTechnology Analyst \n\t\tWorking as a Technology Analyst at Infosys Ltd Bangalore. To develop on Gap Project\n Ingenuousys Technologies Pvt Ltd, Bengaluru \t\t\t\t   Dec 2016– Sep 2017\nSoftware Engineer\nWorking as a Java Developer at Ingenuousys Technologies Bangalore. To develop  MEDMYLIFE\n\nLARA Technologies , Bengaluru \t\t\t\t                             Jan 2016– June 2016\nTrainee Engineer\nWorking as a Java Developer at LARA Technologies Bangalore. \n\n\nTechnical Operating Skills:\t      \t \nSystem \t        \t       : Windows\nProgramming language      : Java 7, Java 8  SQL\nTools\t\t      \t       :  IntelliJ IDEA, Eclipse \nWeb Server\t\t       : Apache Tomcat 8.0\nFrameWork\t\t       :  MicroServices, Spring Boot, Spring  4.2, Hibernate 4.2, Struts 2.0, AngularJS 1.6             \n                                                   &Angular 4.0\n\nDatabase\t\t       : Mongo dB, MYSQL. MySQL Workbench 6.3\nTesting FrameWork\t        : Junit, Mockito\nVersion control\t        : Svn repository system, GitHub\nScripting Language\t        : java Script, Angular JS ,Angular 2.0\nWeb Services\t\t        : (RESTful)\nLogging Tool\t\t        : Log4j\nBuild Tool\t\t        : Maven 3.3\nOthers \t\t\t        : XML, JSON, HTML5, CSS\nGood knowledge of Data Structure and Algorithms.\nEducation:\t\nB.E. (2010-2014)\n\t\t73.80%   Rajeev Gandhi Proudyogiki Vishwavidyala, Bhopal (M.P.)\n\tStream   : Electrical & Electronics.\n12th (2009)\n\t\t71.60%   Bihar School Examination Board Patna (Bihar)\n10th (2007)\n\t\t73.40%   Bihar School Examination Board Patna (Bihar)\n\n\nProject Undertaken:\n· #1 Name of the project                         Gap (Ongoing)\nClient                          :   Gap\nTechnology Used     :  Java 8,  MicroServices, REST Services, Spring Boot \t\t\t        HTML5, CSS Mongodb , JavaScript                        \nEnvironment               :   Windows\n· Description: \n This project   is (Devops)  a leading global retailer offering clothing, accessories, and personal care products for men, women, and children. This project has built a highly profitable online and mobile business with double-digit sales growth and industry leading capabilities. Gap Inc. sites are built on a proprietary e-commerce platform that enables wide-ranging capabilities, including cross-brand shopping, omni-channel services, and an upcoming buy online, pick-up in store service, as well as a new personalization engine powered by customer data. Over the next three years, Gap Inc. will accelerate this work with a continued significant investment in areas including direct fulfillment capacity, loyalty, personalization, omni-channel services, artificial intelligence and other data-driven customer experiences User will have secure online access to get their products are available for purchase .User can create purchase order they can update requirement and   \nResponsibilities: \n\n· Handling SCMS (Supply chain management system) interface of this project.\n· Creating of Po(purchase order) in Scms.\n· Handling User Request and interacting to Clint  \n· Handling data mismatch Sku related issue.  \n· Creating invoice and sending to Clint.\n· Handling the server related issue.\n\n\n\n\t\n\n\n\n\n\t\t\t\t\t\t\t\t\t\nProject Undertaken:\n· #2 Name of the project                         MEDMYLIFE \nClient                          :   Analytic Squad\nTechnology Used     :  Java 8, REST Services, Spring Boot, Hibernate4.2, Angular 2.0  \t\t\t       HTML5, CSS MYSQL, JavaScript                        \nEnvironment               :   Windows, STS\n· Description: \n MEDMYLIFE will enable physicians, patients, pharmacies, hospital and diagnostic/imaging laboratories to access real-time information for their needs. Patients will have secure online access to their health records from a customized home page. Physicians will benefit from cost-effective, complete office automation with central real-time patient information, electronic prescriptions, and pre-defined notes. Pharmacists will appreciate the ease of receiving online prescriptions. Further, legibility problems will be alleviated significantly reducing the risk of error. Two of the most appealing features of this system are that it eliminates wait-time and is extremely cost-effective.\nResponsibilities: \n· Implementation of controller components (Actions).\n· Developed  Restful  services using Spring framework supporting  JSON\n· Developed a Restful service to provide all the CRUD capabilities.\n· Implementation of Presentation Components(Angular 2.0 Form-validations)\n· Development of Server Side Business logic(Session Facades, Application Service)\n· Implemented Persistence Classes, Mapping documents and called from Session beans.\n· Data binding using Angular JS.\n· Unit Testing with JUnit.\n\nProject #3:\t\t\t\t\t\t\t\nTitle                            :   Hive \nTechnology Used       :   Java8, Rest Services, Spring 4.2, Hibernate4.2, Angular JS. \n\t\t\t  MYSQL, HTML5, CSS \nEnvironment              :  Eclipse\nDescription: \nThe Website Hive developed for Apartment owners and  Residence group. \nProvides information regarding   integrated with the Residents List - can easily identify the author of a Conversation post   and the services. Conversations can be made visible to all or to \"owners only\". Have discussions on subjects related to your neighborhood organized as discussion threads. Hive is a one stop shop for all needs of Society Management. It covers all aspects of Society Management and makes life easy for the Management Committee as well as for residents.  \nResponsibilities:\n· Implementation of controller components (Actions).\n· Developed REST web services using Spring framework supporting JSON\n· Developed a RESTful service to provide all the CRUD capabilities.\n· Responsible for writing presentation Angular JS.\n· Responsible for writing the Enterprise Beans.\n· Implemented Persistence Classes, Mapping documents and called from Session beans.\n· Responsible for writing Dao Class.\n· Unit Testing with JUnit.\n\n\n\n\n\n\nProject #4:\t\t\t\t\t\t\t\nTitle                           :   Insurance System\nTechnology Used      :Java 7, Spring 4., Hibernate, Angular JS, MYSQL, HTML5, CSS  \nEnvironment             : \t Eclipse  \nDescription: \nThe Website “Insurance Life Application” developed for Insurance Company usage. \nProvides information regarding the services and its policies. The home page incorporates basic options like available policies and their benefits, terms and conditions new schemes etc., Animated- links are provided to navigate to different types of policies like vehicles Insurance. Life Insurance,   Home Insurance and Health Insurance. This site also allows the policy holders to know the status of information regarding their policy and new policy information. Users can do online transactions.\nResponsibilities:\n· Implementation of controller components (Actions).\n· Responsible for writing presentation  Angular JS.\n· Responsible for writing the Enterprise Beans.\n· Responsible for writing Dao Class.\n· Unit Testing with JUnit.\n\nProject #5:\t\t\t\t\t\t\t\nTitle                            : SPARE PARTS  MANAGEMENT\nClient\t\t          : GSB ENTERPRISES\nTechnology Used       : Java 7, Spring 4.2, Hibernate, JSP, MYSQL, HTML5, CSS  \nEnvironment              : \tEclipse  \nDescription: \nThe Spare Parts Management Systems is an application developed to aid an end user\nto do some transactions like Sales, Orders and Receiving. This Project chooses an automobile\nDealer of particular of company maintaining his stock properly through cumbersome transaction like selling ordering and delivering stock.This Software takes care of daily and monthly report during the transactions.\n\nResponsibilities:\n· Implementation of controller components (Actions).\n· Responsible for writing presentation Action classes and JSP.\n· Responsible for writing the Enterprise Beans.\n· Responsible for writing Dao Class.\n· Unit Testing with JUnit.\n\nPersonal information:\nFather’s Name\t\t\t: \tMahadev Sah\nDate of Birth \t\t\t: \t15/01/1993\nGender\t\t\t\t:\tMale\nLanguages Known    \t\t: \tHindi, English\nDeclaration: I declare that the fact stated hereby is authentic and true to the best of my knowledge.\nPlace: Bangalore\nDate   \t\t\t\t\t\t\t\t\t\t\t\t\t\n(Chandra Bhushan Kumar)","annotation":[{"label":["Name"],"points":[{"start":9416,"end":9436,"text":"Chandra Bhushan Kumar"}]},{"label":["Tools"],"points":[{"start":8473,"end":8479,"text":"Eclipse"}]},{"label":["Skills"],"points":[{"start":8439,"end":8441,"text":"CSS"}]},{"label":["Skills"],"points":[{"start":8432,"end":8436,"text":"HTML5"}]},{"label":["Skills"],"points":[{"start":8425,"end":8429,"text":"MYSQL"}]},{"label":["Skills"],"points":[{"start":8409,"end":8417,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":8397,"end":8402,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":8389,"end":8392,"text":"Java"}]},{"label":["Skills"],"points":[{"start":8126,"end":8135,"text":"Angular JS"}]},{"label":["Tools"],"points":[{"start":7412,"end":7418,"text":"Eclipse"}]},{"label":["Skills"],"points":[{"start":7378,"end":7380,"text":"CSS"}]},{"label":["Skills"],"points":[{"start":7371,"end":7375,"text":"HTML5"}]},{"label":["Skills"],"points":[{"start":7364,"end":7368,"text":"MYSQL"}]},{"label":["Skills"],"points":[{"start":7352,"end":7361,"text":"Angular JS"}]},{"label":["Skills"],"points":[{"start":7341,"end":7349,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":7330,"end":7335,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":7322,"end":7325,"text":"Java"}]},{"label":["Skills"],"points":[{"start":7014,"end":7020,"text":"Angular"}]},{"label":["Skills"],"points":[{"start":6921,"end":6927,"text":"RESTful"}]},{"label":["Skills"],"points":[{"start":6902,"end":6905,"text":"JSON"}]},{"label":["Skills"],"points":[{"start":6874,"end":6879,"text":"Spring"}]},{"label":["Tools"],"points":[{"start":6195,"end":6201,"text":"Eclipse"}]},{"label":["Skills"],"points":[{"start":6162,"end":6164,"text":"CSS"}]},{"label":["Skills"],"points":[{"start":6155,"end":6159,"text":"HTML5"}]},{"label":["Skills"],"points":[{"start":6148,"end":6152,"text":"MYSQL"}]},{"label":["Skills"],"points":[{"start":6130,"end":6136,"text":"Angular"}]},{"label":["Skills"],"points":[{"start":6116,"end":6124,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":6104,"end":6109,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":6082,"end":6085,"text":"Java"}]},{"label":["Skills"],"points":[{"start":5954,"end":5963,"text":"Angular JS"}]},{"label":["Skills"],"points":[{"start":5737,"end":5743,"text":"Angular"}]},{"label":["Skills"],"points":[{"start":5620,"end":5623,"text":"JSON"}]},{"label":["Skills"],"points":[{"start":5591,"end":5596,"text":"Spring"}]},{"label":["Operating_Systems"],"points":[{"start":4766,"end":4772,"text":"Windows"}]},{"label":["Skills"],"points":[{"start":4701,"end":4704,"text":"Java"}]},{"label":["Skills"],"points":[{"start":4694,"end":4698,"text":"MYSQL"}]},{"label":["Skills"],"points":[{"start":4690,"end":4692,"text":"CSS"}]},{"label":["Skills"],"points":[{"start":4683,"end":4687,"text":"HTML5"}]},{"label":["Skills"],"points":[{"start":4660,"end":4666,"text":"Angular"}]},{"label":["Skills"],"points":[{"start":4646,"end":4654,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":4633,"end":4643,"text":"Spring Boot"}]},{"label":["Skills"],"points":[{"start":4610,"end":4613,"text":"Java"}]},{"label":["Operating_Systems"],"points":[{"start":3125,"end":3131,"text":"Windows"}]},{"label":["Skills"],"points":[{"start":3060,"end":3063,"text":"Java"}]},{"label":["Skills"],"points":[{"start":3046,"end":3048,"text":"CSS"}]},{"label":["Skills"],"points":[{"start":3039,"end":3043,"text":"HTML5"}]},{"label":["Skills"],"points":[{"start":3016,"end":3026,"text":"Spring Boot"}]},{"label":["Skills"],"points":[{"start":2986,"end":2998,"text":"MicroServices"}]},{"label":["Skills"],"points":[{"start":2977,"end":2980,"text":"Java"}]},{"label":["Degree"],"points":[{"start":2572,"end":2575,"text":"B.E."}]},{"label":["Skills"],"points":[{"start":2507,"end":2509,"text":"CSS"}]},{"label":["Skills"],"points":[{"start":2500,"end":2504,"text":"HTML5"}]},{"label":["Skills"],"points":[{"start":2494,"end":2497,"text":"JSON"}]},{"label":["Skills"],"points":[{"start":2489,"end":2491,"text":"XML"}]},{"label":["Tools"],"points":[{"start":2459,"end":2463,"text":"Maven"}]},{"label":["Tools"],"points":[{"start":2431,"end":2435,"text":"Log4j"}]},{"label":["Skills"],"points":[{"start":2398,"end":2404,"text":"RESTful"}]},{"label":["Skills"],"points":[{"start":2361,"end":2367,"text":"Angular"}]},{"label":["Skills"],"points":[{"start":2349,"end":2358,"text":"Angular JS"}]},{"label":["Skills"],"points":[{"start":2336,"end":2346,"text":"java Script"}]},{"label":["Tools"],"points":[{"start":2300,"end":2305,"text":"GitHub"}]},{"label":["Tools"],"points":[{"start":2277,"end":2297,"text":"Svn repository system"}]},{"label":["Skills"],"points":[{"start":2243,"end":2249,"text":"Mockito"}]},{"label":["Skills"],"points":[{"start":2236,"end":2240,"text":"Junit"}]},{"label":["Skills"],"points":[{"start":2188,"end":2202,"text":"MySQL Workbench"}]},{"label":["Skills"],"points":[{"start":2181,"end":2185,"text":"MYSQL"}]},{"label":["Skills"],"points":[{"start":2171,"end":2178,"text":"Mongo dB"}]},{"label":["Skills"],"points":[{"start":2139,"end":2145,"text":"Angular"}]},{"label":["Skills"],"points":[{"start":2060,"end":2068,"text":"AngularJS"}]},{"label":["Skills"],"points":[{"start":2048,"end":2053,"text":"Struts"}]},{"label":["Skills"],"points":[{"start":2033,"end":2041,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":2020,"end":2025,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":2007,"end":2017,"text":"Spring Boot"}]},{"label":["Skills"],"points":[{"start":1992,"end":2004,"text":"MicroServices"}]},{"label":["Skills"],"points":[{"start":1953,"end":1965,"text":"Apache Tomcat"}]},{"label":["Tools"],"points":[{"start":1923,"end":1929,"text":"Eclipse"}]},{"label":["Tools"],"points":[{"start":1908,"end":1920,"text":"IntelliJ IDEA"}]},{"label":["Skills"],"points":[{"start":1880,"end":1882,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":1872,"end":1875,"text":"Java"}]},{"label":["Skills"],"points":[{"start":1864,"end":1867,"text":"Java"}]},{"label":["Operating_Systems"],"points":[{"start":1828,"end":1834,"text":"Windows"}]},{"label":["Skills"],"points":[{"start":1715,"end":1718,"text":"Java"}]},{"label":["Skills"],"points":[{"start":1525,"end":1528,"text":"Java"}]},{"label":["Skills"],"points":[{"start":1185,"end":1191,"text":"Angular"}]},{"label":["Skills"],"points":[{"start":1170,"end":1179,"text":"Angular JS"}]},{"label":["Skills"],"points":[{"start":1103,"end":1117,"text":"MySQL Workbench"}]},{"label":["Skills"],"points":[{"start":996,"end":999,"text":"Java"}]},{"label":["Skills"],"points":[{"start":967,"end":977,"text":"Spring Boot"}]},{"label":["Skills"],"points":[{"start":927,"end":932,"text":"Spring"}]},{"label":["Skills"],"points":[{"start":875,"end":883,"text":"Hibernate"}]},{"label":["Skills"],"points":[{"start":707,"end":710,"text":"JAVA"}]},{"label":["Skills"],"points":[{"start":694,"end":704,"text":"Spring Boot"}]},{"label":["Years_of_Experience"],"points":[{"start":554,"end":562,"text":"3.0 years"}]},{"label":["Email_Address"],"points":[{"start":426,"end":447,"text":"-b.chandra10@yahoo.com"}]},{"label":["Mobile_No"],"points":[{"start":367,"end":380,"text":"+91 8105446706"}]},{"label":["Name"],"points":[{"start":330,"end":350,"text":"Chandra Bhushan Kumar"}]},{"label":["Skills"],"points":[{"start":292,"end":298,"text":"Angular"}]},{"label":["Skills"],"points":[{"start":251,"end":261,"text":"SPRING BOOT"}]},{"label":["Skills"],"points":[{"start":208,"end":220,"text":"MICROSERVICES"}]},{"label":["Skills"],"points":[{"start":172,"end":175,"text":"JAVA"}]}],"extras":null,"metadata":{"first_done_at":1564207384000,"last_updated_at":1564207384000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "CURRICULUM VITAE\nCHETHAN R REDDY\nEmail: chethanchiragreddy@gmail.com\n          chethanchiragreddy@yahoo.com\nMobile: +91-9886814537\nRes:     080-28538002\nSynopsis\n\nMy 8 years of professional experience has seen me extensively design and develop Mobile framework & Applications, on Android. 4.6yr in Devops. My strong technical foundations coupled with excellent interpersonal skills, urge to excel, ability to work well within a team and uncompromising integrity serve as my greatest strengths.\n\n· Demonstrated ability in implementing cloud enabled mobile applications\n\n· Developed mobile products that are successful in the market.\nEducation\n\nVEMANA INSTITUTE OF TECHNOLOGY (VIT), Bangalore\n                              2010\n· Conferred a degree of Bachelor of Engineering, Information Technology and Engineering (Secured an Aggregate of 61%)\nExperience Summary\n\n· Working with Robert Bosch Engineering and Business Solution Private Limited. (http://www.bosch-india-software.com) as a permanent employee since December 2016 with the designation of Senior Software Engineer.\n· Worked with ACS, A Xerox Company. (http://www.xerox.com) as a permanent employee since August 2014 to November 2016 with the designation of Systems Development Specialist.\n· Worked with Symphony Teleca Corporation Pvt Ltd. (http://www.symphonyteleca.com) as a permanent employee since March 2013 to July 2014 with the designation of Software Engineer.\n· Worked with v2soft Pvt Ltd. (http://www.v2soft.com) as a permanent employee since November 2012 to March 2013 with the designation of Android Developer.\n· Worked with Softtrends Software Pvt. Ltd. (http://www.softtrends.com) product division’s platform development group & partner of Koe Innovations Software Pvt. Ltd (http://www.koeinnovations.com); developing Mobile Applications for various mobile platforms like (Android, BlackBerry, iPhone, Symbian and J2ME) as a permanent employee since June 2010 to Oct 2012 with the designation of Software Engineer.\n· In-depth design, programming & debugging experience in product development on Android platforms with complete knowledge of Software Development Life Cycle – Waterfall and Agile Methodologies (SCRUM).\n· Designing the application architecture with respect to Module view controller (MVC) architecture standards.\n· Designing the application architecture with respect to Module view Pattern (MVP) architecture standards.\n· Experience in Location Based Search and Mapping using Global Positioning System (GPS) and Integrating Google map on Android mobile platforms\n· Experience in using Content Provider in android platform using java for persisting user data in DB. I have worked extensively on designing, storing and retrieving data from the SQLite database using java.\n\n· Experience in monitoring network connections (Wi-Fi, GPRS etc.)  Using Connectivity Manager in java.\n\n· Worked on recording and playing audio using android media framework.\n· Worked on phone call logging using android Telephony framework and Broadcast Receiver services also managing default Phonebook manager in android.\n· Extensively developed UI using various android UI components like Table View, Custom cells in Table View, Expandable List, Base Adapters, Alert Dialogs, Progress Dialogs, Layouts etc, in android using java.\n\n· Worked in creating and using services in android.\n· Experience in getting the GPS location, heading and plotting the location’s point on the map using Location Manager and Map Activity in Android platform using java. I have extensive knowledge on creating custom overlay pins, plotting custom pins on a map and used Decoder to get the location address, zip code based on GPS location.\n\n· Experience in Phone call monitoring, Recording the Phone Call on Android platform and Audio Recording and playing features on Android and iPhone platforms.\n· Exposure in working with Device Calendar, Device Contacts, Device Camera and triggering / monitoring other applications programmatically.\n· Experience in Designing and Developing in SQLite on Android platforms respectively.\n· Exposure in Developing Services on Android platforms.\n· Monitoring and capturing SMS on Android platforms\n· Experience on Cloud enabled Mobile application.\n· Experience in Product installation, Configuration, Test case preparation, planning and execution of software releases.\n\n· Moderated, Handled and supported a UI Evaluation and Usability workshop for the Product with end-users.\n· Product presentations and demonstrations to senior management and prospective customers.\n\n· Evaluating and integrating third party components and interfacing with vendors to expedite a solution for issues.\n\n· Prepared a comprehensive Ramp-up plan for the new team mates for faster knowledge transfer.\nSkill Set\n\tCore Skills\n\tAndroid, Java, XML, JNI,Dagger,Rx java, GIT,Retrofit,Kotlin,  JSON\n\n\tDatabases\n\tOracle 8.0 ,SQLite 3.0 and MS-Access 2000\n\n\n\tOperating Systems\n\tWindows, Linux and Macintosh\n\n\tSoftware Design\n\tOOAD, UML and Design Patterns.\n\n\tVersion Control \n\tSVN,GIT\n\n\tTool’s/IDE\n\tAndroid Studio, Eclipse, SQLite, Android SDK, Android NDK.\n\n\tBug Tracker\n\tBugzilla\n\n\nProject Summary \n\n· SF Shipyard(San Francisco Shipyard)\n\tOrganization\n\tBosch\n\n\tProject Abstract \n\tSF Shipyard community app is developed for residents and businesses within FivePoint community, With the introduction of new infrastructure, state-of-the-art amenities, and modern housing opportunities along the picturesque waterfront, The San Francisco Shipyard affords residents a vibrant new community conveniently situated near downtown San Francisco and the Peninsula. The San Francisco Shipyard delivers a dynamic lifestyle to residents, offering a mix of one-, two-, and three-bedroom contemporary urban flats and townhomes ranging in size from approximately 500 to 1,500 square feet.\n\n\tTeam Size\n\t1\n\n\tSystem Environment\n\tJava,Kotlin,Android SDK,Andriod Studio ,JIRA,MQTT (Message Queuing Telemetry Transport),Dagger, REST APIs,Retrofit  & GIT\n\n\n\n\tDuration\n\tDec 2016 – in progress.\n\n\n\n\nResponsibilities/Highlights: \n\n· Involved in customer interaction, requirement analysis, design and development for the product.\n· Developing User Interface of the application.\nGoogle Play Link:\nhttps://play.google.com/store/apps/details?id=com.bosch.lennar&hl=en_IN\nDescription:\nSF Shipyard community app is developed for residents and businesses within FivePoint \n\ncommunity, With the introduction of new infrastructure, state-of-the-art amenities, and modern housing opportunities along the picturesque waterfront, The San Francisco Shipyard affords residents a vibrant new community conveniently situated near downtown San Francisco and the Peninsula. The San Francisco Shipyard delivers a dynamic lifestyle to residents, offering a mix of one-, two-, and three-bedroom contemporary urban flats and townhomes ranging in size from approximately 500 to 1,500 square feet.\n\n· See all points of interest near the community, local attractions, check out your \n  neighborhood, restaurants, shops and get connected faster than ever.\n· Stay connected with community blog to discover the latest on local events and business.\n· Spot the shuttle anytime, live shuttle tracking, hours of operation and driver contact details.\n· Access the customer care team has become even more easy.\n· Explore the full SF Shipyard Website.\n· Walk me home feature using MQTT (Message Queuing Telemetry Transport).\n\n· Find the key information on your home, including maintenance tips, warranties and service requests.\n· Itrams (Intelligent Transport Management system)\n\tOrganization\n\tBosch\n\n\tProject Abstract \n\tIntelligent transport management system (iTraMS) is an end-to-end Vehicle to Infrastructure (V2I) ecosystem, designed to address the challenges of modern-day transportation and management of vehicles. This solution can be deployed across all vehicle platforms – commercial vehicles, passenger cars, construction machinery as well as farm equipment. The application sectors are – fleet management, logistics management and off-highway.\n\n\tTeam Size\n\t1\n\n\tSystem Environment\n\tJava, Android SDK,Andriod Studio ,JIRA ,Retrofit & GIT\n\n\n\tDuration\n\tDec 2016 – in progress.\n\n\n\n\nResponsibilities/Highlights: \n\n· Involved in customer interaction, requirement analysis, design and development for the product.\n· Developing User Interface of the application.\nDescription:\nIntelligent transport management system (iTraMS) is an end-to-end Vehicle to Infrastructure (V2I) ecosystem, designed to address the challenges of modern-day transportation and management of vehicles. This solution can be deployed across all vehicle platforms – commercial vehicles, passenger cars, construction machinery as well as farm equipment. The application sectors are – fleet management, logistics management and off-highway.\n· Portable Administrative Sales Device (PASD)\n\tOrganization\n\tACS,a Xerox Company(TSG)\n\n\tProject Abstract \n\tThe PASD (SEPTA) interacts with Account Management for authentication, sales and customer service. It interacts with Equipment Services for maintenance.(EMV Contactless Payment)\n\n\tTeam Size\n\t1\n\n\tSystem Environment\n\tJava, Android SDK,Eclipse & SVN\n\n\n\n\tDuration\n\tJan 2015 –  Nov 2016\n\n\nResponsibilities/Highlights: \n\n· Involved in customer interaction, requirement analysis, design and development for the product.\n\n· Developing User Interface of the application.\n· Validates SEPTA Fare Media Contactless Card.\n· Add wallet to patron’s card using Cash, Credit-Card using Transit Open Payment System.\n· Developing Global Service Manger (GSM) Model .\n· Presented & demonstrated end-to-end product functionality.\nDescription:\nThe PASD interacts with Account Management for authentication, sales and customer service. It interacts with Equipment Services for maintenance.\n· Hand-Held Card Reader, HCR(GCRTA)\n\tOrganization\n\tACS,a Xerox Company\n\n\tProject Abstract \n\tHand-Held Card Reader, HCR under Greater Cleveland Regional transit Authority, GCRTA.\nApplication is used by the transport inspector to inspect the traveller’s pass.\nBy Using Grabba Device (Smart Card reader ,Magnetic strip Card Reader)\n\n\tTeam Size\n\t2\n\n\tSystem Environment\n\tJava, Android SDK,Eclipse & SVN\n\n\n\tDuration\n\tAug 2014 – May 2015\n\n\n\nResponsibilities/Highlights: \n· Developing User Interface of the application\n\n· Parsing downloaded binary files for processing\n·  S- card parsing, Parsing the data mapped in the Mifare Classic card(S-card, Traveller’s Pass)\n·  Writing inspection records of S- card on the database by applying business rules.\nDescription:\n\nHand-Held Card Reader, HCR under Greater Cleveland Regional Transit Authority, GCRTA\n· BMC Crystal Ball (Schneider electric)\n\n\tOrganization\n\tSchneider electric\n\n\n\n\tProject Abstract \n\tBMC Crystal Ball device captures the data from the BMC Milk Container & send the same to the Android devices Via Bluetooth Stack. In which we are building Android application.\n\n\n\tTeam Size\n\t2\n\n\n\tSystem Environment\n\tJava, ,Android SDK,Eclipse & GIT\n\n\n\n\tDuration\n\tOct 2013 –  June 2014\n\n\n\nResponsibilities/Highlights: Involved in customer interaction, requirement analysis, design and development for maintenance and new feature additions for the product.\nDescription:\n\nBMC Crystal Ball device captures the data from the BMC Milk Container & send the same to the Android devices using Bluetooth stack. The data is accumulated in the Android devices and backup to the centralized BMC server.\n· GIGASET (VOIP TABLET)\n\n\tOrganization\n\tGIGASET is VOIP based Tablet\n\n\n\n\tProject Abstract \n\tGIGASET is VOIP based Tablet, which we are building customize applications.\n\n\n\tTeam Size\n\t6\n\n\n\n\tSystem Environment\n\tJava, JNI,Android SDK, Android NDK,Eclipse & GIT\n\n\n\tDuration\n\tMarch 2013 –  May 2014\n\n\n\nResponsibilities/Highlights: \n· Involved in customer interaction, requirement analysis, design and development for the product.\n· Developing Customize Contacts, CallList Application to GIGASET device.\n· Worked on Import/Export Contacts in Contacts Application.\n· Worked on Contacts Sync in Contacts Application.\n· Worked on Groups Contacts in Contacts Application.\n· Creating Customize UI Screens for the Contacts, CallList Application.\n· Share Visible Contacts in Contacts Application.\nDescription:\n \nGIGASET is VOIP based Tablet, Which we are building Customize application like, Contacts,CallList,Phone,Sip-Call etc & Also Customizing the Android Framework. The Tablet is running on Android Version (4.2 Jelly Bean).\n· Backup Assistant\n\n\tOrganization\n\tSynchronoss Technologies\n\n\tTeam Size\n\t6\n\n\tSystem Environment\n\tJava, Android\n\n\tDuration\n\tFeb 2011 – Aug 2012\n\n\nResponsibilities/Highlights: Involved in customer interaction, requirement analysis, design and development for maintenance and new feature additions for the product.\nGoogle Play Link:\nhttps://play.google.com/store/apps/details?id=com.fusionone.android.sync.vzbuaclient&hl=en\nDescription:\n          Backup Assistant is a product that is used to backup the PIM details available on the mobile to a centralized server. Backup Assistant uses SyncML protocol to interact with the centralized Synchronoss server to store and retrieve the PIM data. Backup Assistant is developed on Device Service Platform and Device Synchronization Platform. Currently, only synchronization of only contacts and groups are supported on Android.\n\n· Smart Mobility (Unterthering Application)\n\tOrganization\n\tSynchronoss Technologies\n\n\tTeam Size\n\t4\n\n\tSystem Environment\n\tJava, Android\n\n\tDuration\n\tJuly 2011 – Aug 2011\n\n\nResponsibilities/Highlights: Involved in customer interaction, requirement analysis, design and development.\nDescription:\nUnterthering application would block the Android mobile device to be used as a modem. This application blocks the users to block the mobile being used to connect to internet through USB, Bluetooth and Wifi Hotspot.\n\n· TimedSMS for Android Platform\nDescription: TimedSMS is SMS scheduler application with loads features related to SMS scheduling, archiving, and effectively replacing the default SMS application that comes with Android devices. It allows\nUser to schedule SMS messages to be sent out anytime in the future which can also repeat on daily, weekly or monthly basis. There is no limit on the number of messages that can be scheduled and the messages can be scheduled for delivery to individual users or groups. The system SMS feature in timed SMS allows user to view message sent and received using default SMS application in a conversation style.\nVault features is particularly powerful as the user can define some contacts as vault contacts and all messages to vault contacts and moved to the vault and can only be accessed using password.\nTimedSMS is available in the Android Market in Standard and Advanced version from Sep 10, 2010.\nTeam Members: 2 \nGoogle Play Link:\nhttps://play.google.com/store/apps/details?id=koeinnovations.timedSMSAdvanced&hl=en\nResponsibilities/Highlights:\n\n· Responsible for requirement analysis, design and development of all modules in this application.\n\n· Presented & demonstrated end-to-end product functionality.\n\n· VxRm for Android Platform\nDescription: VxRm is a popular application available for S60 and Windows Mobile devices which is being implemented for Android phone as part of this activity. The Android version has Phone call logging, meeting recording, voice recording and SMS backup options.\n\nThe meeting recording application features a proprietary audio buffering technology that buffers audio in a background audio buffer so that user can select the record key anytime and record the current discussion along with what has already been said earlier. \n\nAll recorded files are automatically created in different folders intuitively named with the meeting name, date/time of meeting, date/time of recording and date time of the call. If automatic backup option is set, the files are emailed and removed from the device storage.\nVxRm is available in the Android Market from Sep 7, 2010\nTeam Members: 2 \nGoogle Play Link:\n\nhttps://play.google.com/store/apps/details?id=koeinnovations.vxrmstandard&hl=en\nResponsibilities/Highlights:\n\n· Responsible for requirement analysis, design, development and debugging of all modules in this application.\n\n· Presented & demonstrated end-to-end product functionality.\n· Smartsacts for Android Platform\nDescription: Smartacts application makes it possible for an owner of Android devices share the devices with multiple users, users who are either too young to use a mobile phone or to be able to find their ways in using the phone. It also restrict the user as to who they can Call/SMS and also who can Call/SMS them when the devices is with them.\nWhile the initial version of Smartacts was as an end user application, subsequently a Smartacts server was implemented and the Smartacts Android device application was server enabled. In the server enabled version, the configured pictures contacts are automatically uploaded to the Smartacts server and separately for the different mobile users and they are downloaded for the respective user when the user logs in.\n\nAll activities including the phone calls made, received, SMS messages sent and received are forwarded to the server for archival and optionally they are emailed to the owner of the devices.   \nSmartacts is available in the Android Market from Sep 13, 2010.\nTeam Members: 2 \nGoogle play Link:\nhttps://play.google.com/store/apps/details?id=koeinnovations.smartactsstandard&hl=en\nResponsibilities/Highlights:\n\n· Responsible for requirement analysis, design, development and debugging of all modules in this application.\n\n· Presented & demonstrated end-to-end product functionality.\nAchievements\n· I was part of runner up team in the cricket tournament held in our college.\nPersonal Details\n\n\tName\n\tCHETHAN R Reddy\n\n\tDate of Birth\n\t12th May 1987\n\n\tSex\n\tMale\n\n\tMarital Status\n\tSingle\n\n\tContact Address\n\tS/o Ravindra Reddy M\n#1,Venkataswamy Patel Layout, Varthur\n\nBalagere Road,Bangalore-560087\n\n\tEmail Address\n\tchethanchiragreddy@gmail.com\nchethanchiragreddy@yahoo.com\n\n\tContact No\n\tMobile: +91-9886814537\n\nRes:     080-28538002","annotation":null,"extras":null,"metadata":{"first_done_at":1564046001000,"last_updated_at":1564046633000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "Nagendra                                                                                                        Contact No:9347595513\n                                                                                                                                                                Email: nagendravfp@gmail.com\n\t\n\n \t\t\t\t   \t                                      \n\n\tRole\nSFDC Developer\n\nKey Technical Skills & Knowledge\n· Salesforce.com\n(Configuration & ,Customization)\n· Force.com (Apex, Visual Force)\n· Lightning\n· SQL, SOQL.\n· Wave Analytics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\t\n\tProfile Summary\nNagendra, having overall 6+Years of experience in Software Development which includes 3+years in Oracle Apps Technical and 3+ years in Salesforce Configuration and Customization (Coding) experience. During this tenure I was involved in various stages of software development life cycle including – development, testing and deployment.\n\nSkill set Summary\n· Extensive experience in developing APEX classes, Triggers, Visual Force pages, Controllers, writing Workflows, Force.com API.\n· Hands on experience in querying salesforce.com database using SOQL & SOSL queries using Force.com Explorer.\n· Strong working experience configuration knowledge of Salesforce.com Application. \n· Proficiency in administrative tasks like creating Profiles, Roles, Users, Workflows & Approvals, Reports & Dashboards, Developed Formula fields, Validation rules.\n· Implemented security and sharing rules at object, field, and record level for different users at different levels of organization.\n· Extensive work experience in designing custom objects, custom fields, Page layouts, custom Tabs, custom settings, custom labels.\n· Experience in Data Migration using Import wizard, Apex Data Loader.\n· Sound knowledge in Communities, Salesforce to Salesforce, and Salesforce to Facebook.\n· Creativity and flexibility to face and resolve challenges.\n· Experience in Custom meta Datatype.\n· Experience on Salesforce to JIRA Integration and Salesforce to Survey Monkey\n\n\nTechnical Skills\n· Force.com: Salesforce Configuration, Apex including Triggers, Classes, and Visual Force Pages, Communities and Test classes, Lightning Components, bindings in Wave Analytics.\n· Database: SOSL, SOQL, SAQL.\n· Programming Language: Apex, Visual Force, Html, Css, Ajax.\n· Tools: Apex Data Loader, Eclipse.\n· Operating System: Windows Family.\nProfessional Experience:\n· Working as a Senior Developer in Salesforce Development in Apps Associates, Hyderabad from 2016 November to till now.\n\n· Worked as a Senior Software Engineer in Accenture from Aug 2014 to October 2016 (From August 2014 to October 2015 in Oracle Apps Technology and from November 2015 to October 2016 in Salesforce Development).\n· Worked as a Consultant in Oracle Apps in Genpact from August 2012 to August 2014.\n\nEducational Qualification\n\nM.Tech from JNTUH, Hyderabad \n\nCAREER PROFILE:\n\nPROJECT #4  Data Migrations and Support Project\nROLE: Salesforce Developer\nPROJECT: Taconic Biosciences\nCLIENT: Taconic Biosciences\nDURATION: October 2018 to till Date\nProject Summary: \nTaconic Biosciences is a breeder and supplier of laboratory animals like Rats and Mice for research purpose operating in over 50 companies.\n\nRoles and Responsibilities:\n\n· Daily monitoring of EBS-Mulesoft-Salesforce Sales orders sync issues\nand reprocessing of non-sync sales orders by Data loader process\n· Data migration from EBS to Sales force by Data loader for Items, Purchase orders.\n· Performing Support activities based on priority in JIRA Support Tool.\n· Preparing documents for Prod Migration.\n· Code, configure, unit test and create supporting documentation for application development tasks.\n· Developed various Custom Objects, Tabs, validation rules on the Fields and Visual Force Pages.\n· Customization that includes setting up Roles, Profiles, Sharing Rules\n\n· Preparing Minutes of Meeting Documents\n  \n\nPROJECT #3 Development Project\nROLE: Salesforce Developer.\nSOFTWARE: Salesforce.com\nPROJECT: EMPIRIX.\nCLIENT: EMPIRIX.\nDURATION: From June 2017 to September 2018.\n\nProject Summary:\nEmpirix is a network equipment manufacturing company. Which designs and manufactures service assurance testing and monitoring equipment for IP-based communications networks such as Voice-over-Internet-Protocol (VoIP), IP Multimedia Subsystem (IMS)-based, next generation network and 4G wireless networks. Empirix is headquartered in Billerica, MA\n\nAccountabilities: \n                           \n· Service requests include task related Enhancement requests for the existing functionality and also new implementation\n· On an overview the tasks include opportunities, cases, Approval process, Process Builder,email alerts, Communities, Lightning components, web-lead, web-case, Batch Apex, schedule Apex, User Time Zone conversion with triggers, Visualforce page, Global actions , Console Apps,\n· Worked on test class for Apexclass and triggers followed by best practice\n· Good Experience on changeset to deploy components from one org to another org\n· This also included moving code and configuration between development and QA/Testing sandboxes during the development/configuration/testing stages prior to moving to Production.\n\nPROJECT #2\nROLE: Salesforce Developer.\nSOFTWARE: Salesforce.com\nPROJECT: Health Care.\nCLIENT: Alnylam Pharmaceuticals.\nDURATION: From November 2016 to May 2017\n\nSynopsis: \n       Alnylam Pharmaceuticals is a biopharmaceutical company focused on the discovery, development of RNA interference (RNAi) therapeutics for genetically defined diseases.\n\nAccountabilities:\n· Worked with various salesforce.com Custom Objects and Standard Objects. \n· This also included moving code and configuration between development and QA/Testing sandboxes during the development/configuration/testing stages prior to moving to Production.\n· Good experience on Change set to deploy from One Org to another Org.\n· Developed SOQL queries for the application in Apex classes.\n· Workflow rules, Field updates, email alerts, filed updates to implement the business logic for Custom Objects as per the business needs.\n· Create, update and maintain Validation rules, Workflows.\n· Created Custom settings and Custom Labels as per the business needs.\n· Created Communities for the customer access from the salesforce.\n\nPROJECT#1 CISCO INTERNAL PROJECT\nROLE: Salesforce Developer.\nSOFTWARE: Salesforce.com\nPROJECT: XRM CTB\nCLIENT: CISCO XRM CTB.\nDURATION: November 2015 to October 2016  \n\t\n· Worked in Data Migration from Traditional Applications to Sales Force Using Data Loader Utility.\n·  Attending the daily status calls with Onsite and off-shore\n· Enhancement of the existing Reports\n· Development of Pre-Cutover Reports\n· Development of Post-Cutover Reports\n· Worked independently for the on time deliverables\n· Worked as a Single point of contact at off-shore for the project\n\n\n\n                                                                                          -Nagendra\n\n\n\n\n            Nagendra                                                                                                             e-Mail: nagendravfp@gmail.com","annotation":[{"label":["Email_Address"],"points":[{"start":7122,"end":7142,"text":"nagendravfp@gmail.com"}]},{"label":["Name"],"points":[{"start":6997,"end":7004,"text":"Nagendra"}]},{"label":["Name"],"points":[{"start":6972,"end":6979,"text":"Nagendra"}]},{"label":["Skills"],"points":[{"start":6386,"end":6399,"text":"Salesforce.com"}]},{"label":["Skills"],"points":[{"start":6257,"end":6267,"text":"Communities"}]},{"label":["Skills"],"points":[{"start":5964,"end":5967,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":5928,"end":5931,"text":"SOQL"}]},{"label":["Skills"],"points":[{"start":5275,"end":5288,"text":"Salesforce.com"}]},{"label":["Skills"],"points":[{"start":4917,"end":4920,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":4794,"end":4797,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":4779,"end":4782,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":4731,"end":4739,"text":"Lightning"}]},{"label":["Skills"],"points":[{"start":4718,"end":4728,"text":"Communities"}]},{"label":["Skills"],"points":[{"start":3986,"end":3999,"text":"Salesforce.com"}]},{"label":["Skills"],"points":[{"start":3779,"end":3796,"text":"Visual Force Pages"}]},{"label":["Degree"],"points":[{"start":2864,"end":2869,"text":"M.Tech"}]},{"label":["Tools"],"points":[{"start":2381,"end":2387,"text":"Windows"}]},{"label":["Tools"],"points":[{"start":2352,"end":2358,"text":"Eclipse"}]},{"label":["Tools"],"points":[{"start":2334,"end":2349,"text":"Apex Data Loader"}]},{"label":["Skills"],"points":[{"start":2319,"end":2322,"text":"Ajax"}]},{"label":["Skills"],"points":[{"start":2314,"end":2316,"text":"Css"}]},{"label":["Skills"],"points":[{"start":2308,"end":2311,"text":"Html"}]},{"label":["Skills"],"points":[{"start":2294,"end":2305,"text":"Visual Force"}]},{"label":["Skills"],"points":[{"start":2288,"end":2291,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":2258,"end":2261,"text":"SAQL"}]},{"label":["Skills"],"points":[{"start":2252,"end":2255,"text":"SOQL"}]},{"label":["Skills"],"points":[{"start":2246,"end":2249,"text":"SOSL"}]},{"label":["Skills"],"points":[{"start":2206,"end":2231,"text":"bindings in Wave Analytics"}]},{"label":["Skills"],"points":[{"start":2184,"end":2203,"text":"Lightning Components"}]},{"label":["Skills"],"points":[{"start":2170,"end":2181,"text":"Test classes"}]},{"label":["Skills"],"points":[{"start":2154,"end":2164,"text":"Communities"}]},{"label":["Skills"],"points":[{"start":2134,"end":2151,"text":"Visual Force Pages"}]},{"label":["Skills"],"points":[{"start":2121,"end":2127,"text":"Classes"}]},{"label":["Skills"],"points":[{"start":2096,"end":2118,"text":"Apex including Triggers"}]},{"label":["Skills"],"points":[{"start":2070,"end":2093,"text":"Salesforce Configuration"}]},{"label":["Skills"],"points":[{"start":2059,"end":2067,"text":"Force.com"}]},{"label":["Skills"],"points":[{"start":1793,"end":1803,"text":"Communities"}]},{"label":["Skills"],"points":[{"start":1753,"end":1769,"text":" Apex Data Loader"}]},{"label":["Skills"],"points":[{"start":1244,"end":1257,"text":"Salesforce.com"}]},{"label":["Skills"],"points":[{"start":1169,"end":1177,"text":"Force.com"}]},{"label":["Skills"],"points":[{"start":1150,"end":1153,"text":"SOSL"}]},{"label":["Skills"],"points":[{"start":1143,"end":1146,"text":"SOQL"}]},{"label":["Skills"],"points":[{"start":1064,"end":1072,"text":"Force.com"}]},{"label":["Skills"],"points":[{"start":1012,"end":1023,"text":"Visual Force"}]},{"label":["Skills"],"points":[{"start":732,"end":755,"text":"Salesforce Configuration"}]},{"label":["Years_of_Experience"],"points":[{"start":683,"end":689,"text":"3+years"}]},{"label":["Name"],"points":[{"start":597,"end":604,"text":"Nagendra"}]},{"label":["Skills"],"points":[{"start":540,"end":553,"text":"Wave Analytics"}]},{"label":["Skills"],"points":[{"start":532,"end":535,"text":"SOQL"}]},{"label":["Skills"],"points":[{"start":527,"end":529,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":515,"end":523,"text":"Lightning"}]},{"label":["Skills"],"points":[{"start":499,"end":510,"text":"Visual Force"}]},{"label":["Skills"],"points":[{"start":493,"end":496,"text":"Apex"}]},{"label":["Skills"],"points":[{"start":482,"end":490,"text":"Force.com"}]},{"label":["Skills"],"points":[{"start":432,"end":445,"text":"Salesforce.com"}]},{"label":["Email_Address"],"points":[{"start":301,"end":321,"text":"nagendravfp@gmail.com"}]},{"label":["Mobile_No"],"points":[{"start":123,"end":132,"text":"9347595513"}]},{"label":["Name"],"points":[{"start":0,"end":7,"text":"Nagendra"}]}],"extras":null,"metadata":{"first_done_at":1564230220000,"last_updated_at":1564230220000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
{"content": "Name : KPM Mounika\t\n\tData Analyst\nPhone : 9980333101\t\n\t\t\n\t\t\n\tEmail:\tkpmmounica@gmail.com\n\nBackground\nWorking as Senior Analyst in HCL technologies\nA technocrat with 3 years of experience in data analytics, Natural language processing, predictive modelling.\nI have completed my B.E from SRKR Engineering College with a percentage of 82.6%\nProfessional experience\n\nData Analyst for Deutsche bank client:\nWorked as data analyst for Deutsche bank advanced analytics project.\nUnderstanding the business problems.\nAnalysed large datasets and Data Cleaning.\nBuilt the models on the datasets and tested the performance of the models using Cross Validation.\nCommunicate the results of analyses, modelling to stakeholders.\nPerformed natural language processing on the credit card review data to find out whether the review given by the customers is positive or not.\nCalculated the optimum interest rate for the bank customers applying for loan.\nPerformed regression to suggest the amount of loan to be given the customers depending on their previous behaviour.\nPerformed classification on HR data to  estimate the attrition rate.\nPerformed classification on customers data to estimate whether  to approve their loan or not.\n\nSkills\nStatistical and Analytical Skills:\nWorked on Supervised Machine learning models like Linear regression, logistic regression, , Decision tree, Random forest, SVM, KNN using  ScikitLearn, Pandas, and Numpy packages in python.\nWorked on Unsupervised Machine learning models like KMeans, DBSCAN, Hierarchical clustering using ScikitLearn package in python.\nWorked on dimensionality reduction technique PCA.\nWorked on Natural language processing using NLTK package in python.\nWorked on time series using exponential smoothing and ARIMA.\nStrong understanding of machine learning concepts.\n\nTechnical Skills\nPython\nSQL\n\n\n\nData Analyst for Careoregon client:\nWorked as data analyst for Careoregon Analytics project.\nUnderstanding the business problems.\nAnalysed large datasets and Data Cleaning.\nBuilt the models on the datasets and tested the performance of the models using Cross Validation.\nCommunicate the results of analyses, modelling to stakeholders.\nWorked on the Insurance plan clustering according to the customers.\nPerformed classification to suggest which kind of customers to be targeted for contacting regarding the insurance plan.\nWorked on total claim forecasting for each month for a particular year.\nWorked on Customer Churn  to analyse which customers will cancel their contracts in near future.\n\n\n\n\n\n\n‹#›","annotation":[{"label":["Skills"],"points":[{"start":1830,"end":1832,"text":"SQL"}]},{"label":["Skills"],"points":[{"start":1823,"end":1828,"text":"Python"}]},{"label":["Skills"],"points":[{"start":1778,"end":1793,"text":"machine learning"}]},{"label":["Skills"],"points":[{"start":1747,"end":1751,"text":"ARIMA"}]},{"label":["Skills"],"points":[{"start":1685,"end":1690,"text":"python"}]},{"label":["Skills"],"points":[{"start":1669,"end":1673,"text":"NLTK "}]},{"label":["Skills"],"points":[{"start":1635,"end":1661,"text":"Natural language processing"}]},{"label":["Skills"],"points":[{"start":1585,"end":1623,"text":"dimensionality reduction technique PCA."}]},{"label":["Skills"],"points":[{"start":1567,"end":1572,"text":"python"}]},{"label":["Skills"],"points":[{"start":1544,"end":1554,"text":"ScikitLearn"}]},{"label":["Skills"],"points":[{"start":1514,"end":1536,"text":"Hierarchical clustering"}]},{"label":["Skills"],"points":[{"start":1506,"end":1511,"text":"DBSCAN"}]},{"label":["Skills"],"points":[{"start":1498,"end":1503,"text":"KMeans"}]},{"label":["Skills"],"points":[{"start":1456,"end":1484,"text":"Unsupervised Machine learning"}]},{"label":["Skills"],"points":[{"start":1438,"end":1443,"text":"python"}]},{"label":["Skills"],"points":[{"start":1420,"end":1424,"text":"Numpy"}]},{"label":["Skills"],"points":[{"start":1408,"end":1413,"text":"Pandas"}]},{"label":["Skills"],"points":[{"start":1395,"end":1405,"text":"ScikitLearn"}]},{"label":["Skills"],"points":[{"start":1384,"end":1386,"text":"KNN"}]},{"label":["Skills"],"points":[{"start":1379,"end":1381,"text":"SVM"}]},{"label":["Skills"],"points":[{"start":1364,"end":1376,"text":"Random forest"}]},{"label":["Skills"],"points":[{"start":1349,"end":1361,"text":"Decision tree"}]},{"label":["Skills"],"points":[{"start":1326,"end":1344,"text":"logistic regression"}]},{"label":["Skills"],"points":[{"start":1307,"end":1323,"text":"Linear regression"}]},{"label":["Skills"],"points":[{"start":1267,"end":1293,"text":"Supervised Machine learning"}]},{"label":["Degree"],"points":[{"start":277,"end":279,"text":"B.E"}]},{"label":["Skills"],"points":[{"start":206,"end":232,"text":"Natural language processing"}]},{"label":["Years_of_Experience"],"points":[{"start":165,"end":171,"text":"3 years"}]},{"label":["Email_Address"],"points":[{"start":68,"end":87,"text":"kpmmounica@gmail.com"}]},{"label":["Mobile_No"],"points":[{"start":42,"end":51,"text":"9980333101"}]},{"label":["Name"],"points":[{"start":7,"end":17,"text":"KPM Mounika"}]}],"extras":null,"metadata":{"first_done_at":1564130079000,"last_updated_at":1564130079000,"sec_taken":0,"last_updated_by":"NACC1DG9AJh3Aj2aOEp6wp7ldPi2","status":"done","evaluation":"NONE"}}
