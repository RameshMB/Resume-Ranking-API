{"text": "DEBJYOTI\n        \t   Mobile: +91 - 8050740371\t \n            \tE-mail:d.hota84@outlook.com\n\n      \nDATA WAREHOUSE SPECIALIST\n\nOrchestrating technology change management initiatives to drive cost savings and ROI while                                     expanding infrastructure capacity and performance.\n\n\n       EXPERIENCE SUMMARY\n\nSeasoned data warehouse/Cloud specialist and business analyst around 9 years of data management experience seeking a challenging position at a company that uses analytics to drive strategy.  Specializing in rapid deployment of Data Warehouse solutions, I have a track record of delivering data management solutions on time and under budget, providing significant documentable ROI.  Experienced with full-lifecycle development including complex SQL development, OLAP development, DBA duties, ETL coding, requirements documentation, data modeling, and report writing in a cloud environment.  Reputation for high-quality work, clever solutions, and a commitment to user satisfaction. Core competencies include:\n\n\t* Business analysis and requirement gathering\n* Project planning\n* System Configuration                                        \n* Business intelligence solution architecture\n* Data warehouse design                                    \n* ETL design\n* ETL process development                                 \n* OLAP design\n* Data cleaning\n\t* Data integration from multiple sources & confirmation\n* Data validation                                                \n* Performance tuning\n* Report development                                         \n* Dashboard design\n* Security plan design                                         \n* Data warehouse maintenance\n* Ongoing user support and education\n* Cloud\n\n\n                                              \n\n\u00b7 Having around 8.8+ years of IT experience, in various data warehousing tools like Informatica, ICS (Informatica Cloud Services), Mercator (Data stage TX), and OBIEE reports testing.\n\u00b7 Worked on all phases of data warehouse development lifecycle, from gathering requirements to testing, implementation, upgrade and support (In cloud)002E\n\u00b7 Worked on various source systems like Oracle, Siebel, SAP, and MS SQL Server, DB2 etc.\n\u00b7 Having 5 lifecycle Implementations, 1 Upgrade and 2 supporting (including testing) project knowledge with Good analysis, design, development, customization, upgradation and implementation and testing knowledge.\n\u00b7 Good experience on Data Mapping, Transformation and Loading in complex, high-volume job environment using Informatica (including MDM and IDQ), Informatica cloud, Data stage TX, Mercator and super tools like HP3000 Emulator.\n\u00b7 Possesses extensive programming skills in ETL and EDI. \n\u00b7 Good knowledge on OBIEE and MicroStrategy.\n\u00b7 Excellent technical and analytical skills with clear understanding of design goals of ER modeling for OLTP and dimension modeling for OLAP.\n\u00b7 Excellent skills in Oracle 9i/10g, SQL, PL/SQL.\n\u00b7 A self-motivated team player with Excellent interpersonal and communication skills. Experienced in working with senior level managers, business people and developers across multiple disciplines.\n\n\n\nEDUCATION\n\n                 Master\u2019s in computer application (M.C.A.) Degree (2008)\nSilicon Institute of Technology\n                 ***Completed with recognition***\n \n    TECHNICAL PROFILE\n\n     \nETL/EDI tools              :      Informatica 8.6/9.1, Mercator (Data stage TX), HP 3000   Emulator\nReporting tool            :     Business Objects, OBIEE\nLanguages\t\t     :     SQL, PL/SQL, C, C++, Java, J2EE\nDatabase\t\t     :     Oracle 9i, SQL Server 2005\nThird Party Tools       :     TOAD, WINSCP, putty, SQL developer, MS SQL Server\nOperating System\t     :     Windows, UNIX, Linux \n\n \n\n     CERTIFICATIONS\n\n\u00b7 Oracle Certified Associate (Oracle 9i)\n\u00b7 LOMA 280(Insurance Domain certification, US)\n\n \nJOB PROFILE\n\u00b7 Working as a senior analyst for Accenture, Bangalore from August\u20192014 to till date.\n\u00b7 Engaged with IGATE, Bangalore as a Technical Lead from July\u20192013 to August\u20192014.\n\u00b7 Engaged with L & T INFOTECH, Chennai/Bangalore as an Informatica Analyst from May\u20192010 to July\u2019 2013.\n\n\n                                 PROFESSIONAL EXPERIENCE\n\n------------------------------------------------------------------------------------\nAccenture\u2013 Bangalore, INDIA                                                                2014 to Present\nAccenture is a multinational management consulting, technology services and outsourcing company. Its incorporated headquarters are in\u00a0Dublin,\u00a0Ireland\u00a0since September 1, 2009. It is the world's largest\u00a0consulting firm\u00a0as measured by revenues\u00a0and is a\u00a0Fortune Global 500\u00a0company.As of 2014, the company reported net revenues of $30.0 billionwith approximately 319,000 employees, serving clients in more than 200 cities in 56 countries.\u00a0Accenture has more employees in\u00a0India\u00a0than in any other country; in the\u00a0US, it has about 40,000 employees and 35,000 located in the\u00a0Philippines.[Accenture's current clients include 89 of the Fortune Global 100 and more than three-quarters of the Fortune Global 500.\n\n\nProject #7: ECOLABS, USA\n\n        Client \t                   : ECOLABS,USA\n        Environment             : ICS (Informatica cloud service), UNIX, Oracle, SQL Server \n 2005, Microstrategy\n        Tools                        :  Oracle SQL Developer, \u00a0Service Now, JIRA,\u00a0Uc4\n        Role                          : Lead\n        Team Size                 :  15\n        Time period\t         :  MAY\u201918 to till date.\n        Work Location            :  Offshore\n\n\n\nProject Description\n              Ecolab Inc., headquartered in\u00a0St. Paul, Minnesota, is an American global provider of water, hygiene and energy technologies and services to the food, energy, healthcare, industrial and hospitality markets. The company's food safety services provide consulting to restaurants, hospitals, food retailers and food & beverage manufacturing facilities. It is also a supplier of chemicals used by beef and poultry processors - to reduce pathogens, such as E. coli and salmonella - in uncooked beef and poultry. It was founded as\u00a0Economics Laboratory\u00a0in 1923 by Merritt J. Osborn and renamed \"Ecolab\" in 1986.\n\nResponsibilities:\n\n\u00b7 Supported integrations for Informatica PowerCenter and Informatica cloud services.\n\u00b7 Responsible for Change, Incident, Problem and Release management.\n\u00b7 Working in an agile environment for each phase like analyzing, designing, coding, testing, support and documentation.\n\u00b7 Worked closely with Architects when designing the Data Marts and Data Warehouses, with Business Analyst in understanding the business needs and interacting with other team members in completing the task as scheduled.\n\u00b7 Identifying risks proactively and proposing solutions to resolve them.\n\u00b7 Interact with Business Analysts for reporting requirements to define business and functional specifications.\n\u00b7 Involved in ETL process from development to testing and production environments.\n\u00b7 Worked on Informatica Source Analyzer (Analyzing Source System data), Warehouse Designer, Mapping Designer, Mapplet and Transformations.\n\u00b7 Used most of the Transformations such as Source Qualifier, Aggregator, Lookup, Filter, Sequence generator, Router, Update strategy etc.,\n-----------------------------------------------------------\n\n\n\nProject #7: Disney, USA\n\n        Client \t                   : DISNEY,USA\n        Environment             : \u00a0Informatica , UNIX, Oracle, SQL Server 2005,Microstrategy\n        Tools                        :  Oracle SQL Developer\n        Role                          : Senior Tech Architect\n        Team Size                 :  15\n        Time period\t         :  DEC\u201915 to till date.\n        Work Location            :  Offshore\n\n\n\nProject Description\n              The Walt Disney company commonly known as Disney\u00a0\u00a0is an American  diversified\u00a0multinational\u00a0mass media\u00a0and entertainment\u00a0conglomerate, headquartered at the\u00a0Walt Disney Studios\u00a0in\u00a0Burbank, California. It is the world's second-largest media conglomerate in terms of revenue, after\u00a0Comcast The Project requirement is to build a Data warehouse to integrate different source applications so as to pull data for a set of report and dashboard. As a part of the long term solution the requirement is to understand the user requirements, source systems, gain access to source systems, design and develop data warehouse tables, develop reporting tables and maintain historical data. \n\nResponsibilities:\n\n\u00b7 Implemented ETL solution using Informatica.\n\u00b7 Analyzing Source System data.\n\u00b7 Worked on Informatica \u2013 Source Analyzer, Warehouse Designer, Mapping Designer, Mapplet and Transformations.\n\u00b7 Involved in importing Source/Target Tables from the respective databases.\n\u00b7 Developed data Mappings between source systems and warehouse components using Mapping Designer to resolve the code issue.\n\u00b7 Used most of the Transformations such as Source Qualifier, Aggregator, Lookup, Filter, Sequence generator, Router, Update strategy etc.,\n\u00b7 Extensively used ETL to load data from Oracle and Flat files to Data Warehouse. \n\u00b7 Analysis of complex issue by backtracking the code and providing the solution to resolve the issue .\n\n-----------------------------------------------------------\n\n\n\n\n\nProject #6: KPN, nETHERLANDS\n\n        Client \t                   : KPN, NETHERLANDS\n        Environment             : \u00a0Informatica , UNIX, Oracle, SQL Server 2005\n        Tools                        :  Oracle SQL Developer\n        Role                          : Senior Analyst\n        Team Size                 :  5\n        Time period\t         :  AUGl\u201914 to NOV\u201915.\n        Work Location            :  Offshore\n\n\n\nProject Description:\n        KPN is a Dutch is a landline and mobile telecommunications company. In the Netherlands, KPN has 6.3 million fixed-line telephone customers. Its mobile division, KPN Mobile, has more than 33 million subscribers in the Netherlands, Germany, Belgium, France, and Spain under different brand names. Through its ownership of several European\u00a0Internet service providers, KPN also provides Internet access to 2.1 million customers, and it offers business network services and data transport throughout\u00a0Western Europe.The Project requirement is to build a Data warehouse to integrate different source applications so as to pull data for a set of report and dashboard. As a part of the long term solution the requirement is to understand the user requirements, source systems, gain access to source systems, design and develop data warehouse tables, develop reporting tables and maintain historical data. \n\nResponsibilities:\n\n\u00b7 Implemented ETL solution using Informatica.\n\u00b7 Analyzing Source System data.\n\u00b7 Worked on Informatica \u2013 Source Analyzer, Warehouse Designer, Mapping Designer, Mapplet and Transformations.\n\u00b7 Involved in importing Source/Target Tables from the respective databases.\n\u00b7 Developed data Mappings between source systems and warehouse components using Mapping Designer\n\u00b7 Used most of the Transformations such as Source Qualifier, Aggregator, Lookup, Filter, Sequence generator, Router, Update strategy etc.,\n\u00b7 Extensively used ETL to load data from Oracle and Flat files to Data Warehouse. \n\u00b7 Used Informatica Workflow Manager to create Sessions, Workflows to run with the logic embedded in the mappings.\n\n-----------------------------------------------------------\n\n\n\n\n\n\niGATE Corp \u2013 Bangalore, INDIA                                                                2013 to Present\n\n       iGATE Corporation (IGTE:US) is a NASDAQ-listed, US-incorporated outsourcing company. iGate Inc is an American multinational corporation which provides information technology, consulting and business process outsourcing (BPO) services. It is headquartered in Fremont, California, USA. Customers include companies in banking & financial services; insurance & healthcare; life sciences; manufacturing, retail, distribution & logistics; media, entertainment, leisure & travel; communication, energy & utilities; federal government; and independent software vendors across the Americas, Europe- Middle East-Africa (EMEA) and Asia-Pacific.\n-------------------------------------------------------------------\n\n\nProject #5: GE, us \n\n        Client \t                   : GE, US\n        Environment             : \u00a0Informatica Power center 9.1, UNIX, Oracle, SQL Server 2005, \n        Tools                        :  Oracle SQL Developer\n        Role                          : Technical Lead\n        Team Size                 :  7\n        Time period\t         :  Jul\u201913 to Jul\u201914.\n        Work Location            :  Offshore\n\n\n\nProject Description:\n        The Project requirement is to build a Data warehouse to integrate different source applications so as to pull data for a set of metrics. This data will be used to generate reports so as to digitize the current Ops metrics which is currently done manually. As a part of the long term solution the requirement is to understand the user requirements, source systems, gain access to source systems, design and develop data warehouse tables, develop reporting tables and maintain historical data. \n\nResponsibilities:\n\n\u00b7 Debugging and fixing the code bugs of Informatica mapping.\n\u00b7 Involvement in design with onsite POC\u00a0development.\n\u00b7 Worked extensively on Oracle and SQL Server database\u00a0and flat files as data sources.\n\u00b7 Making changes in mapping as per the requirement and then did unit testing.\n\u00b7 Managed the entry in transport table for new interface using aqua data for moving data from one location to other.\n\u00b7 Testing \u2013 Unit, Preparing the UTC, UTR and process related documents.\n\u00b7 Sharing business knowledge to team and sharing best practices.\n\u00b7 Worked on various issues on existing Informatica Mappings to produce correct output.\n\u00b7 Developed Informatica objects - Mappings, sessions, workflows based on the prepared low level design documents.\n\n\n\n-------------------------------------------------------------------\nL&T InfoTech \u2013 Bangalore, INDIA                                                                 2010 to 2103\n\n\tL&T InfoTech is one of the biggest IT services and consulting firm in India. It has its reach in over more than 36 countries. It specializes in software services and consulting, engineering, construction, manufacturing goods and financial services.\n-------------------------------------------------------------------\n\n\n\nProject #4: Paramount, us \n\n        Client \t                   : Paramount, US\n        Environment             : \u00a0Informatica Power center 8.6.1, UNIX, Oracle, SQL Server 2005\n        Role                         : Informatica Analyst \n        Tools                        :  JIRA, Confluence, VL Trader, AQUA DATA\n        Team Size                 :  4\n        Time period\t         :  Aug\u201912 to JUN\u201913.\n        Work Location           :  Offshore\n\n\n\nProject Description:\n        The oldest film studio and an American film production and Distribution Company. This project includes development and enhancement of numerous components to automate client\u2019s business data processing and integration. There are various interfaces like TAP, RAFT, TAP web services, claim etc.\n\n\nResponsibilities:\n\n\u00b7 Understanding and exploring Business requirement and developing the\u00a0Confluence page for the same and get the approval by client for freezing\u00a0\u00a0\u00a0\u00a0\u00a0 the requirement.\n\u00b7 Debugging and fixing the code bugs of Informatica mapping.\n\u00b7 Involvement in design with onsite POC\u00a0development.\n\u00b7 Worked extensively on SQL Server database\u00a0and flat files as data sources.\n\u00b7 Making changes in mapping as per the requirement and then did unit testing.\n\u00b7 Developed confluence pages with respect to each interface.\n\u00b7 Managed the entry in transport table for new interface using aqua data for moving data from one location to other.\n\u00b7 Developed script in VL trader.\n\u00b7 Testing \u2013 Unit, Preparing the UTC, UTR and process related documents.\n\u00b7 Sharing business knowledge to team and sharing best practices.\n\u00b7 Worked on various issues on existing Informatica Mappings to produce correct output.\n\u00b7 Developed Informatica objects - Mappings, sessions, workflows based on the prepared low level design documents.\n\n\n\n\n\n\nProject #3: Coloplast, UK\n\n               Client \t                   : Coloplast, UK\n        Environment             : Informatica, Siebel, OBIEE, SQL server 2008, DAC\n        Role                         : Informatica Consultant\n        Team Size                 :  3\n        Time period\t         :  Jan\u201912 to Aug\u201912.\n        Work Location           : Offshore\n\n\nProject Description:  \n               Coloplast in the UK has been using Siebel on Premises Life Sciences industry vertical and Siebel Analytics for more than 6 years and has been one of the early Oracle customers embracing Siebel as its CRM solution. Siebel CRM application in the UK organization, caters to 80000+ customers, churns over 3000+ sales orders per day, integrates with JDE ERP for order fulfillment and is a vital stakeholder in serving the customers in true sense. Coloplast uses Siebel analytics instance for marketing analytics and has been using hundreds of custom reports and dashboards. The Siebel CRM and Siebel Analytics instances have been running on very old versions and hence qualify for an upgrade. Coloplast believes that the Siebel CRM and Siebel analytics are very critical to their UK business and has decided to upgrade these applications to the latest releases. Coloplast is also planning to upgrade the hardware along with the application upgrade. This upgrade is a Like for like upgrade, which means Coloplast would expect the upgraded applications with same functionalities and reports/ dashboards as they are at present.\n\n\n\n\nResponsibilities:\n\n\u00b7 Played a critical role in upgrading the environment (Informatica 7.0 to 9.1). \n\u00b7 Involved in Retro fix of various mappings with the same functionalities as they were present in old environment.\n\u00b7 Testing \u2013 Unit, Preparing the UTC, UTR and process related documents.\n\u00b7 Implemented ETL solution using Informatica.\n\u00b7 Analyzing Source System data.\n\u00b7 Worked on Informatica \u2013 Source Analyzer, Warehouse Designer, Mapping Designer, Mapplet and Transformations.\n\u00b7 Sharing business knowledge to team and sharing best practices.\n\n\n\n\n\n\n\nProject #2: MMC Global Technology Infrastructure, USA\n\n\n          Client \t         : MMC Global Technology Infrastructure, USA\n        Environment   : Oracle10g, Informatica 8.6, UNIX \n        Tool               : putty, WINSCP, SQL Developer\n        Role               : Informatica Consultant\n        Team Size       :  4\n        Time period     : Aug\u201911 to Dec\u201911        \n        Work Location  : Offshore\n\n\nProject Description:\n\n              Marsh & McLennan Companies, Inc. (MMC) is a US-based global professional service and insurance brokerage firm.MMC is a diversified risk, insurance and professional services firm which is the world\u2019s leading broker and risk adviser. The ultimate aim of the project is used in the  Revenue Management and Billing (RM&B) Business Intelligence (BI) .This describes the data stores, source data flows and target reporting & warehouse Tiers using diagrams required to provide the reporting capabilities needed for the US BASYS replacement.  The intention is to provide the basis for a global reporting platform; however this initial phase intends to focus on the US requirements.\n\n\n\nResponsibilities:\n\n\u00b7 Implemented ETL solution using Informatica.\n\u00b7 Analyzing Source System data.\n\u00b7 Worked on Informatica \u2013 Source Analyzer, Warehouse Designer, Mapping Designer, Mapplet and Transformations.\n\u00b7 Involved in importing Source/Target Tables from the respective databases.\n\u00b7 Developed data Mappings between source systems and warehouse components using Mapping Designer\n\u00b7 Used most of the Transformations such as Source Qualifier, Aggregator, Lookup, Filter, Sequence generator, Router, Update strategy etc.,\n\u00b7 Extensively used ETL to load data from Oracle and Flat files to Data Warehouse. \n\u00b7 Used Informatica Workflow Manager to create Sessions, Workflows to run with the logic embedded in the mappings.\n\n\n\n\nProject #1: MMC Global Technology Infrastructure, USA\n\n\nClient                           : MMC Global Technology Infrastructure, USA\nEnvironment                 : Informatica 8.5, Mercator (Data Stage TX), putty, FTP client, UNIX,\nTool                             : putty, WINSCP, SQL Developer, Sup tool (HP3000 Emulator)\nRole                             : EDI Analyst                            \nTeam Size\t               :  2\nTime period                  : May\u201910 to Aug\u201911           \nWork Location               : Offshore\n\n\nProject Description: \n\n              Marsh & McLennan Companies, Inc. (MMC) is a US-based global professional service and insurance brokerage firm.MMC is a diversified risk, insurance and professional services firm which is the world\u2019s leading broker and risk adviser. The ultimate aim of the project is used as a data validation and conversion package for many of the above mentioned file types that is being sent and received to or from various internal systems.\u00a0 It allows legacy systems to create or accept one file layout because it can accept basically any layout and convert it to the layout that either the legacy or client system is expecting.\u00a0 The system is a control file driven system.\n\nResponsibilities:  \n\n\n\u00b7 Analysis of business requirements.\n\u00b7 Production support (40%), enhancement (60%).\n\u00b7 Working on issues, enhancements, defects and Change Requests.\n\u00b7 Involved in data validation and conversion package for many of the file types that is being sent & received.\n\u00b7 Responsible for conversion and validation of data and for applying the business rule in order to obtain the required data from input file.\n\u00b7 Used most of the Transformations such as Source Qualifier, Aggregator, Lookup, Filter, Sequence generator, Router, Update strategy etc.,\n\u00b7 Extensively used ETL to load data from Oracle and Flat files to Data Warehouse. \n\u00b7 Used Informatica Workflow Manager to create Sessions, Workflows to run with the logic embedded in the mappings.\n\n\n\n\nKey Achievements:\n\n\u00b7 Established a positive and productive workplace by personally training, supporting and interacting with employees and\u00a0managers at all levels of the organization.\n\u00b7 Managed significant change and improvement initiatives.\n\u00b7 Led help desk and support functions for 650 users in global locations; ensured immediate diagnosis and resolution of complex\u00a0issues and updates.\n\u00b7 Identified and capitalized on cost reductions by merging team functions, training employees and updating or transitioning to more cost-effective systems (such as involving automated systems).\n\nOther Achievements & Awards:\n\n\u00b7 Awarded best Employee in 2012 for Coloplast.", "entities": [[20366, 20369, "Skills", "SQL"], [20358, 20364, "Tools", "WINSCP"], [20351, 20356, "Tools", "putty"], [20310, 20315, "Operating_Systems", "UNIX,"], [20291, 20296, "Tools", "putty"], [18470, 18473, "Skills", "SQL"], [18462, 18468, "Tools", "WINSCP"], [18455, 18460, "Tools", "putty"], [18420, 18424, "Skills", "UNIX"], [16315, 16318, "Skills", "SQL"], [16308, 16313, "Tools", "OBIEE"], [15483, 15493, "Skills", "SQL Server"], [14548, 14558, "Skills", "SQL Server"], [14534, 14539, "Operating_Systems", "UNIX,"], [13301, 13311, "Skills", "SQL Server"], [12404, 12407, "Skills", "SQL"], [12339, 12349, "Skills", "SQL Server"], [12325, 12330, "Operating_Systems", "UNIX,"], [9457, 9460, "Skills", "SQL"], [9394, 9404, "Skills", "SQL Server"], [9380, 9385, "Operating_Systems", "UNIX,"], [7523, 7526, "Skills", "SQL"], [3805, 3849, "Certifications", "LOMA 280(Insurance Domain certification, US)"], [3764, 3802, "Certifications", "Oracle Certified Associate (Oracle 9i)"], [3730, 3735, "Operating_Systems", "Linux"], [3724, 3728, "Operating_Systems", "UNIX"], [3715, 3722, "Operating_Systems", "Windows"], [3673, 3686, "Tools", "MS SQL Server"], [3658, 3671, "Tools", "SQL developer"], [3651, 3656, "Tools", "putty"], [3643, 3649, "Tools", "WINSCP"], [3637, 3641, "Tools", "TOAD"], [3591, 3601, "Skills", "SQL Server"], [3580, 3589, "Skills", "Oracle 9i"], [3554, 3558, "Skills", "J2EE"], [3548, 3552, "Skills", "Java"], [3543, 3546, "Skills", "C++"], [3540, 3541, "Skills", "C"], [3532, 3538, "Skills", "PL/SQL"], [3527, 3530, "Skills", "SQL"], [3499, 3504, "Tools", "OBIEE"], [3481, 3497, "Tools", "Business Objects"], [3430, 3448, "Tools", "HP 3000   Emulator"], [3404, 3428, "Tools", "Mercator (Data stage TX)"], [3383, 3402, "Tools", "Informatica 8.6/9.1"], [3180, 3221, "Degree", "Master\u2019s in computer application (M.C.A.)"], [2944, 2950, "Skills", "PL/SQL"], [2939, 2942, "Skills", "SQL"], [2924, 2933, "Skills", "Oracle 9i"], [2735, 2740, "Tools", "OBIEE"], [2194, 2207, "Tools", "MS SQL Server"], [1951, 1956, "Tools", "OBIEE"], [1921, 1945, "Tools", "Mercator (Data stage TX)"], [775, 778, "Skills", "SQL"], [400, 407, "Years_of_Experience", "9 years"], [68, 88, "Email_Address", "d.hota84@outlook.com"], [29, 45, "Mobile_No", "+91 - 8050740371"], [0, 8, "Name", "DEBJYOTI"]]}