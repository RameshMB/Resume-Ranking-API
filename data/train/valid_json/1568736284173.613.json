{"text": "CURRICULUM VITAE\nJyoti Ranjan Mohanty                                             \nSenior Consultant at Oracle SSI                                       \n\n\n\t   Professional Summary\n\t\n\n\t\u00b7 Currently working with Oracle Solutions Services India as a Senior Consultant.\n\u00b7 Having 7 Years of Experience in top IT MNCs on various projects Development, Enhancement,POC and Customization projects .\n\u00b7 Having  3 years of relevant    experience on  below aspects of Hadoop ecosystem - Spark (Spark-SQL, Pyspark) , HDFS, Hive, Pig, Sqoop,Oracle  big  data cloud  service(Cloudera), AWS EMR \u2013 (S3,Redshift) environment, Python .\n\n\u00b7 Having 4 years of IT experience in other areas like Java, SQL, PL/SQL, Web services (REST and SOAP) , Oracle cloud products and services etc.\n\u00b7 Experience in data analysis, data model design, web application development, testing,  deployment etc..Understanding, testing technical and functional issues, debugging and solving them.\n\n\u00b7 Have very good understanding of software development life cycles along with Agile methodology.\n\u00b7 Have used operating systems like Windows, Linux and Unix.\n\n\u00b7 Very good in consulting, onsite offshore interaction, customer interaction, bidding/estimation, leading team, presentation and interest to travel. Was involved in recruitment activities.\n\n\u00b7 Good interpersonal and communication skills, commitment, result oriented professional.\n\n\u00b7 Hardworking with a quest and zeal to learn new technologies and undertake challenging tasks.\n\n\n\t  Technical Skills\n\t\n\n\tFramework/Languages    :      Spark 2.2 (Pyspark,Spark-SQL), Hive, Pig, Sqoop,Oozie,\n                                                   Python,HDFS File system,S3 File System, AWS-Redshift,     \n\n                                                   Java, SQL/PLSQL, MySQL,Web-services, Java script\n\nDatabases\n\n     :      MySQL, Oracle, DBCS, SQL sever\n\nTools\n\n\n     :      GIT, SVN, SSH Client, Putty,Soap UI, SQL Developer, PVCS,QC\n\n                                                    ,  \n\nGUI Tools\n\n     :      Pycharm ,Jupyter notebook, JDeveloper ,Eclipse ,HUE\n\nCloud technologies           :      Oracle cloud services, AWS\n\nServers                               :      Weblogic server 11g and 12c, JCS , SOA server, Tomcat\n\n\n\n\n\t  Professional  Experience     \n\t\n\n\t\u00b7 Currently working as Senior Consultant in Oracle SSI since May 2016. Currently working as a Spark developer.\n\n\u00b7 Worked as ADF and SOA Technical Consultant in Tech Mahindra since March  2015 to May 2016.\n\n\u00b7 Worked as ADF Technical Consultant for IGATE Global Solutions since July 2014 to March 2015.\n\n\u00b7 Worked as ADF & OAF Technical Consultant for Wipro Technologies from February 2012 to July 2014.\n\n\n\tEducation\n\t\u00b7 BE in Computer science from Sapthagiri college of Engineering, Anna University Coimbatore  with 80.0%\n\u00b7 Intermediate from CHSE, Orissa with 75.6%\n\u00b7 10th from HSC, Orissa with 76.5%\n\n\n\n1.  World Health Organization(WHO)\nDescription:  \n\nWorld Health Organization is an UN health care organization which is solely responsible for standardizing healthcare across world and helping the countries in overcoming difficult situations like disease outbreaks. \nTeam Size: 12\nRole:\nWorked as a senior consultant from start to end of the project. Analysis of Requirement specification, Preparing Technical Design documentation. Was actively involved in customer interaction for clarification, presentation/demo and project estimation. Got spot awards recognition for quality project delivery.\n\nProject details:\n\nThe IT entity is focused on building out the cloud-based architecture (Amazon S3, Redshift supported via EMR/Spark) and populating the S3 data lake landing and staging environments with required information housed together.Each of the campaign data included various kinds of data e.g. campaign location info , diesease outbreaks info, symptoms, number of deaths, doctors/staff applied for position, interviewed, employed ,campaign cost involved, effectiveness metric etc.\n S3 Object Storage to provide the underlying data lake enablement. It will  include three architecture hops: Landing, Staging and Warehouse.\n\nLanding Zone \u2013 Data is mainly consumed from different sources (HR, Payroll, Taleo, Custom application DB)  as a CSV files in S3 location.\n\nStaging Zone \u2013 The preprocessor job used for data analyzing and cleansing based on S2T doc from Landing Zone to Staging.\n\nWarehouse \u2013 Dimension and Fact structure was created on S3 bucket after applying transformations on top of staging data. Applying CDC logic to find insert, update, delete and unchanged record, distributed across History, Current Backup and Current dimension.\n\nRedshift \u2013 Tables will be created on top of S3 History dimension for the end system.\n\nSkills: Spark SQL, Pyspark, Python\n\nEnvironment: AWS EMR, S3 and Redshift\n2.  eHealth NSWH, Sydney\nDescription:  \n\neHealth is the IT division of NSW health, which is a government organization present in Sydney, NSW. \nTeam Size: 10\nRole:\nCurrently working as a spark developer. Was actively involved in customer interaction for clarification, presentation/demo, providing training, supporting the onsite team on issues and project estimation. \n\nProject details:\nCloud data lake is maintained in Hive Database and downstream applications such as PI, PING consume data in different formats like JSON, structured data etc. as per requirement. The main aim of project is to setup a generalized framework and act as an anchor to ingest data from multiple sources and provide insights of commercial value to the users.\nData Ingestion \u2013 Data is mainly consumed from different sources (Oracle RDBMS and different entities , Third party vendors) as a flat files moved from edge node to hdfs.\n\nRaw Layer \u2013 The external table is created on top of flat files without any normalization.The managed table is created from the external table by applying proper metadata based on the mapping document.\n\nCurated Layer \u2013 On top of the raw layer, we will introduce a data quality rule framework layer which will check data quality for rules such as null check, duplicate check, count check. special character check etc. by reading data in external data layer, fed from the external sources as it is, and applying the rules on it. The records passing the screening will be ingested into the system data lake for further processing.\n\nSDS \u2013 In Service Data Store, the JSON files (API: Member, Benefits, Claims) will be generated from the curated layer based on the member and their eligibility coverage which will be sent to end system. SDS automation script helps in testing each object from a JSON file and produce test report. Using Python script, it automatically does count check and data comparison check for each object. Mapping Dictionary checks makes sure the attribute naming matched the mapping document and API swagger.\n\nRejected Data - The data rejected in data quality framework will be sent to the user with rejection reasons for correction. Also, an audit will be maintained to track the number of records passing or failing from it.\n\nSkills: Hive, Spark SQL, Pyspark, Shell\n\nEnvironment: Cloudera (Oracle big data cloud service)\n\n3.  Metro Cash And Carry, Russia\nDescription:  \nMetro Cash And Carry is a whole sale organization and for recruitment and managing their IT operation they use various oracle cloud products. \nTeam Size: 3\nRole\nWorked as a hadoop developer for various data transformation and analysis.\n\nProject details:\nObjective: It is basically an Ab Inisio ETL tool to hadoop migration project. All information with respect to billing, asset,sales,revenue, address,footfalls, products and site related information was extracted from different source systems and fed to CRM systems to increase customer satisfaction, Customer Order placing and Customer Order Management. This also enabled Metro to get insights from data applying many machine learning algorithms. Followed by the analysis many decisions were taken like based on purchases made providing offers, better membership cards ,notifications for product offer  to good customers etc\nData Ingestion Framework- This framework would also take care of putting data from RDBMS system to HDFS filestystem using sqoop . Then hive tables were created for data analysis for end systems which uses hadoop. At the same time the processed data after PIG ETL process was also exported back to oracle RDBMS .These data are specifically used by end systems which still uses oracle RDBMS.\nSkills: Hive, Apache Pig, Oozie, Sqoop, Shell\n\nEnvironment: Cloudera (Oracle Big Data Cloud Service)\n4.  Biassyala / External Assignment Management System(Swedbank)\nDescription: \n\nBiassyala is a PaaS extension hosted on JCS (java cloud services) provides the solution for automated Request generation and approval process. Swedbank had paper based approval system which was replaced by our system. \nTeam Size: 3\nRole:\n\n        Worked on Developing Dynamic Request generation and document approval. Historical Data load, Scheduled processes. Done the built and Deployment of application in the development environments during each release. Fixed many critical issues, when asked to be deployed in the short period of time. Used technologies like Fusion HCM, Web Proxies, JCS, ADF 12c, ADF-BC, ADF Rich Faces, Task flows, Data Control, Java, JSF, Oracle12c.\n5.  Equity bank, Kenya\n\nDescription:  \n\nEquity bank is a big bank in Kenya. I was responsible for developing banking application using BPM, ADF etc. I was involved in creating custom ADF screens which were used in Human tasks in BPM approval process.\nTeam Size: 15\nRole\nWorked as a senior consultant in designing, analysis of requirement specification, preparing technical design documentation, development of UIs in ADF. Basically, I used BPM data controls as backend and SOAP web services are used for some data retrieval\n\n6.   Accounting and Corporate Regulatory Authority(ACRA), Singapore\n Description:\n\n          The Accounting and Corporate Regulatory Authority (ACRA) is the national regulator of business entities and public accountants in Singapore. ACRA also plays the role of a facilitator for the development of business entities and the public accountancy profession.\nTeam Size: 12\n\nRole:\nWorked as a java and ADF Developer. Involved in fresh development and Defect fixing activities. Responsible for quality deliverables.\n\n7.  Cisco(Cisco_CVCM_QTC_CCW_ASE_OM), California\n\nDescription:\n\n     In cisco I was working as a SOA and BAM developer. Used technologies like SOA 11g ,12c and BAM 11g and 12c. SOA was used to integrate e-business suite order management system and push data into BAM objects. I was also responsible for creating BAM data objects, real time Reports, Alerts, export import BAM objects etc.\nTeam Size: 4\nRole:\nWorked as a middleware developer and BAM developer. Involved in fresh development and lead the team. Responsible for quality deliverables.\n8.   CISCO Advanced Services Project (CISCO-CSSD-AS-PROJECT), California\nDescription:\n\n          The project was basically about accounting the revenue i.e. creating budgets and recognizing revenues.\n\nTeam Size: 15\n\nRole:\nAnalyzing Functional document and creating technical document. The project is based on Oracle Projects module which is combination of modules like Project management, resource management, Billing, Costing etc. Have worked on creating custom ADF screens and used oracle 11g as back end for the same. Used java, SQL queries, PL/SQL. Worked on developing custom OAF pages from scratch and have done enhancements on existing applications. \n\tPersonal \n\nInformation\n\t\u00b7 Name                     : Jyoti Ranjan Mohanty\n\n\u00b7 Date of Birth          :  29th June 1990\n\n\u00b7 Marital Status         :  Married\n\n\u00b7 Current Address      :  Flat 301, Jeevanadiganga apartment, Gururaja   \n\n                                             Layout, Doddanekundi, Marthahalli, Bengaluru,   \n\n                                             Karnataka, India, Pin 560037\n\n\u00b7 Permanent Address :   c/o- Maheswar Mohanty ,\n                                Bhitaradiga patna, Bhuban,\n                                Dhenkanal, Orissa, India, pin-759017 \n\n\u00b7 Languages known    :  English, Hindi, Oriya\n\n\n\n\nDeclaration:\n\nI hereby declare that the above-mentioned information is correct up to my knowledge and I bear the   responsibility for the correctness of the above-mentioned particulars.\n\nPlace: Bengaluru\n\nJyoti Ranjan Mohanty\n PROJECT PROFILE\ufffd\n\ufffd", "entities": [[12389, 12409, "Name", "Jyoti Ranjan Mohanty"], [11608, 11628, "Name", "Jyoti Ranjan Mohanty"], [11205, 11211, "Skills", "Oracle"], [9302, 9308, "Skills", "Oracle"], [9291, 9295, "Skills", "Java"], [8527, 8533, "Skills", "Oracle"], [8490, 8495, "Skills", "Sqoop"], [8483, 8488, "Skills", "Oozie"], [8478, 8481, "Skills", "Pig"], [8465, 8469, "Skills", "Hive"], [8166, 8170, "Skills", "HDFS"], [7109, 7115, "Skills", "Oracle"], [7070, 7077, "Skills", "Pyspark"], [7059, 7064, "Skills", "Spark"], [7053, 7057, "Skills", "Hive"], [6630, 6636, "Skills", "Python"], [5595, 5601, "Skills", "Oracle"], [5212, 5216, "Skills", "Hive"], [4767, 4770, "Skills", "AWS"], [4746, 4752, "Skills", "Python"], [4737, 4744, "Skills", "Pyspark"], [4726, 4731, "Skills", "Spark"], [3606, 3611, "Skills", "Spark"], [2697, 2719, "Degree", "BE in Computer science"], [2374, 2379, "Skills", "Spark"], [2324, 2330, "Skills", "Oracle"], [2135, 2138, "Skills", "AWS"], [2112, 2133, "Skills", "Oracle cloud services"], [1939, 1941, "Tools", "QC"], [1934, 1938, "Tools", "PVCS"], [1919, 1932, "Tools", "SQL Developer"], [1910, 1917, "Tools", "Soap UI"], [1904, 1909, "Tools", "Putty"], [1892, 1902, "Tools", "SSH Client"], [1887, 1890, "Tools", "SVN"], [1882, 1885, "Tools", "GIT"], [1851, 1860, "Skills", "SQL sever"], [1845, 1849, "Skills", "DBCS"], [1837, 1843, "Skills", "Oracle"], [1830, 1835, "Skills", "MySQL"], [1794, 1805, "Skills", "Java script"], [1780, 1792, "Skills", "Web-services"], [1774, 1779, "Skills", "MySQL"], [1763, 1772, "Skills", "SQL/PLSQL"], [1757, 1761, "Skills", "Java"], [1686, 1698, "Skills", "AWS-Redshift"], [1670, 1684, "Skills", "S3 File System"], [1653, 1669, "Skills", "HDFS File system"], [1646, 1652, "Skills", "Python"], [1588, 1593, "Skills", "Oozie"], [1582, 1587, "Skills", "Sqoop"], [1577, 1580, "Skills", "Pig"], [1571, 1575, "Skills", "Hive"], [1559, 1568, "Skills", "Spark-SQL"], [1551, 1558, "Skills", "Pyspark"], [1540, 1545, "Skills", "Spark"], [721, 727, "Skills", "Oracle"], [671, 675, "Skills", "Java"], [607, 613, "Skills", "Python"], [570, 573, "Skills", "AWS"], [526, 532, "Skills", "Oracle"], [520, 525, "Skills", "Sqoop"], [515, 518, "Skills", "Pig"], [509, 513, "Skills", "Hive"], [503, 507, "Skills", "HDFS"], [492, 499, "Skills", "Pyspark"], [481, 490, "Skills", "Spark-SQL"], [474, 479, "Skills", "Spark"], [275, 282, "Years_of_Experience", "7 Years"], [210, 216, "Skills", "Oracle"], [104, 110, "Skills", "Oracle"], [17, 37, "Name", "Jyoti Ranjan Mohanty"]]}