{"text": "Kishore Kumar R\n\nNo.34, Sri SriNidhi Nilaya, 7th main, 3rd cross,\nRaghavendra Nagar, R.M.Nagar,\nBangalore, Karnataka 560016 India\nMobile: 8548855784\nEmail: kishore.kumar6r@gmail.com\nLinkedIn: https://www.linkedin.com/in/kishore-kumar-r-a5762616b/\n\nMotivated and Smart working professional with skills in Hadoop & Spark.\n \nHighly motivated and enterprising professional with good knowledge of Hadoop - Bigdata. Able to use own initiative, working alone or as part of a team under pressure to meet deadlines and objectives. Excellent communication and organizational skills, a strong work ethic and determination to succeed. Enthusiastic learner, eager to meet new challengers and get ahead in the IT industry.\n\nSummary\n\nAround 6.4 years of experience in IT out of which having 5 years of experience in big data technologies.\n\n\u00b7 Deep expertise in Hadoop ecosystem- Spark, Spark Data frames, Spark SQL, Spark Streaming (POC), HDFS, Hive, Sqoop, Flume, Oozie, Pig, Impala, Python, Hbase, Map Reduce, Kafka and Cassandra (Datastax training completed).\n\u00b7 Multiple Frameworks built and supported for vendors.\n\u00b7 Worked on importing and exporting data from different databases like oracle, mysql into HDFS and hive/hbase using SQOOP.\n\u00b7 Implemented real time streaming applications involving json data using FLUME and KAFKA.\n\u00b7 Proven ability to excel in fast paced development environment using latest frameworks/tools- GIT, Jenkins, SVN.\n\u00b7 Loading and analyzing flat files from LFS to HDFS and working knowledge in Linux commands.\n\u00b7 Successfully designed, developed and implemented big data projects in production environment.\n\u00b7 Good exposure in following all the process in a production environment like change management, incident management etc. Expertise in handling development.\n\u00b7 Designed and implemented process to read transactional logs of RDBMS into HDFS in real time using KAFKA & CAMUS (POC).\n\n\n\n\n\n\nSKILLS\n\n\tBig Data\n\tHadoop, Map Reduce, HDFS, Hive, Sqoop, Flume, Oozie, Pig, Spark, Kafka, Impala, AWS, Camus, Confluent Platform, Cloudera, Horton Works, Hbase, Cassandra (training completed)\n\n\tDatabase\n\tMysql, Hive, Oracle, DB2, SQL Server\n\n\tLanguages\n\tUnix Shell, Python, SQL\n\n\tDevOps Tools\n\tGithub, Jenkins, SVN, Anisible\n\n\tOther Tools/Utilities\n\tWinSCP, Putty, Autosys, VM Workstation, PyCharm.\n\n\n\n\nPROFESSIONAL EXPERIENCE\n\nProject Name: US Bank Client Sep '16 to Present\nCompany: Accenture Solutions Private Limited, Bangalore \nRole: Hadoop Developer\nResponsibilities:\n\u00b7 Implemented process to generate oozie xml for Shell, hive, hdfs actions during runtime.\n\u00b7 Data ingestion from multiple (Flat files && RDBMS) sources for CHATBOT application.\n\u00b7 Built multiple Frameworks for processing Files, sql's, validations, file extractions sent to downstream.\n\u00b7 Data Offload from Teradata, Oracle and SQL server using SQOOP to HDFS and access via Hive/Impala for Client reporting.\n\u00b7 Analytics queries developed in Spark-data frames (python)/ JSON flattening using pyspark.\n\u00b7 Accelerate app (innovation app to bring data in real time) Working on Fake News detection innovation and social media - user network clustering.\n\u00b7 Implemented Real time streaming applications involving json data using FLUME and KAFKA.\n\u00b7 Designed and implemented process to read transactional logs of RDBMS into HDFS in real time using KAFKA & CAMUS (POC).\n\nProject Name: Enterprise Data Lake (US Retail) Nov '15 to Sep \u201916 \nCompany: Accenture Solutions Private Limited, Mumbai \nRole: Hadoop Developer\nResponsibilities:\n\u00b7 Independently worked on user stories for Detokenization in Hive, Hive performance optimization and Multiple Flume Agents.\n\u00b7 Loading and analyzing flat files from LFS to HDFS and working knowledge in Linux commands.\n\u00b7 Worked on importing and exporting data from different databases like Teradata, MySQL into HDFS and hive/hbase using SQOOP.\n\n\nProject Name: US Bank Client Jan '13 to Aug \u201915 \nCompany: TCS, Bangalore\nRole: Mainframe/Hadoop Developer\nResponsibilities: \n\u00b7 Active part of End to End design, development and deployment projects for validating files received from different client systems.\n\u00b7 Unit Test preparations and Unit testing and documentation in QC. Requirement analysis and Post deployment support process.\n\u00b7 Maintenance and support of front end applications java, jsp.\n\u00b7 Proof of Concepts to find duplicate images/feature extraction stored in hdfs based on hash values.\n\u00b7 360 degree customer centric view Proof of concept using neo4j, Sentimental analysis of customer feedback.\n\nSignificant Highlights\n\n\u00b7 Awarded Accenture Centre of Excellence (ACE)-Future Skills DEC 2018.\n\u00b7 Awarded Star Performer of the Month FEB 2018 for exceeded expectations in the deliverables and mentored above and beyond expectations.\n\u00b7 Awarded Accenture Centre of Excellence (ACE)-Pathfinder AUG 2016, highest level of accreditation in Accenture, for delivery of multiple critical and complex Hadoop releases in less time.\n\u00b7 Appreciations from client and Accenture leadership on multiple occasions for outstanding performance & defect free project deliveries.\n\u00b7 Active member of Innovation team with prototype reaching top 100 in Accenture global.\n\u00b7 Independently worked on proof of concepts for tools such as Accelerate APP within Accenture.\n\u00b7 Active member CSR community within Accenture and organized many events at DU/IG levels.\n\nAcademics\n\n\tDegree\n\tUniversity\n\t% Marks\n\n\tBE\n\tCMRIT (VTU)\n\t81%\n\n\tII PUC\n\tSt Joseph\u2019s PUC\n\t85%\n\n\t10th\n\tITI Vidya Mandir\n\t89%\n\n\n\nHobbies/Interests\n\n\u00b7 Upgrade skills in Hacker Rank/geeksforgeeks etc.\n\u00b7 Setup multi node Hadoop cluster in AWS and Proof of Concepts in AWS.\n\u00b7 Active part of Charity/CSR events in Accenture.\n\u00b7 Innovation and updating the concepts and skills in Github and Gaming.", "entities": [[5697, 5703, "Tools", "Github"], [5589, 5592, "Skills", "AWS"], [5560, 5563, "Skills", "AWS"], [5542, 5548, "Skills", "Hadoop"], [5368, 5370, "Degree", "BE"], [4885, 4891, "Skills", "Hadoop"], [3927, 3933, "Skills", "Hadoop"], [3803, 3807, "Skills", "HDFS"], [3794, 3797, "Skills", "SQL"], [3665, 3669, "Skills", "HDFS"], [3604, 3609, "Skills", "Flume"], [3561, 3565, "Skills", "Hive"], [3555, 3559, "Skills", "Hive"], [3459, 3465, "Skills", "Hadoop"], [3286, 3290, "Skills", "HDFS"], [2914, 2919, "Skills", "Spark"], [2852, 2858, "Skills", "Impala"], [2847, 2851, "Skills", "Hive"], [2827, 2831, "Skills", "HDFS"], [2801, 2804, "Skills", "SQL"], [2790, 2796, "Skills", "Oracle"], [2442, 2448, "Skills", "Hadoop"], [2293, 2300, "Tools", "PyCharm"], [2277, 2291, "Tools", "VM Workstation"], [2268, 2275, "Tools", "Autosys"], [2261, 2266, "Tools", "Putty"], [2253, 2259, "Tools", "WinSCP"], [2219, 2227, "Tools", "Anisible"], [2214, 2217, "Tools", "SVN"], [2205, 2212, "Tools", "Jenkins"], [2197, 2203, "Tools", "Github"], [2177, 2180, "Skills", "SQL"], [2169, 2175, "Skills", "Python"], [2157, 2167, "Skills", "Unix Shell"], [2133, 2143, "Skills", "SQL Server"], [2128, 2131, "Skills", "DB2"], [2120, 2126, "Skills", "Oracle"], [2114, 2118, "Skills", "Hive"], [2107, 2112, "Skills", "Mysql"], [2064, 2073, "Skills", "Cassandra"], [2057, 2062, "Skills", "Hbase"], [2043, 2055, "Skills", "Horton Works"], [2033, 2041, "Skills", "Cloudera"], [2013, 2031, "Skills", "Confluent Platform"], [2006, 2011, "Skills", "Camus"], [2001, 2004, "Skills", "AWS"], [1993, 1999, "Skills", "Impala"], [1986, 1991, "Skills", "Kafka"], [1979, 1984, "Skills", "Spark"], [1974, 1977, "Skills", "Pig"], [1967, 1972, "Skills", "Oozie"], [1960, 1965, "Skills", "Flume"], [1953, 1958, "Skills", "Sqoop"], [1947, 1951, "Skills", "Hive"], [1941, 1945, "Skills", "HDFS"], [1929, 1939, "Skills", "Map Reduce"], [1921, 1927, "Skills", "Hadoop"], [1911, 1919, "Skills", "Big Data"], [1851, 1855, "Skills", "HDFS"], [1476, 1480, "Skills", "HDFS"], [1424, 1427, "Tools", "SVN"], [1415, 1422, "Tools", "Jenkins"], [1192, 1196, "Skills", "HDFS"], [1006, 1015, "Skills", "Cassandra"], [996, 1001, "Skills", "Kafka"], [984, 994, "Skills", "Map Reduce"], [977, 982, "Skills", "Hbase"], [969, 975, "Skills", "Python"], [961, 967, "Skills", "Impala"], [956, 959, "Skills", "Pig"], [949, 954, "Skills", "Oozie"], [942, 947, "Skills", "Flume"], [935, 940, "Skills", "Sqoop"], [929, 933, "Skills", "Hive"], [923, 927, "Skills", "HDFS"], [900, 905, "Skills", "Spark"], [895, 898, "Skills", "SQL"], [889, 894, "Skills", "Spark"], [870, 875, "Skills", "Spark"], [863, 868, "Skills", "Spark"], [845, 851, "Skills", "Hadoop"], [726, 735, "Years_of_Experience", "6.4 years"], [392, 398, "Skills", "Hadoop"], [313, 318, "Skills", "Spark"], [304, 310, "Skills", "Hadoop"], [156, 181, "Email_Address", "kishore.kumar6r@gmail.com"], [138, 148, "Mobile_No", "8548855784"], [0, 15, "Name", "Kishore Kumar R"]]}