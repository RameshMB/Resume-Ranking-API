{"text": "Nilesh Kumar\nMail : nilesh.it447@gmail.com                                                         Mobile No :+91-9008630725\n\n\t\n\t\n\n\tTechnology Professional with over 8+ Years of experience in software development comprising:\n\u00b7 Expert in JAVA, REST API, Kafka, Spark and related Technologies\n\n\u00b7 Having good experience in Micro service Architecture.\n\n\u00b7 Java, Scala, REST API, Hibernate, Oracle, Spring Core, Spring MVC\n\u00b7 Big Data Technologies (Spark, Kafka, Hadoop, HIVE)\n\n\u00b7 CI and Code quality tools (Hudson, Sonar, Jira)\n\n\u00b7 Version control- GIT , BitBucket\n\u00b7 Excellent track-record of implementing technology solutions to increase operational efficiency.\n\n\u00b7 Have extensive experience in the SDLC process of analysis, design, development\n\u00b7 Good in problem solving using Data Structures and Algorithms.\n\u00b7 Using tools to achieve AGILE goals.\n\n\u00b7 Mentoring juniors, code reviews\n\n\u00b7 Excellent and fast at self-learning of any Technologies.\n\u00b7 Good hands on experience in deployment tools like docker and Kubernates\n\n\tWork Experience:\n\t\n\n\tORACLE India Private Ltd.                                    October 2015\u2013till Date                \n\nSenior Member Technical Staff\n\nPROJECT: DATACATALOG\nDuration: 1.5 year\n\nData Catalog is cloud service for discovering, finding, organizing, enriching and tracing data assets on Oracle cloud and beyond, in order to provide efficient and governed data exploitation.\n\nRoles and Responsibilities:\n\n\u00b7 Worked on design and development of metadata Harvest module to connect to different data sources system like databases, object storage, Kafka, cloud storage and read the metadata from different files systems.\n\u00b7 Develop wrapper scheduling framework on top of cron job. To trigger schedule job for metadata harvest.\nTechnologies: Kafka, spark, Hadoop, REST web services, JAVA, Maven, Git, Docker, Kubernetes , IntelliJ\nPROJECT: DataflowML\nDuration: 2 year\n\nDFML is a cloud platform that provides easy to use services to define manage and execute smart pipelines to process and transform data. It's a 'Lambda Application Platform with Real-time Analytics' as a Service based on Apache Spark, Machine Learning, Mesos, Kafka, YARN, DockerContainers, and written in Scala                 .\n\nRoles and Responsibilities:\n\n\u00b7 Worked on runtime and scheduling framework and actively involved in coding/development of ingestion service, execution of runtime pipeline and managing the lifecycle of the pipelines using spark and Kafka.\n\u00b7 Worked on Development of scheduling framework REST wrapper on top of Chronos/Mesos scheduling framework which will handle the error/exception reporting.\n\nTechnologies: Kafka, spark, REST web services, Scala , JAVA, Chronos, mesos Maven, Git, Docker                             \nEMC Data Storage Systems, India               September 2010 \u2013 2015 (5 yrs. exp.)\nAssociate consultant II\n\nPROJECT: CeTi LifeCare\nClient: Apollo , Manipal\n\nDuration: 8 Month\n\n\u00b7 LifeCare Project is the Cloud based enterprise application, which is for checking of disease of patient and generate the alert if he/she require consulting the doctor based on measurement. A Medhub device (Bluetooth enabled) which takes the measurement from different medical devices and process the measurement and give the alert if it\u2019s required.\n\nRoles and Responsibilities:\n\u00b7 Worked on processing module to process the different type of vital, and call the CDSS web services for generating the alert.\n\u00b7 Design the architecture of processing module and mediation flow based on spring integration.\nTechnologies used: Spring integration, Spring data JPA, Spring web Services, Maven, Git.\n\nPROJECT: SGX\nClient: Singapore Stock Exchange \nDuration: 8 Month\n\n\u00b7 SGX Project is the Cloud based enterprise application, which manages the stock related activity in fast and more reliable by uses of best architecture and best practice which helps and supports the clients to make use of Java and spring related application development.\n\nRoles and Responsibilities:\n\n\u00b7 Worked on Audit trail framework, User, Customer and Spring Security module.\n\n\u00b7 Utilizing and applying best practices of all spring related technology.\nTechnologies used: vFabric SQLFire, Spring, JIRA, Maven, JavaScript, Github,\n\nSonar.\nAchievement: Got Silver Prize from company for good contribution in the project.\nPROJECT: EMC Cloud\nClient: EMC Initiative/ Product\n\nDuration: 6 Month\n\n\u00b7 EMC Cloud competency makes the technology Cloud enabled by using EMC cloud products like Cloud foundry and applying best practices of Platform as a Service using Cloud foundry. It helps and supports EMC internal clients to make use of Cloud platform and Java application development.\n\nRoles and Responsibilities:\n\n\u00b7 Ramp up on vFabric SQLFire.\n\n\u00b7 POCs on Spring Data, SQLFire, Spring MVC, Spring Security\n\n\u00b7 JavaScript frameworks including Bootstrap.js\n\n\u00b7 Utilizing, and applying best practices of Platform as a Service using Cloud foundry.\n\nTechnologies used: vFabric SQLFire, Spring, JIRA, Maven, JavaScript, Github.\nPROJECT: Data As a Service (DAAS)\n\nRole: Developer / Analysis, Design & Implementation.\nClient: EMC Initiative/ Product\nDuration: 6 Month\n\nSKILLS: Activiti BPM, Spring MVC, GreenPlum, Web Services , Attivio , GlobalIds\n\nDAAS is providing end user the application that has capability to provide on time data with minimal time and maximum accuracy. The data is for business people that do not understand the database technologies but want to access data for their analysis and reporting work.\n\nThis program includes:\n\nThis project includes workflow management that is done by Activiti BPM an Open source Business Process Management engine. Activiti is integrated with data governance tool GlobalIds through web services to get the Metadata. Once got the metadata Activiti flow goes further to Attivio through their API and creates a universal index for the particular table that exists in any other database. At the end an external web table is created in GreenPlum i.e. Analytical database with the URL provided by Attivio for universal data access.\n\nResponsibilities:\n\nAnalyzing the core functionality of the project and requirements during the requirements analysis phase of projects. Set up Activiti BPM process and create the process diagram to decide the flow of entire application. Integrate Activiti with GlobalIds and Attivio for access data at application level, sending request to create index for Attivio and finally create external table in GP i.e. GreenPlum.\n\nFull flow testing and Activiti BPM work flow management.\n\nPROJECT: Paetec Network \u2013 EMC, Bangalore\n\nClient: Paetec (wind stream), USA.\n\nDuration: \u00a06 Month\nObjective of this project is to migrate the Existing legacy database \u2018AS400\u2019 to Oracle database.\n\nPAETEC is telecom domain project which will maintain wired network. Each module will provide new scope for maintaining Paetec wired network and its components like Site, Shelf, Slot, and Port. And migrating the network database \u2018AS400\u2019 to oracle database.\n\n\u00b7 Developed and analyze the design.\n\n\u00b7 Management of telecom inventory helps Paetec wired network to be more flexible.\n\n\u00b7 Maintenance of such large wired networks manually is very difficult. We develop codes to automate all the manual process for high performance.\n\n\u00b7 Removal of unwanted and free slots and ports greatly increase network performance.\n\nResponsibilities:\n\n\u00b7 Analyzing the design document and requirements during the requirements analysis phase of projects.\n\n\u00b7 Develop the assigned requirement and design the job according to requirement in talend tool.\n\n\u00b7 Develop the modules and working as per project need.\n\n\u00b7 Writing, Reviewing and Executing codes.\n\n\u00b7 Working with SQL, PL/SQL code to fetch the proper data from Paetec Databases i.e. used by Paetec to manage network data.\n\nPROJECT: Lightspeed (Wired Network Management)\n\nRole: Developer / Analysis, Design & Implementation.\nClient: ATnT LABS/ EMC Consulting Integrated Product\n\nDuration: 1.8 years\nSKILLS:  Core JAVA, SOAP, XML, SQL, Junit\n\nLIGHTSPEED is telecom domain project which will maintain ATnT wired network. Each module will provide new scope for maintaining AT&T wired network and its components like Site, Shelf, Slot, and Port. Also manages Inventory system that contains the data i.e. Granite Inventory.\n\nResponsibilities:\n\nAnalyzing the design document and requirements during the requirements analysis phase of projects. Develop the assigned requirement and unit test the same. Develop and unit test modules are working as per project need and those services are giving desired output. Writing, Reviewing and Executing codes. Working with SQL PLUS to fetch the proper data from Granite telcodia inventory i.e. used by AT&T to manage network data. Resolving the issues about project requirements (Software, Hardware, Resources).\n\nProduct Training: Information Storage Management(ISM), Spring Source.\nAwards and Recognitions: Got Silver\u00a0award\u00a0as individual contributor in Java based Project. Appreciated For the good knowledge of Core JAVA, Recognized as fast learner of core Spring and J2EE.\n\n\t\n\t\n\n\tEDUCATION:\n\tInstitute   : Gandhi Institute of Engineering and Technology Orissa.\n\nUniversity : Biju Patnaik University of Technology\u00a0(BPUT) Raurkela\n\nGrade        : B.TECH (IT) in 2006-2010 with CGPA 7.48\n\n\t\n\t   \n\n\tCERTIFICATIONS:\n\n\t\u00b7 Oracle Certified java professional (java-1.6).\n\u00b7 Oracle Certified Associates (Oracle9i) (SQL, PL/SQL).\n\n\tPERSONAL:\n\tDate of Birth:  13th  April 1983\nPlace of Birth:  Motihari , India\n\nCitizenship:  Indian\n\nPassport No: J5027309,Valid till 2020\nHobbies: Social Working, Listening to music, Enjoy with friends & Team\nStrength: Enthusiasm, Devotion to work, Quick learner, Sincere & Responsible\n\n\n\nDECLARATIONS: I vouch for the authenticity of the above information.\n                                                                                                                                                                                                                                                                                Place: Bangalore                                                                                                      Nilesh Kumar", "entities": [[10130, 10142, "Name", "Nilesh Kumar"], [9325, 9377, "Certifications", "Oracle Certified Associates (Oracle9i) (SQL, PL/SQL)"], [9276, 9321, "Certifications", "Oracle Certified java professional (java-1.6)"], [9206, 9217, "Degree", "B.TECH (IT)"], [8976, 8980, "Skills", "JAVA"], [7939, 7943, "Skills", "JAVA"], [4968, 4974, "Tools", "Github"], [4187, 4193, "Tools", "Github"], [3591, 3594, "Tools", "Git"], [2694, 2700, "Tools", "Docker"], [2689, 2692, "Tools", "Git"], [2676, 2687, "Skills", "mesos Maven"], [2667, 2674, "Skills", "Chronos"], [2661, 2665, "Skills", "JAVA"], [2653, 2658, "Skills", "Scala"], [2634, 2651, "Skills", "REST web services"], [2627, 2632, "Skills", "spark"], [2620, 2625, "Skills", "Kafka"], [2521, 2528, "Skills", "Chronos"], [2443, 2448, "Skills", "Kafka"], [2433, 2438, "Skills", "spark"], [2188, 2193, "Skills", "Scala"], [2155, 2161, "Tools", "Docker"], [2142, 2147, "Skills", "Kafka"], [1815, 1821, "Tools", "Docker"], [1810, 1813, "Tools", "Git"], [1797, 1801, "Skills", "JAVA"], [1778, 1795, "Skills", "REST web services"], [1763, 1768, "Skills", "spark"], [1756, 1761, "Skills", "Kafka"], [1564, 1569, "Skills", "Kafka"], [449, 454, "Skills", "Kafka"], [357, 362, "Skills", "Scala"], [253, 258, "Skills", "Kafka"], [237, 241, "Skills", "JAVA"], [166, 174, "Years_of_Experience", "8+ Years"], [110, 124, "Mobile_No", "+91-9008630725"], [20, 42, "Email_Address", "nilesh.it447@gmail.com"], [0, 12, "Name", "Nilesh Kumar"]]}