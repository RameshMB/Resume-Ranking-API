{"text": "Pankaj Kumar\n\t\n\tHSR Layout, Sector 2\n\t\n\n\t\n\t\n\t\n\tBengaluru/Bangalore, KA 560102\n\t\n\n\t\n\tMu-Sigma,\n\t\n\t(+91) 94808 62869\n\t\n\n\t\n\t\n\t\n\tPK179288@gmail.com\n\t\n\n\t\n\tData Engineer\n\t\n\t\n\t\n\n\t\n\t(Oct, 2018 \u2013 Present)\n\t\n\t\n\t\n\t\n\n\t\n\t\n\t\n\t\n\t\n\t\n\n\n\n\n\nEXECUTIVE SUMMARY\n\n\nI am a results-oriented accomplished professional serving the industry with 6.7 years of extensive experience in Data Engineering (D3 Stack), Devops/CICD using GITLAB, Docker & Kubernetes, Cloud Computing, EDGE Computing, IoT Analytics, Hadoop Ecosystem, Data Integration & Business Intelligence Tools.\n\u00b7 I am a fundamental, strong and a promising professional.\n\n\u00b7 Market/self-driven consistent performer, working towards scaling/expanding the business.\n\n\u00b7 Can organize constructive meetings with clients to understand the business trends and requirements.\n\n\u00b7 Strengths: quick learner, adaptive, highly motivated, growth & development oriented.\n\n\u00b7 Demonstrated experience with excellent problem-solving skills, solutioning, high analytical and interpersonal skills.\n\u00b7 I am an IT geek with a never-ending zeal and hunger to learn and grow.\n\n\nTECHNICAL SUMMARY\n\n\n\u00b7 End-to-end design, solutioning and deployment experience of data engineering pipelines on AWS and Open Stack or on premise.\n\n\u00b7 Research & implementation experience on IoT Analytics using edge devices such as Raspberry PI, Beacons, Cameras, Sensors, etc.\n\u00b7 Experience in implementing, fully managed high-end Hadoop, Spark and\n\nKafka clusters on bare metal and cloud using YARN and Kubernetes/Docker for research and development purpose.\n\n\u00b7 Aware, engaged & experienced in designing and developing end to end\n\nData Engineering pipelines on Lambda and Kappa architectures on cloud and on-premise landscape: Data Modelling, Data Ingestion, Data Scrubbing, Data Curation, Data Wrangling/Crunching/Munching, Data Governance & Security, Data Lineage, Data Sciences, Decision Sciences (D3 Stack), Data Consumption, DevOps (CICD), AI/ML and IoT.\n\n\u00b7 2+ years of relevant hands-on experience on Hadoop Technologies - HDFS, Map Reduce, Pig, Hive, Sqoop, Flume, Oozie/Zookeeper, HBase, Cassandra.\n\n\u00b7 5+ years of relevant hands-on experience in Teradata \u2013 architecture, utilities, performance tuning, stored procs, etc.\n\n\u00b7 4+ years of relevant hands-on experience in Informatica PC, architecture, performance tuning, etc.\n\u00b7 1+ Year of relevant hands-on experience in Tableau & SAP BO.\n\n\u00b7 3+ Years of relevant experience on Unix/Linux/Python Scripting.\n\n\u00b7 Strong conceptual knowledge on OOP\u2019s & Structured programming languages\u2013 Core Java, C++, C, Python, etc.\n\n\nEDUCATIONAL QUALIFICATIONS\n\n\n\tDegree\n\tUniversity/College\n\tYear of Passing\n\tCGPA\n\n\tM.Tech\n\tBits Pilani [WILP/WASE] *\n\t2016\n\t6.85\n\n\tBCA\n\tGraphic Era University, Dehradun\n\t2012\n\t68.80%\n\n\t\n\t\n\t\n\t\n\n\t12th\n\tSGRR Public School, Dehradun\n\t2008\n\t60%\n\n\t10th\n\tSGRR Public School, Dehradun\n\t2006\n\t66%\n\n\n\nAll courses are Full Time\n\n*WILP stands for Work Integrated Learning Program/WASE \u2013 Wipro Academy of Software Excellence\n\n\n\n\nPROFESSIONAL COMPETENCIES\n\n\nSpark, PySpark\n\nKafka\n\nTeradata\n\nInformatica\n\nHadoop \u2013 Hive, Pig, Sqoop, etc\n\nGitlab/Github\n\nData Engineering\n\nCassandra, HBase, MongoDB\n\nShell scripting\n\nTableau\n\nHyperion Essbase\n\nDocker on Kubernetes\n\nJira\n\nJupyter/Zeppelin/Atom\n\nPython\n\nApplied Stats using R & SAS\n\nScala/Java/C++/C\n\n\nAcquired Skills\n\n\nIoT and Sensor Analytics\n\nMachine Learning\n\nDecision Sciences\n\nKubernetes and Docker\n\nCICD using DevOps\n\nWeb App Development\n\n\n\nNOTEWORTHY CREDITS\n\n\n\u00b7 \u2018Early Prodigy & Feather in Cap Award\u2019.\n\n\u00b7 Felicitated with \u2018Peoples Champ Award winner\u2019 for 4 years in a row.\n\n\u00b7 Got rewarded with \u2018Project Execution Excellence Award\u2019 and \u2018Long Service Award\u2019 in 2017.\n\n\u00b7 Major achievements include Client Satisfaction, multiple appreciations and accolades from clients.\n\n\u00b7 Major contributor in Business development & Growth initiatives.\n\n\u00b7 3 things which best describes me \u2013 A Traveler, A reader (Technical and Non-Fiction) & A big foodie.\n\n\nPERSONAL DETAILS\n\n\nDate of Birth: 13-Nov-1990\tGender: Male\n\nNationality: Indian\nLanguages Known: English and Hindi\n\nCAREER PROGRESSION\n\n\n\nCompany - Mu-Sigma, Bengaluru Team - Mu-Sigma LABS (R&D wing of Mu-Sigma) Data Sciences Team (DEFT)\n\n\n\n\nRole: Data Engineer\n\nOctober 2018 \u2013 Till Date\n\nTeam Size: 14\n\n\n\nSpark    Kafka    NoSQL    Kubernetes    Docker   GCP   AWS\n\n\n\n\nAlteryx\n\n\n\nSAS    Python & R    Teradata\n\n\n\n\nOpenStack\n\n\n\n\nData Engineering\n\n\n\n\nInformatica BDM\n\n\n\n\nAzure\n\n\n\nEDGE Computing   AWS Kinesis\n\n\n\n\nIOT Analytics\n\n\n\n\nResearch and Development wing of Mu-Sigma (DEFT):\n\nBeing part of LABS team in Mu-Sigma, I have helped the organization in growing and building endless capabilities around major and niche-end technologies of the current and future era. I have worked round the clock and helped multiple teams accelerate in Data engineering space. With an aim of business and revenue growth, I have been closely working with presales and sales team to help engage themselves more and more with clients in the space of Data Engineering or D3 Stack.\n\n\u00b7 Worked on creating capabilities around real-time and batch-based data pipelines using Lambda and Kappa architectures on cloud and on-premise infrastructure. Heavily engaged in handling multiple clients on data engineering portfolios from different domains such as Telecom, Retail, Manufacturing, Banking, E-Commerce, Healthcare and from different geographies across the globe as well.\n\n\u00b7 Worked with teams on operationalizing, curating and creating meaningful insights from data of up to 40+ TB\u2019s by suggesting architectures, landscapes and deploying pipelines in client environments;\n\n\u00b7 Implemented and created reusable artefacts from the research and experiments around multiple cloud vendors (AWS, GCP, Azure);\n\n\u00b7 Benchmarked different open-source and proprietary tools on different API\u2019s, platforms, versions using different resource managers;\n\u00b7 Benchmarked Apache products like Spark, Flink, Kafka, etc. using YARN/Kubernetes on cloud and on-premise;\n\n\u00b7 Created and shared a dozen of artefacts within the org and with the clients for showcasing capabilities in space of Data Engineering and cloud;\n\n\u00b7 Created real-time and batch-based data pipelines on Lambda and Kappa architecture using Kafka, Spark, AWS Kinesis, Cassandra, Redshift and Python, keeping compliance, InfoSec and security protocols as top concern;\n\n\n\nClient: World\u2019s Largest Networking Company [B2B] Project: Retail/Finance ASBI Domain: Manufacturing [NEPC] Metrics \u2013 Finance and Retail\n\n\n\n\nRole: Senior Data Analyst\n\nJune 2017 \u2013 Till Date\nTeam Size: 12\n\nWipro Limited, Bengaluru\n\n\n\n Hive\n\n\n\n Sqoop\n\n\n\n Teradata\n\n\n\n Cassandra\n\n\n\n Impala\n\n\n\n Oracle\n\n\n\n Informatica\n\n\n\n Spark\n\n\n\n Scala\n\n\n\n Python\n\n\n\n\nProject Description: Setting up batch and real time data pipeline for data analysis\n\nThis is a Teradata to Hadoop and SAP HANA migration project. The objective of this project is to induct the data into Hadoop environments from existing legacy systems [Teradata, Oracle, etc.]. The project deals with Finance and Revenue metrics. With source of truth as Teradata for business users, providing solutions to the customers where in optimal data ingestions techniques have to be laid down. Major challenges involved in this project are: first is to integrate various metrics from Teradata in a single Hadoop platform without hampering the data; secondly to keep up the current reporting system with incremental and parallel releases in Hadoop environment; implementing an automated synchronous data load mechanism to reduce the manual intervention while loading data to multiple platforms including Hadoop.\n\n\n\tClient: World\u2019s leading electronics manufacturer [B2C/B2B]\n\t\n\tRole: Data Analyst\n\t\n\t\n\t\n\n\tProject: GBI TURBO & GBI SPA\n\t\n\t\n\t\n\t\n\tOct 2012 \u2013 May 2017\n\t\n\t\n\n\tDomain: Manufacturing\n\t\n\t\n\t\n\t\n\tTeam Size: 16\n\t\n\t\n\t\n\n\tMetrics - Finance/Retail/Operations/Sales\n\t\n\t\n\t\n\tWipro Limited, Bengaluru\n\t\n\t\n\n\tTeradata\n\tInformatica\n\tSAP BO\n\tEssbase\n\tTableau\n\tShell Scripting\n\tPython\n\tHive\n\tOracle\n\tCassandra\n\tSqoop\n\n\n\n\nProject Description: Global Business Intelligence [GBI] is an ecosystem of multiple Analytical tools and technologies which helps Business users to make decisions, forecasting, perform analysis, etc.\n\nThe objective of this project was to fill gaps in the reporting and analysis tools available to Finance analysts. The project will integrate units, revenue, and margin data into Essbase and automate reporting and analysis. This engagement started with providing world-class Essbase finance solutions using Informatica PC and Teradata as ETL Tools to the finance and retail business users. The project continues to provide seamless reporting experience.\n\nProjects/Application Implementations (Oct,2012 \u2013 May,2017):\n\n\tSPA (Ship Plan Attainment)\n\tDeveloped Corp Units\n\tSTF Actuals and Forecast\n\n\tEssbase DR Setup\n\tEQM [Essbase Quality Management]\n\tFAST [Finance Analytics Spreadsheet Tool]\n\n\tEssbase Alias Optimization\n\tRAST [Retail Analytics Spreadsheet Tool]\n\tFINOPEX [Financial Operating Expenses]\n\n\tDeveloped URM & GM/PL\n\tHyperion Upgrades and enhancements\n\tSupported EDW migrations/up-gradations", "entities": [[8491, 8499, "Skills", "Teradata"], [8472, 8483, "Skills", "Informatica"], [7955, 7960, "Skills", "Sqoop"], [7944, 7953, "Tools", "Cassandra"], [7930, 7934, "Skills", "Hive"], [7922, 7928, "Skills", "Python"], [7896, 7903, "Tools", "Tableau"], [7866, 7877, "Skills", "Informatica"], [7856, 7864, "Skills", "Teradata"], [7559, 7565, "Skills", "Hadoop"], [7396, 7402, "Skills", "Hadoop"], [7261, 7267, "Skills", "Hadoop"], [7240, 7248, "Skills", "Teradata"], [7018, 7026, "Skills", "Teradata"], [6917, 6925, "Skills", "Teradata"], [6867, 6873, "Skills", "Hadoop"], [6771, 6777, "Skills", "Hadoop"], [6759, 6767, "Skills", "Teradata"], [6653, 6659, "Skills", "Python"], [6643, 6648, "Skills", "Scala"], [6633, 6638, "Skills", "Spark"], [6617, 6628, "Skills", "Informatica"], [6581, 6590, "Tools", "Cassandra"], [6568, 6576, "Skills", "Teradata"], [6558, 6563, "Skills", "Sqoop"], [6549, 6553, "Skills", "Hive"], [6238, 6244, "Skills", "Python"], [6214, 6223, "Skills", "Cassandra"], [6194, 6199, "Skills", "Spark"], [6187, 6192, "Skills", "Kafka"], [5913, 5923, "Tools", "Kubernetes"], [5890, 5895, "Skills", "Kafka"], [5876, 5881, "Skills", "Spark"], [4381, 4392, "Skills", "Informatica"], [4333, 4341, "Skills", "Teradata"], [4328, 4329, "Skills", "R"], [4319, 4325, "Skills", "Python"], [4312, 4315, "Skills", "SAS"], [4278, 4284, "Tools", "Docker"], [4264, 4274, "Tools", "Kubernetes"], [4246, 4251, "Skills", "Kafka"], [4237, 4242, "Skills", "Spark"], [3381, 3387, "Tools", "Docker"], [3366, 3376, "Tools", "Kubernetes"], [3347, 3364, "Skills", "Decision Sciences"], [3329, 3345, "Skills", "Machine Learning"], [3311, 3327, "Skills", "Sensor Analytics"], [3303, 3306, "Skills", "IoT"], [3281, 3282, "Skills", "C"], [3277, 3280, "Skills", "C++"], [3272, 3276, "Skills", "Java"], [3266, 3271, "Skills", "Scala"], [3261, 3264, "Skills", "SAS"], [3257, 3258, "Skills", "R"], [3229, 3235, "Skills", "Python"], [3223, 3227, "Tools", "Atom"], [3214, 3222, "Tools", "Zeppelin"], [3206, 3213, "Tools", "Jupyter"], [3200, 3204, "Tools", "Jira"], [3188, 3198, "Tools", "Kubernetes"], [3178, 3184, "Tools", "Docker"], [3151, 3158, "Tools", "Tableau"], [3134, 3149, "Skills", "Shell scripting"], [3125, 3132, "Tools", "MongoDB"], [3118, 3123, "Tools", "HBase"], [3107, 3116, "Tools", "Cassandra"], [3081, 3087, "Tools", "Github"], [3074, 3080, "Tools", "Gitlab"], [3062, 3067, "Skills", "Sqoop"], [3057, 3060, "Skills", "Pig"], [3051, 3055, "Skills", "Hive"], [3042, 3048, "Skills", "Hadoop"], [3029, 3040, "Skills", "Informatica"], [3019, 3027, "Skills", "Teradata"], [3012, 3017, "Skills", "Kafka"], [3003, 3010, "Skills", "PySpark"], [2996, 3001, "Skills", "Spark"], [2635, 2641, "Degree", "M.Tech"], [2538, 2544, "Skills", "Python"], [2535, 2536, "Skills", "C"], [2530, 2533, "Skills", "C++"], [2524, 2528, "Skills", "Java"], [2425, 2431, "Skills", "Python"], [2358, 2365, "Tools", "Tableau"], [2258, 2269, "Skills", "Informatica"], [2136, 2144, "Skills", "Teradata"], [2078, 2087, "Tools", "Cassandra"], [2078, 2079, "Skills", "C"], [2071, 2076, "Tools", "HBase"], [2040, 2045, "Skills", "Sqoop"], [2034, 2038, "Skills", "Hive"], [2029, 2032, "Skills", "Pig"], [1989, 1995, "Skills", "Hadoop"], [1937, 1940, "Skills", "IoT"], [1864, 1881, "Skills", "Decision Sciences"], [1496, 1502, "Tools", "Docker"], [1485, 1495, "Tools", "Kubernetes"], [1431, 1436, "Skills", "Kafka"], [1420, 1425, "Skills", "Spark"], [1412, 1418, "Skills", "Hadoop"], [1272, 1275, "Skills", "IoT"], [479, 485, "Skills", "Hadoop"], [464, 467, "Skills", "IoT"], [419, 429, "Tools", "Kubernetes"], [410, 416, "Tools", "Docker"], [318, 327, "Years_of_Experience", "6.7 years"], [125, 143, "Email_Address", "PK179288@gmail.com"], [97, 114, "Mobile_No", "(+91) 94808 62869"], [0, 12, "Name", "Pankaj Kumar"]]}