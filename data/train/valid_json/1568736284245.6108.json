{"text": "Rahul Pratap Singh \nE-Mail: rathaur.rps@gmail.com\n\n\n\nContact: +91 9953579739(M)\nObjective\nTo seek the challenging position in Software industry that needs innovation, creativity, dedication and enable me to continue to work in a challenging and fast paced environment, leveraging my current knowledge and fostering creativity with many learning opportunities. \n\nProfessional Synopsis\n\u00b7 A growth oriented professional total experience of 8 years in IT industry including 3 years in the BigData Technologies.\n\u00b7  Having exposure in the complete Software development life cycle involving development, documentation, and testing.  Currently working with the Wipro Technologies Solutions, Bangalore, India as project Lead\n\u00b7 Experience of end-to-end implementation Of BIG data & Financial Solutions\n\u00b7 Hands on Experience in working with Hadoop ecosystem like Hive, PIG, Sqoop, Map Reduce, Oozie\n\n\u00b7 Hands on Experience in Scala ,Spark Core, Spark Sql, Spark Streaming, Kafka, Cassandra\n\u00b7 Proficient in team handling and people management. \n\n\u00b7 Exposure in handling client queries, providing them feasible solutions & building healthy relationships thereby achieving high customer satisfaction.\n\n\u00b7 Ability to perform and deliver under pressure and deadlines, and to work with a team\n\u00b7 Capable to delve into the new leading Technologies\nExperience Summary\n\u00b7 Working with Wipro Technologies since  FEB 2016 to till date. Currently Working as Big data Analytics project lead for last 3 years\n\u00b7 Worked with NSEL Noida as a software Engineer before the Wipro engagement. \nWorked on CAS product enhancement, implementation and support project for 4.7 years & delivered \nQuality services to the reputed Banks and NBFC\u2019s \n\n\tTechnical Skills\n\t\n\n\tBigData Skills\n\tHadoop, HDFS, , Hive, Pig, Sqoop, MapReduce, Oozie, Hbase,  Spark, Scala, Kafka, Cassndra, \n\n\tJava Technologies       \n\tJava/J2ee, Struts1.2, Rest Web Services , java Scripts, Spring\n\n\tRDBMS\n\tOracle SQL/PL-SQL\n\n\tFramework\n\tHadoop (HDFS), Struts 1.2\n\n\tVersion control Tool\n\t     SVN, Git\n\n\tApplication Servers\n\tOracle -10g ,  Weblogic , WAS\n\n\tHadoop Distributions\n\tCloudera \n\n\tDev Tools\n\tWindows, Linux, Eclipse\n\n\nProject Exposure in Wipro\n\tPROJECT-1 EDP Yes Bank\n\t\n\n\tClient\n\nSkills \n\tTD Bank()\nHDFS, Hive, Pig , Shell Script, Tableau, HBASE, Rest API, OOZIE, IMPALA, Spark, Scala\n\n\tRole \n\tProject Lead\n\n\tTeam Size\n\t07                                 \n\n\tDuration\n\nProcess\n\tDEC 2017 to till date.\nAgile\n\n\n\nDescription: \n\n\u00b7 Working with leading Bank to build Enterprise Data provisioning platform. \n\u00b7 Data ingestion is taking place from various Upstream systems, FCR, API data & SFDC.\n\u00b7 Data is ingestion is done into HDFS.\n\u00b7 The incoming data is specifically related to banking customers, Retail & corporate Lending, Cards and account details which is filtered against data quality checks and stored for further processing.\n\u00b7 The input data is dierectly loaded from oracle tables to HDFS using Sqoop. Some data is in  XML, CSV and JSON File formats and same is loaded using HDFS Scripts.\n\u00b7  Job workflows are scheduled using Oozie.\n\u00b7 Data is transformed using Pig scripts and loaded in the Hive tables in ORC format.\n\u00b7 Based on the requirement data is loaded in HBASE ,Cassandra & RDBMS tables, and also directly used by downstream systems from Hive \n\n\u00b7 HBASE is used as input for mReport system which is Data as a service implementation.\nResponsibilities:  \n\nInvolved in the business oriented process like requirement gathering, design the solution on Big Data Platform, implementation/development of the requirement using Big Data capabilities, ramping up new team members, client\u2019s status call, Unit Testing.\n\u00b7 Leading a Team of 7 members involved in the EDPP analytics project\n\u00b7 Requirement gathering & Co-ordination with Client.\n\n\u00b7 Big data Project Implementation based on the requirement\n\u00b7 Loading Data from source Systems to HDFS\n\u00b7 Performing ETL operations using pig.\n\n\u00b7 Implementing pig operations using Cascading APIs.\n\n\u00b7 Loading data into hive tables and Creating hive views.\n\u00b7 Loading data into Hbase.\n\n\u00b7 Writing UDF\u2019s & map reduce code\n\u00b7 Involved in writing Junit test cases.\n..\n\n\tPROJECT 2- EDPP\u2013 BOQ\n\t\n\n\tClient\n\nSkills \n\tBOQ\n\nJava, HDFS, Hive, Pig , Shell Script, Tableau, HBASE, Rest API, OOZIE, IMPALA\n\n\tRole \n\tSenior Developer\n\n\tTeam Size\n\t05                                \n\n\tDuration\n\nProcess\n\tFEB 2016- DEC 2017 \nAgile\n\n\n\n\nDescription: \n\n\u00b7 Working with leading Bank to build Enterprise Data provisioning platform. \n\u00b7 Data ingestion is taking place from various Upstream systems like  Lending , FCR, API data & SFDC.\n\n\u00b7 Data ingestion is done by Sqoop and HDFS commands.\n\n\u00b7 The incoming data is specifically related to banking customers, Retail & corporate Lending, Cards and account details which is filtered against data quality checks and stored for further processing.\n\n\u00b7 The input data is directly loaded from oracle tables to HDFS using Sqoop. Some data is in  XML, CSV and JSON File formats and same is loaded into HDFS using HDFS commands. \n\u00b7 Job workflows are scheduled using Oozie.\n\n\u00b7 Data is transformed using Pig scripts and loaded in the Hive tables in ORC format.\n\n\u00b7 Based on the requirement data is loaded in HBASE , RDBMS tables, and also directly used by downstream systems from Hive \n\nResponsibilities:  \n\nInvolved in the business oriented process like requirement gathering, design the solution on Big Data Platform, implementation/development of the requirement using Big Data capabilities, ramping up new team members, client\u2019s status call, Unit Testing.\n\u00b7 Requirement gathering & Co-ordination with Client.\n\n\u00b7 Big data Project Implementation based on the requirement\n\n\u00b7 Loading Data from source Systems to HDFS\n\n\u00b7 Involved for writing sqoop scripts for data import in HDFS\n\u00b7 Performing data cleansing operations using pig.\n\n\u00b7 Implementing pig operations using Cascading APIs.\n\n\u00b7 Loading data into hive tables and Creating hive views.\n\n\u00b7 Loading data into Hbase.\n\n\u00b7 Writing UDF\u2019s & map reduce code\n\n\u00b7 Involved in writing Junit test cases.\n\n.\nProject # 3: Aircel Campaign Project \n\nCampaign Analytics component retrieves the required data from the source system - Nextra Unified.\n\nCampaign Analytics Unified generates Events and Customer Data. These are exported at the pre-set frequency (Configurable - say 15 min, 30 minutes etc., for Event Data and once in a day for Customer Data, which is configurable). These are exported using shell script, which SFTPs and coverts the files from TCI to CSV format.\n\nETL Tool PIG Latin processes the flat files, cleanses the data and applies the business rules, loading the data into HDFS (Hive output tables).\n\nETL Tool PIG Latin processes the flat files, cleanses the data and applies the business rules, loading the data into HDFS (Hive output tables). \n\n.\nResponsibilities:  \n\nInvolved in the business oriented process like requirement gathering, design the solution on Big Data Platform, implementation/development of the requirement using Big Data capabilities, ramping up new team members, client\u2019s status call, Unit Testing.\n\u00b7 Requirement gathering & Co-ordination with Client.\n\n\u00b7 Big data Project Implementation based on the requirement\n\n\u00b7 Loading Data from source Systems to HDFS\n\n\u00b7 Performing data cleansing operations using pig.\n\u00b7 Loading data into hive tables and Creating hive views.\n\nProject # 3: HDFC CAS on mobile implementation. Handling and leading Implementation \n\n\u00b7 Interacting with the client for requirements gathering & scoping and providing solution.\n\n\u00b7 Handling architecture and designing the solution.\n\n\u00b7 Developing application and undertaking code review.\n\n\u00b7 Managing process setup & development with the team\n\n\u00b7 Production Implementation of the product\n\n\u00b7 Coordinating for Client Testing, Deployment and Training.\n\n\u00b7 Debugging and troubleshooting the application. \n\tTechnical Skills\n\t\n\n\tTechnologies Used       \n\t Java, Servlets, JSP, JDBC, Struts 1.2 Frame Work, XML and Java Script and,    android, Webservices , Oracle PL/SQL\n\n\tTools Used       \n\tJDeveloper , Eclipse\n\n\nProject#4 BOM Philippines CAS on mobile implementation. Handling and leading implementation\n\n\u00b7 Interacting with the client for requirements gathering & scoping and providing solution.\n\n\u00b7 Handling architecture and designing the solution.\n\n\u00b7 Developing application and undertaking code review.\n\n\u00b7 Managing process setup & development with the team\n\n\u00b7 Production Implementation of the product\n\n\u00b7 Coordinating for Client Testing, Deployment and Training.\n\n\u00b7 Debugging and troubleshooting the application. \n\n\tTechnical Skills\n\t\n\n\tTechnologies Used       \n\tJava, Servlets, JSP, JDBC, Struts 1.2 Frame Work, XML and Java Script and,    android, Webservices , Oracle PL/SQL\n\n\tTools Used       \n\tJDeveloper , Eclipse\n\n\nProject # 5: CAS SBI\nProject Description:  Customer Acquisition System (CAS) is a Web-based system that fulfils the requirements of banking and non-banking financial organizations in automating the loan origination and processing operations This system serve as an open standard, scalable, and maintainable solution for organizations in the lending industry .the loan processing mechanism takes into account various categories and products for which loan is sought, such as \n\n1. AUTO LOAN \n\n2. PERSION LOAN \n\n3. EDUCATION LOANS\n\n4. HOME LOANS\n\n\tTechnical Skills\n\t\n\n\tTechnologies Used       \n\t  Java, Servlets, JSP, JDBC, Struts 1.2 Frame Work, XML and Java Script and,  Webservices , Oracle PL/SQL\n\n\tTools Used       \n\tJDeveloper , Eclipse\n\n\n\t\n\n\n\t\n\n\nProclamation                                                                                                           Date: Monday, April 22, 2019 \n\nHereby I declared the above reveal data is true and correct the supportive information and document can be provided on demand.\nYours Sincerely\nRahul Pratap Singh\nAcademia Dossier\n\n\n\nB.TECH-(Computer Science)\n\n (UPTU) \u2013 2010  -  63%\n\n\n\n10+2- (UP Board) - 67%\n\n\n\n10th - (UP Board) - 77%\n\n\n\n\n\nPersonal Dossier\n\n\n\nAddress:  27 - B Ritty Villa, Kodathi Gate, Sarjapura  Road ,  Bangalore\n\n\n\n\n\nLanguages Known: English, Hindi and Sanskrit\n\n\n\nMarital Status: Married", "entities": [[9815, 9821, "Degree", "B.TECH"], [9776, 9794, "Name", "Rahul Pratap Singh"], [9424, 9430, "Skills", "PL/SQL"], [9417, 9423, "Skills", "Oracle"], [9385, 9389, "Skills", "Java"], [9354, 9360, "Skills", "Struts"], [9327, 9331, "Skills", "Java"], [8682, 8688, "Skills", "PL/SQL"], [8675, 8681, "Skills", "Oracle"], [8632, 8636, "Skills", "Java"], [8601, 8607, "Skills", "Struts"], [8574, 8578, "Skills", "Java"], [7972, 7978, "Skills", "PL/SQL"], [7965, 7971, "Skills", "Oracle"], [7922, 7926, "Skills", "Java"], [7891, 7897, "Skills", "Struts"], [7864, 7868, "Skills", "Java"], [7206, 7210, "Skills", "HDFS"], [6756, 6760, "Skills", "Hive"], [6750, 6754, "Skills", "HDFS"], [6611, 6615, "Skills", "Hive"], [6605, 6609, "Skills", "HDFS"], [5938, 5943, "Skills", "Hbase"], [5751, 5755, "Skills", "HDFS"], [5689, 5693, "Skills", "HDFS"], [5257, 5261, "Skills", "Hive"], [5112, 5116, "Skills", "Hive"], [5082, 5085, "Skills", "Pig"], [5046, 5051, "Skills", "Oozie"], [4994, 4998, "Skills", "HDFS"], [4983, 4987, "Skills", "HDFS"], [4904, 4909, "Skills", "Sqoop"], [4893, 4897, "Skills", "HDFS"], [4617, 4621, "Skills", "HDFS"], [4607, 4612, "Skills", "Sqoop"], [4200, 4203, "Skills", "Pig"], [4194, 4198, "Skills", "Hive"], [4188, 4192, "Skills", "HDFS"], [4182, 4186, "Skills", "Java"], [4048, 4053, "Skills", "Hbase"], [3873, 3877, "Skills", "HDFS"], [3286, 3290, "Skills", "Hive"], [3131, 3135, "Skills", "Hive"], [3101, 3104, "Skills", "Pig"], [3066, 3071, "Skills", "Oozie"], [3015, 3019, "Skills", "HDFS"], [2935, 2940, "Skills", "Sqoop"], [2924, 2928, "Skills", "HDFS"], [2658, 2662, "Skills", "HDFS"], [2317, 2322, "Skills", "Scala"], [2310, 2315, "Skills", "Spark"], [2249, 2252, "Skills", "Pig"], [2243, 2247, "Skills", "Hive"], [2237, 2241, "Skills", "HDFS"], [2139, 2144, "Operating_Systems", "Linux"], [2130, 2137, "Operating_Systems", "Windows"], [2085, 2091, "Skills", "Hadoop"], [2079, 2082, "Skills", "WAS"], [2068, 2076, "Skills", "Weblogic"], [2053, 2059, "Skills", "Oracle"], [2026, 2029, "Tools", "Git"], [2021, 2024, "Tools", "SVN"], [1981, 1987, "Skills", "Struts"], [1974, 1978, "Skills", "HDFS"], [1966, 1972, "Skills", "Hadoop"], [1946, 1952, "Skills", "PL-SQL"], [1942, 1945, "Skills", "SQL"], [1935, 1941, "Skills", "Oracle"], [1919, 1925, "Skills", "Spring"], [1905, 1917, "Skills", "java Scripts"], [1885, 1902, "Skills", "Rest Web Services"], [1874, 1883, "Skills", "Struts1.2"], [1868, 1872, "Skills", "J2ee"], [1863, 1867, "Skills", "Java"], [1837, 1841, "Skills", "Java"], [1824, 1832, "Skills", "Cassndra"], [1817, 1822, "Skills", "Kafka"], [1810, 1815, "Skills", "Scala"], [1803, 1808, "Skills", "Spark"], [1795, 1800, "Skills", "Hbase"], [1788, 1793, "Skills", "Oozie"], [1777, 1786, "Skills", "MapReduce"], [1770, 1775, "Skills", "Sqoop"], [1765, 1768, "Skills", "Pig"], [1759, 1763, "Skills", "Hive"], [1751, 1755, "Skills", "HDFS"], [1743, 1749, "Skills", "Hadoop"], [1727, 1734, "Skills", "BigData"], [961, 966, "Skills", "Kafka"], [944, 949, "Skills", "Spark"], [933, 938, "Skills", "Spark"], [921, 926, "Skills", "Spark"], [914, 919, "Skills", "Scala"], [882, 887, "Skills", "Oozie"], [863, 868, "Skills", "Sqoop"], [852, 856, "Skills", "Hive"], [830, 836, "Skills", "Hadoop"], [485, 492, "Skills", "BigData"], [437, 444, "Years_of_Experience", "8 years"], [62, 79, "Mobile_No", "+91 9953579739(M)"], [28, 49, "Email_Address", "rathaur.rps@gmail.com"], [0, 18, "Name", "Rahul Pratap Singh"]]}