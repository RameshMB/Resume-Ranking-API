{"text": "R.Krishnaveni\nMobile :9092116379\nE-Mail: krishnaraju03jan@gmail.com\nLinkedIn: linkedin.com/in/krishna-raju-124bbb167\n\n\nProfessional Summary\n\nKrishnaveni  Raju  has 8+ years of professional experience in  the IT industry  which includes 3+ yrs of Bigdata experience as a senior data analyst at UST Global.\n\n\u00b7 Worked as independent Bigdata  engineer\n\u00b7 Good experience in Pyspark\n\u00b7 Experienced on Datamining, Data analystics, Testing\n\u00b7 Worked in Yarn application\n\u00b7 Experienced on Aws with PySpark.\n\u00b7 Working experience on S3,Crawler,Data catalog\n\u00b7 Experienced in AWS Glue PySpark Transforms for use in PySpark ETL operations\n\u00b7 Working experience on DynamicFrameCollection Class,DynamicFrame Class\n\u00b7 Writing the Schemas in the Data Catalog\n\u00b7 Crawl the Data in the Amazon S3 Bucket\n\u00b7 Worked on , Map the Data and Use Apache Spark Lambda Functions\n\u00b7 Experienced in using Job Bookmarks for generating AWS scripts\n\u00b7 experienced in job monitoring and debugging\n\u00b7 moving the data to and from  Amazon Redshift.\n\u00b7 Experience in unit testing for testing the  script\n\u00b7 working with the tools in Visual Studio, Atom, Eclipse, Kafka Tool.\n\u00b7 Used Hortonworks, cloudera distribution management systems\n\u00b7 Developed the script for Data generation File.\n\u00b7 Developed the programming in Spark streaming for Transforming the data.\n\u00b7 Using REST API Services\n\u00b7 Worked on data analytics tool like Elastic search\n\u00b7 Experience on Storm with Kafka.\n\u00b7 Executing Analytics, Dashboard and Reporting solutions like Kibana.\n\u00b7 Hands on experience with Hadoop, Hive,Sql.\n\u00b7 Having good hands on experience in Data Warehousing such as ETL for Data Ingestion\n\u00b7 Experience with techniques of performance optimization for both data loading and data retrieval\n\u00b7 Good knowledge in SQL queries.\n\u00b7 Ability to deploy and maintain multi-node Hadoop cluster\n\u00b7 Experience of data ingestion tools like Sqoop\n\u00b7 Experience with Spark Streaming, Apache NiFi and Kafka for real-time data processing\n\u00b7 Participated in Designing the Architecture of the project\n\u00b7 Work with cross functional teams excellently\n\u00b7 Done Enhancements to  production jobs in order to meet the customer requirements\n\u00b7 Manage and Review data backups and log files\n\u00b7 Developed Hadoop Migration Solution approach\n\u00b7 Finished ITIL certification\n\n\n\n\n\nRoles Played in earlier/current Projects:\n\u00b7 Design and Development: Have experience as a good Developer and Analyst with good interaction skills to convert the requirement into solutions.\n\u00b7 Maintenance and Support Analyst:\u00a0 Worked as Application technical support analyst in analyzing issues and providing permanent fixes or short term workarounds.\n\n\nEmployment summary\nOrganization\t\t\tDesignation\t\t\t\tDuration\n\nUST Global\t\t\tSenior Data analyst\t\t\t2018-Till\nHP Global Soft Pvt Ltd\t\tSenior Data Engineer\t\t\t2014-2018\nSI Systems\t\t\tSenior Software Engineer\t\t2009-2014\n\n\n\n\n\nTechnical Skills\n\tHadoop Management \n\tCloudera ,Hortonworks\n\n\tHadoopEco System\n\tHadoop, HDFS, Sqoop \n\n\tDistributed Processing,Security tools.\n\tSpark, Storm,Hive,Kerbros\n\n\tNOSQL\n\tHbase\n\n\tData Visualization Tools,Data analytics tool\n\tKibana, Zepplin, AWS, S3, Redshift\n\n\tProgramming\n\tScala, Java, PL/SQL\n\n\tSupporting Tools\n\tElasticSearch, Logstash\n\n\tRDBMS\n\tOracle, SQL Server  and Mysql, Kafka\n\n\tOperating system\n\tPython, Linux, Centos, Windows\n\n\tDevelopment Tools\n\tVisual Studio, Eclipse, Intellija, Atom, Scala, Toad, PL/SQL Developer,SQL Developer\n\n\n\n\n\n\n\n\n\n\n\n\n\nPROJECT #\nTitle\t\t:\tSAIE\nClient\t\t:\tDell\nPeriod\t\t:\tSince Sep\u20192018 Till\nRole\t\t:\tSenior Data Analyst\nEnvironeent\t:\tHadoop, Spark,Java,Kafka,Hive,Hbase,Zipplin,Storm,Informatica,Oracle,\n\t\t\tAws,Elasticsearch,Kibana,Tableau\n\n\nKey contribution\n\n\u00b7 working with the tools in VisualStudio, Eclipse,Kafka Tool\n\u00b7 Experience in Pyspark\n\u00b7 Experience with Spark coding for Ingestion. \n\u00b7 Developed the programming in Spark streaming for Transforming the data.\n\u00b7 Experienced in Nifi,Oozie\n\u00b7 Experience in Hive,Hbase.\n\u00b7 Worked Kerbros for security.\n\u00b7 Experienced in AWS Glue PySpark Transforms for use in PySpark ETL operations\n\u00b7 Working experience on DynamicFrameCollection Class,DynamicFrame Class\n\u00b7 Writing the Schemas in the Data Catalog\n\u00b7 Crawl the Data in the Amazon S3 Bucket\n\u00b7 Worked on , Map the Data and Use Apache Spark Lambda Functions\n\u00b7 Experienced in using Job Bookmarks for generating AWS scripts\n\u00b7 experienced in job monitoring and debugging\n\u00b7 moving the data to and from  Amazon Redshift.\n\u00b7 Experience in unit testing for testing the  script\n\u00b7 Created the index, Data loading in Elastic search\n\u00b7 Having good experience in Storm with Kafka.\n\u00b7 Experience with Spark with AWS cluster.\n\u00b7 Hands on experience with Hadoop, Hive , Sql.\n\u00b7 Experience with techniques of performance optimization for both data loading and data retrieval\n\u00b7 Moving data from HDFS to Oracle and vice-versa using REST API\n\u00b7 Developed Storm workflow for scheduling and orchestrating the ETL process\n\u00b7 Executing Analytics, Dashboard and Reporting solutions \n\n\n\n\n\nPROJECT #\nTitle\t\t:\tCanon\nClient\t\t:\tCanon\nPeriod\t\t:\t2014-2018 \nRole\t\t:\tSenior Data Engineer\nEnvironeent\t:\tHadoop, Spark,Scala,Hive,Hbase,Sqoop,Nifi,Oracle,\n\t\t\tElasticsearch,Kibana,Tableau,Kafka\n\n\n\nKey Contribution\n\u00b7 Developed  Hadoop Migration Solution approach\n\u00b7 working with the tools in Hadoop Ecosystem including  Hive, HDFS, Sqoop, Kafka\n\u00b7 Experience in in Spark using Scala.\n\u00b7 Experience with Spark Streaming, Apache NiFi and Kafka for real-time data processing\n\u00b7 Developed the programming in Spark streaming for Transforming the data.\n\u00b7 Develop the programming in Scala, Python.\n\u00b7 Created the index, Data loading in Elastic search\n\u00b7 Using Logstash to store the log files and retrieve the log files\n\u00b7 Hands on experience with Hadoop, Hive, Sql.\n\u00b7 Experience with techniques of performance optimization for both data loading and data retrieval\n\u00b7 Moving data from Oracle into  HDFS  using Sqoop\n\u00b7 Created and worked Sqoop jobs with incremental load to populate Hive External tables.\n\u00b7 Very good experience in monitoring and managing the Hadoop cluster using Cloudera Manager\n\u00b7 Executing Analytics, Dashboard and Reporting solutions \n\u00b7 Business operations report are been generated for end users using Kibana\n\u00b7 Importing and Exporting data into Hdfs using Sqoop, NiFi.\n\u00b7 Participated in Designing the Architecture of the project\n\nPROJECT #\nTitle\t\t:\tSymantec\nClient\t\t:\tSymantec\nPeriod\t\t:\t2014-2015 \nRole\t\t:\tSenior Application Engineer\nEnvironeent\t:\tOracle,Java, Ebs,Icon, AppInterface,Appmigration,OpenText\n\n\nKey Contribution\n\u00b7 Experience in developing extensions, upgrades and implementation Oracle E-Business Suite (11i and R12)\n\u00b7 Strong expertise in PL/SQL developing - Packages, Stored Procedures and Functions, Database Triggers\n\u00b7 Experience in developing, customization and extend Java, Host and PL\\SQL Concurrent programs\n\u00b7 Hands-on experience with Oracle Application Framework, Oracle\n\u00b7 Experience with supporting Month End Close and Year-End Close activities\n\n\nPROJECT #\nTitle\t\t:\tProcurement Systems\nClient\t\t:\tAMD Services\nPeriod\t\t:\t2012-2014 at SI Systems\nRole\t\t:\tSenior Software Engineer\nEnvironeent\t:\tOracle,Java,Sql Developer, Toad\n\n\nKey Contribution\n\u00b7 Analyzing existing code for necessary changes\n\u00b7 Incorporating new changes in existing packages and writing Triggers.\n\u00b7 Responsible for SQL tuning and security\n\u00b7 Creating external table to load data from CSV file\n\u00b7 Extracting data through ETL and stored procedures from CSV file to Oracle Database\n\u00b7 Extending Level 2 & 3 supports for the application in resolving day-to-day issues Working on night support for daily maintenance of the applications\n\u00b7 Executing performance tuning while incorporating indexes, full table scan, temporary tables and parallel wherever required to improve performance\n\u00b7 Examining and implementing change requests\n\u00b7 Participating in bug fixing and peer to peer code review activities.\n\u00b7 Handling implementation of security of data according to client's requirements by creating views.Analysis of the customer\u2019s current business process\n\n\n\nPROJECT #\nTitle\t\t:\tPrice Tracking Systems\nClient\t\t:\tBanCorp, US\nPeriod\t\t:\t2011-2012 at SI Systems\nRole\t\t:\tSenior  Engineer\nEnvironeent\t:\tOracle,Java,Sql Developer, Toad\n\n\nKey Contribution\n\u00b7 Serving as an Developer\n\u00b7 Analyzing existing code for necessary changes\n\u00b7 Writing PL/SQL package & procedures\n\u00b7 Creating external table to load data from CSV file\n\u00b7 Extracting data through ETL and stored procedures from CSV file to Oracle Database\n\u00b7 Extending Level 2 & 3 supports for the application in resolving day-to-day issues \n\u00b7 Working on night support for daily maintenance of the applications\n\u00b7 Executing performance tuning while incorporating indexes, full table scan, temporary tables and parallel wherever required to improve performance\n\u00b7 Examining and implementing change requests\n\u00b7 Participating in bug fixing and peer to peer code review activities\n\u00b7 Incorporating new changes in existing packages\n\u00b7 Developing PL/SQL packages with error handling capabilities\n\nEducation\nMS (Information Technology) from Madurai kamaraj university,Madurai. \nB.Sc. (Computers) from Madurai Kamaraj University,", "entities": [[8929, 8956, "Degree", "MS (Information Technology)"], [8869, 8875, "Skills", "PL/SQL"], [8374, 8380, "Skills", "Oracle"], [8224, 8230, "Skills", "PL/SQL"], [8116, 8120, "Tools", "Toad"], [8096, 8100, "Skills", "Java"], [8089, 8095, "Skills", "Oracle"], [7367, 7373, "Skills", "Oracle"], [7060, 7064, "Tools", "Toad"], [7040, 7044, "Skills", "Java"], [7033, 7039, "Skills", "Oracle"], [6806, 6812, "Skills", "Oracle"], [6776, 6782, "Skills", "Oracle"], [6707, 6711, "Skills", "Java"], [6573, 6579, "Skills", "PL/SQL"], [6513, 6519, "Skills", "Oracle"], [6376, 6380, "Skills", "Java"], [6369, 6375, "Skills", "Oracle"], [6177, 6182, "Skills", "Sqoop"], [6123, 6129, "Tools", "Kibana"], [5883, 5887, "Skills", "Hive"], [5838, 5843, "Skills", "Sqoop"], [5811, 5816, "Skills", "Sqoop"], [5799, 5803, "Skills", "HDFS"], [5786, 5792, "Skills", "Oracle"], [5658, 5662, "Skills", "Hive"], [5564, 5572, "Tools", "Logstash"], [5489, 5494, "Skills", "Scala"], [5417, 5422, "Skills", "Spark"], [5350, 5355, "Skills", "Kafka"], [5317, 5322, "Skills", "Spark"], [5292, 5297, "Skills", "Scala"], [5280, 5285, "Skills", "Spark"], [5255, 5260, "Skills", "Kafka"], [5248, 5253, "Skills", "Sqoop"], [5242, 5246, "Skills", "HDFS"], [5236, 5240, "Skills", "Hive"], [5106, 5111, "Skills", "Kafka"], [5091, 5097, "Tools", "Kibana"], [5066, 5072, "Skills", "Oracle"], [5055, 5060, "Skills", "Sqoop"], [5049, 5054, "Skills", "Hbase"], [5044, 5048, "Skills", "Hive"], [5038, 5043, "Skills", "Scala"], [5032, 5037, "Skills", "Spark"], [4792, 4797, "Skills", "Storm"], [4743, 4749, "Skills", "Oracle"], [4735, 4739, "Skills", "HDFS"], [4606, 4610, "Skills", "Hive"], [4558, 4561, "Tools", "AWS"], [4547, 4552, "Skills", "Spark"], [4522, 4527, "Skills", "Kafka"], [4511, 4516, "Skills", "Storm"], [4368, 4376, "Tools", "Redshift"], [4272, 4275, "Tools", "AWS"], [4197, 4202, "Skills", "Spark"], [4145, 4147, "Tools", "S3"], [3979, 3984, "Skills", "Spark"], [3949, 3954, "Skills", "Spark"], [3938, 3941, "Tools", "AWS"], [3899, 3906, "Skills", "Kerbros"], [3883, 3888, "Skills", "Hbase"], [3878, 3882, "Skills", "Hive"], [3791, 3796, "Skills", "Spark"], [3731, 3736, "Skills", "Spark"], [3678, 3683, "Skills", "Kafka"], [3670, 3677, "Tools", "Eclipse"], [3593, 3599, "Tools", "Kibana"], [3564, 3570, "Skills", "Oracle"], [3546, 3551, "Skills", "Storm"], [3532, 3537, "Skills", "Hbase"], [3527, 3531, "Skills", "Hive"], [3521, 3526, "Skills", "Kafka"], [3516, 3520, "Skills", "Java"], [3510, 3515, "Skills", "Spark"], [3364, 3377, "Tools", "SQL Developer"], [3347, 3363, "Tools", "PL/SQL Developer"], [3341, 3345, "Tools", "Toad"], [3334, 3339, "Skills", "Scala"], [3328, 3332, "Tools", "Atom"], [3317, 3326, "Tools", "Intellija"], [3308, 3315, "Tools", "Eclipse"], [3293, 3306, "Tools", "Visual Studio"], [3264, 3271, "Operating_Systems", "Windows"], [3256, 3262, "Operating_Systems", "Centos"], [3249, 3254, "Operating_Systems", "Linux"], [3215, 3220, "Skills", "Kafka"], [3208, 3213, "Skills", "Mysql"], [3192, 3202, "Skills", "SQL Server"], [3184, 3190, "Skills", "Oracle"], [3166, 3174, "Tools", "Logstash"], [3151, 3164, "Tools", "ElasticSearch"], [3124, 3130, "Skills", "PL/SQL"], [3118, 3122, "Skills", "Java"], [3111, 3116, "Skills", "Scala"], [3087, 3095, "Tools", "Redshift"], [3083, 3085, "Tools", "S3"], [3078, 3081, "Tools", "AWS"], [3069, 3076, "Tools", "Zepplin"], [3061, 3067, "Tools", "Kibana"], [3007, 3012, "Skills", "Hbase"], [3000, 3005, "Skills", "NOSQL"], [2990, 2997, "Skills", "Kerbros"], [2985, 2989, "Skills", "Hive"], [2979, 2984, "Skills", "Storm"], [2972, 2977, "Skills", "Spark"], [2955, 2969, "Skills", "Security tools"], [2932, 2954, "Skills", "Distributed Processing"], [2923, 2928, "Skills", "Sqoop"], [2917, 2921, "Skills", "HDFS"], [2891, 2915, "Skills", "HadoopEco System\n\tHadoop"], [2877, 2888, "Skills", "Hortonworks"], [2847, 2875, "Skills", "Hadoop Management \n\tCloudera"], [1908, 1913, "Skills", "Kafka"], [1875, 1880, "Skills", "Spark"], [1851, 1856, "Skills", "Sqoop"], [1524, 1528, "Skills", "Hive"], [1481, 1487, "Tools", "Kibana"], [1412, 1417, "Skills", "Kafka"], [1401, 1406, "Skills", "Storm"], [1264, 1269, "Skills", "Spark"], [1130, 1141, "Skills", "Hortonworks"], [1111, 1116, "Skills", "Kafka"], [1102, 1109, "Tools", "Eclipse"], [1096, 1100, "Tools", "Atom"], [1081, 1094, "Tools", "Visual Studio"], [990, 998, "Tools", "Redshift"], [894, 897, "Tools", "AWS"], [819, 824, "Skills", "Spark"], [767, 769, "Tools", "S3"], [601, 606, "Skills", "Spark"], [571, 576, "Skills", "Spark"], [560, 563, "Tools", "AWS"], [519, 521, "Tools", "S3"], [488, 493, "Skills", "Spark"], [164, 172, "Years_of_Experience", "8+ years"], [41, 67, "Email_Address", "krishnaraju03jan@gmail.com"], [22, 32, "Mobile_No", "9092116379"], [0, 13, "Name", "R.Krishnaveni"]]}