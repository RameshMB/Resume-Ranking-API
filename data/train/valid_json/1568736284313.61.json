{"text": "Satheesh Reddy L\t\t\t\t\tEmail\t: Pandu.satheeshL@gmail.com\nBigData-Hadoop Developer\t\t\t\tMobile\t: +91-9700917927\nCareer Objective\n\nServing Notice period and can join within 30 days. Working in BT E-Serv popularly known as British Telecom as a BigData Hadoop Developer. Looking for an intellectually challenging opportunity in Hadoop development and Project Management that requires Core software experience in all phases of software development life cycle (SDLC) starting from requirements, analysis, architecture, technical design, implementation, performance tuning. \n\nExperience Summary\n\n\u00b7 Having 5 years of experience in BigData technology (Spark, Scala, Hive, MapReduce, Sqoop, Pig, Oozie, Shell, Kibana) as a Developer and Project Lead with multiple teams.\n\u00b7 Working on CDH distribution and good command on Hadoop cluster.\n\u00b7 Extracting Structured and Semi-Structured data from Oracle and other File sources from upstream systems into HDFS using scheduled jobs (Sqoop and shell actions).\n\u00b7 Worked on GPG encryption. Feeds which are coming from upstream systems will be zipped and encrypted using 64 bit algorithm. Need to Decrypt and process into DAA.\n\u00b7 Used Pig as ETL to process all our Entities and Extracts. Once done will update Oracle using Sqoop and will send the feeds to Down Stream using Local Shell actions.\n\u00b7 Worked on Oozie scheduler for unloads and regular jobs (Workflows and Co-ord).\n\u00b7 Used Hive as reporting for end user / business to perform ad-hoc analysis. Also used to analyze Broken Hierarchy between SAC, CUG, CUS and organization hierarchy.\n\u00b7 Implemented Partitions, Bucketing for monthly unloads and Legacy data in-order to improve the performance of Pig and spark jobs.\n\u00b7 Developed existing pig jobs into Spark Scala to as part of migration and to reduce the exec timings (Build time has been reduced to 3 days from 15 days)\n\u00b7 Data Lake story: Bringing the consumer data from various sources and expose the same to multiple systems. Biggest development and crucial part in all our releases.\n\u00b7 Designed and Developed Kibana dashboard. Generating Json files with index for Logstash. Having excellent command on ELK Stack. Productionised the same.\n\u00b7 Working on CRM systems as it holds business data and these are destinations.\n\u00b7 Working in Agile methodology and chairing the scrum calls as part of Agile and make sure everything is going smooth for each release.\n\nWorkspace Environment\n\n\n\tBig Data Ecosystem \n\tSpark, Scala, HDFS, Map Reduce, Hive, Pig, Sqoop, Oozie, Kibana\n\n\tProgramming Languages\n\tCore Java, Pig Latin, Scala\n\n\tDatabases\n\tOracle, CRM systems\n\n\tScripting Languages\n\tUnix and Shell Scripting\n\n\tTools\n\tEclipse, Win SCP, Putty, Webextam, Winmerge\n\n\tVisualization\n\tKibana, Thoughspot\n\n\tVersioning Tools\n\tSVN, Git(Local), Jira.\n\n\n\nEmployer Details\n\n\tEmployer Name\n\tDuration\n\tRole\n\n\tBT E- serv, Bangalore\n\tDec-2016 to Till date\n\tLead & Developer\n\n\tWipro technologies, Bangalore\n\tApr-2014 to Nov-2016\n\tdeveloper\n\n\n\nAcademic Details\n\n\tB.Tech\n(Computer science engineering)\n\tSiddhartha Institute of Engineering and technology, Puttur\n\t76.00%\n\tMay-2013\n\n\tIntermediate\n(MPC)\n\tSri Chaitanya Jr. College, Tirupathi\n\t94.50%\n\tMay-2009\n\n\tHigher education\n(SSC)\n\tAPR School, Gyarampalli\n\t93.00%\n\tMay-2007\n\n\n\n\nProject Summary Details\n\nProject #3:\n\nProject Name\t\t\t: CSS OUK (EE Orange)\nClient\t\t\t\t: British Telecom && EE Orange\nTechnical Environment   \t: HDFS, Spark, Scala, Hive, Sqoop, Oozie, UNIX and shell scripting\nTeam Members\t\t\t : 14\nDuration \t\t\t: January 2018 to till date\nRole\t\t\t\t: Lead and Developer\n\nDescription: EE is a British mobile network operator, internet service provider and acquired by BT Business. It is having main three data areas specified as Orange, Red and green. Each divided based on importance of data it has. We have multiple vendors for data acquisition. We have a systems where feed is being generated using Abinito and the same will be transferred to PIS for GPG encryption for high security. Once encryption completes it will be transferred to SDEDS, from where we BT will be receiving the data. Once we receive the data from SDEDS the processing will starts from here.\n\nRoles and Responsibilities:\n\u00b7 As the data is highly critical, it is being encrypted with 64 bit cryptographic gpg encryption. Public key will be shared with PIS which they will be used for encryption.\n\u00b7 Will use Private key to decrypt the same, Unzip it the local and move the files into HDFS post processing store into HDFS (Using Oozie shell action to do this)\n\u00b7 Once receive the data into HDFS, process the same using Spark Scala with certain conditions and create Feeds and Data Lake into CSS Haas.\n\u00b7 These feeds are being used by all our DS. We have regular ETL jobs in spark which are written in Scala. After processing storing the same into Hive for other jobs. \n\u00b7 Hive has been used as a reporting and auditing purpose. \n\u00b7 Broken Hierarchy has been created using Pig in-order to populate missing linkages.\n\u00b7 The same thing has been visualized in ETL stack (Kibana) and is self-reflective.\n\u00b7 Used External Hive tables on top of the output and used for business analysis.\n\u00b7 Use of Oozie workflow to run multiple serial and parallel action using workflows and co-ord.\n\u00b7 Chairing the Scrum calls as part of integration with various vendors as part of Agile.\n\u00b7 Post Data Lake, we have design documents received from architects which is in progress of project development phase-2. \n\nProject #2:\n\nProject Name\t\t\t: SIM REWRITE (BT E- serv)\nClient\t\t\t\t: BT\nTechnical Environment   \t: Spark, Scala, HDFS, Pig, Hive, Sqoop, Oozie, Spark, Scala\nTeam Members\t\t\t : 28 \nDuration \t\t\t: December 2016 to January 2018\nRole\t\t\t\t: Developer\n\nDescription: BT is huge telecom retailer which is maintaining billing accounts, address and\nAssets of customer groups in CRM systems to maintain customer info. We are working with various vendors to rewrite all existing legacy applications into Hadoop. Developed new unload framework with complete architecture and redesign. Storing all existing data from various sources into Hadoop as a part of data lake story. Working on various technologies In Hadoop to implement all new design and stability. Using agile methodology and chairing Scrum to discuss the status with vendors.\n\nRoles and Responsibilities:\n\u00b7 As a part of rewrite, All AI jobs (ETL) has been successfully written into Spark with Scala which reduces project cost and increased execution time (15 days to 1 day).\n\u00b7 Developing various Pig scripts for ETL processing and scheduled as part of automation.\n\u00b7 Created multiple hive jobs to create audit tables to identify broken hierarchy.\n\u00b7 Automated the Unload framework with redesign and new architecture.\n\u00b7 Using Sqoop to import and export data from ORACLE to HDFS and vice-versa.\n\u00b7 Developed shell scripts in order to curl data from AI servers and vice versa.\n\u00b7 Created various Shell scripts for various monitoring reports (Edge node space, queue size, delta count)\u2014scheduled using Sqoop as shell default jobs disabled.\n\u00b7 Used External Hive tables on top of the output and used for business analysis.\n\u00b7 Use of Oozie workflow to run multiple serial and parallel action using workflows and co-ord.\n\u00b7 Chairing the Scrum calls as part of integration with various vendors as part of Agile\n\u00b7 Developed Spark applications as part of rewrite and done POC before implementing the same.\n\nProject #1:\n\nProject Name\t\t\t: Shell FMO Build (Wipro Project)\nClient\t\t\t\t: Shell\nTechnical Environment   \t: HDFS, Map Reduce, Pig, Hive, Sqoop\nProject Duration\t\t: September 2014 to till date\nRole\t\t\t\t: Hadoop Developer & Production Support\n\nDescription: Royal Dutch Shell plc commonly known as Shell is an Anglo-Dutch multinational oil and gas company headquartered in the Netherlands. It is the seventh largest company in the world as of 2016, in terms of revenue, and one of the six oil and gas \"super majors\". It is having its business unit entire world which is categorized Asia-Pac, Europe, Americas, Australia and Coastline. The data is getting generated from all there areas and is populated in different databases for different regions.\n\tIn Shell FMO Build the aim is to merge all these domains and collect data into a single system. The data is stored in Hadoop cluster of 22 nodes (may be increase in future). Data is collected from all existing RDMS systems into HDFS and from different staging locations. The raw data will be ingested into HDFS by using Sqoop from various sources. Data transformations are done by using Pig and SSIS ETL tools. Later on it is moved to hive External tables for reports. Reports are being generated on weekly and monthly basis based on customer requirement.\n\nRoles and Responsibilities:\n\u00b7 Moving data files generated from various location into HDFS using Sqoop and Shell scripts.\n\u00b7 Developing Hive and Pig queries for data analysis and reporting.\n\u00b7 Developed PIG scripts to convert the semi structured data to structured data.\n\u00b7 Worked on Hive partitioning and bucketing to improve the performance of system.\n\u00b7 Monitoring Hadoop scripts that will load data into HDFS at regular intervals.\n\u00b7 Writing custom Pig functions as per customer request to clean the data and provide results.\n\u00b7 Monitoring all Pig and Sqoop jobs day to day.\n\u00b7 Submitting the Test case and Test result document to the clients.\n\u00b7 Getting connected with the clients and onshore team to review the code and validation of test results.\n\nAchievements and Appreciations\n\n\u00b7 Rewarded BMPS award for two quarters continuously. Posted in Hal of Fame for six months.\n\u00b7 Awarded with Brilliant performance and promoted within 18 months (Least period in BT).\n\u00b7 Got best team lead award for SIM-Rewrite in BT TSO year end awards.\n\u00b7 Migrated/developed completed AI to Hadoop within span of six months in BT which reduces project cost and saved execution time (Build reduced from 15 days to 3 days). \n\u00b7 Received multiple e-thank cards and appreciations in various all hands meet. \n\u00b7 Two continuous outstanding awards in Wipro for excellent performance in shell.\n\nPersonal Information\n\nName  \t\t\t\t: Satheesh Reddy L\t\nMarital Status\t\t\t: Unmarried\nPassport\t\t\t: L2858858\nNationality\t\t\t: Indian\nLanguages known\t\t: Hindi, English, Telugu\nNotice Period\t\t\t: Serving Notice (30 Days)\n\nDeclaration\n\nI hereby confirm that the information furnished above is true to the best of my knowledge.\n\nPlace: Bangalore\t\t\t\t\t\t\t(Satheesh Reddy)\nL Satheesh Reddy\tBigdata Developer\t9700917927", "entities": [[10037, 10053, "Name", "Satheesh Reddy L"], [9193, 9198, "Skills", "Sqoop"], [9185, 9188, "Skills", "Pig"], [9091, 9094, "Skills", "Pig"], [9047, 9051, "Skills", "HDFS"], [8924, 8928, "Skills", "Hive"], [8787, 8790, "Skills", "Pig"], [8778, 8782, "Skills", "Hive"], [8740, 8745, "Skills", "Sqoop"], [8729, 8733, "Skills", "HDFS"], [8474, 8477, "Skills", "Pig"], [8407, 8412, "Skills", "Sqoop"], [8393, 8397, "Skills", "HDFS"], [8315, 8319, "Skills", "HDFS"], [7479, 7484, "Skills", "Sqoop"], [7473, 7477, "Skills", "Hive"], [7468, 7471, "Skills", "Pig"], [7456, 7466, "Skills", "Map Reduce"], [7450, 7454, "Skills", "HDFS"], [7261, 7266, "Skills", "Spark"], [7075, 7080, "Skills", "Oozie"], [7001, 7005, "Skills", "Hive"], [6947, 6952, "Skills", "Sqoop"], [6724, 6728, "Skills", "HDFS"], [6677, 6682, "Skills", "Sqoop"], [6450, 6453, "Skills", "Pig"], [6450, 6453, "Skills", "Pig"], [6347, 6352, "Skills", "Scala"], [6336, 6341, "Skills", "Spark"], [5773, 5784, "Skills", "CRM systems"], [5559, 5564, "Skills", "Scala"], [5552, 5557, "Skills", "Spark"], [5545, 5550, "Skills", "Oozie"], [5538, 5543, "Skills", "Sqoop"], [5532, 5536, "Skills", "Hive"], [5527, 5530, "Skills", "Pig"], [5521, 5525, "Skills", "HDFS"], [5514, 5519, "Skills", "Scala"], [5507, 5512, "Skills", "Spark"], [5112, 5117, "Skills", "Oozie"], [5038, 5042, "Skills", "Hive"], [4990, 4996, "Skills", "Kibana"], [4896, 4899, "Skills", "Pig"], [4896, 4899, "Skills", "Pig"], [4797, 4801, "Skills", "Hive"], [4773, 4777, "Skills", "Hive"], [4727, 4732, "Skills", "Scala"], [4552, 4557, "Skills", "Scala"], [4546, 4551, "Skills", "Spark"], [4517, 4521, "Skills", "HDFS"], [4457, 4462, "Skills", "Oozie"], [4445, 4449, "Skills", "HDFS"], [4413, 4417, "Skills", "HDFS"], [3407, 3412, "Skills", "Oozie"], [3400, 3405, "Skills", "Sqoop"], [3394, 3398, "Skills", "Hive"], [3387, 3392, "Skills", "Scala"], [3380, 3385, "Skills", "Spark"], [3374, 3378, "Skills", "HDFS"], [2966, 2972, "Degree", "B.Tech"], [2756, 2760, "Tools", "Jira"], [2744, 2754, "Tools", "Git(Local)"], [2739, 2742, "Tools", "SVN"], [2708, 2718, "Skills", "Thoughspot"], [2700, 2706, "Skills", "Kibana"], [2674, 2682, "Tools", "Winmerge"], [2664, 2672, "Tools", "Webextam"], [2657, 2662, "Tools", "Putty"], [2648, 2655, "Tools", "Win SCP"], [2639, 2646, "Tools", "Eclipse"], [2614, 2629, "Skills", "Shell Scripting"], [2605, 2609, "Skills", "Unix"], [2570, 2581, "Skills", "CRM systems"], [2562, 2568, "Skills", "Oracle"], [2543, 2548, "Skills", "Scala"], [2532, 2535, "Skills", "Pig"], [2532, 2535, "Skills", "Pig"], [2532, 2541, "Skills", "Pig Latin"], [2521, 2530, "Skills", "Core Java"], [2489, 2495, "Skills", "Kibana"], [2482, 2487, "Skills", "Oozie"], [2475, 2480, "Skills", "Sqoop"], [2470, 2473, "Skills", "Pig"], [2464, 2468, "Skills", "Hive"], [2452, 2462, "Skills", "Map Reduce"], [2446, 2450, "Skills", "HDFS"], [2439, 2444, "Skills", "Scala"], [2432, 2437, "Skills", "Spark"], [2411, 2419, "Skills", "Big Data"], [2183, 2194, "Skills", "CRM systems"], [2041, 2047, "Skills", "Kibana"], [1736, 1741, "Skills", "Scala"], [1730, 1735, "Skills", "Spark"], [1675, 1678, "Skills", "Pig"], [1675, 1678, "Skills", "Pig"], [1406, 1410, "Skills", "Hive"], [1330, 1335, "Skills", "Oozie"], [1246, 1251, "Skills", "Sqoop"], [1233, 1239, "Skills", "Oracle"], [1158, 1161, "Skills", "Pig"], [1158, 1161, "Skills", "Pig"], [961, 966, "Skills", "Sqoop"], [934, 938, "Skills", "HDFS"], [877, 883, "Skills", "Oracle"], [696, 702, "Skills", "Kibana"], [682, 687, "Skills", "Oozie"], [677, 680, "Skills", "Pig"], [670, 675, "Skills", "Sqoop"], [653, 657, "Skills", "Hive"], [646, 651, "Skills", "Scala"], [639, 644, "Skills", "Spark"], [594, 601, "Years_of_Experience", "5 years"], [92, 106, "Mobile_No", "+91-9700917927"], [29, 54, "Email_Address", "Pandu.satheeshL@gmail.com"], [0, 16, "Name", "Satheesh Reddy L"]]}